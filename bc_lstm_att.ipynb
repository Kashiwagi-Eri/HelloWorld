{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCXShUtAkCm1wcyT48WXHH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kashiwagi-Eri/HelloWorld/blob/master/bc_lstm_att.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The code below is modified from original repo:\n",
        "https://github.com/declare-lab/conv-emotion/tree/master/bc-LSTM-pytorch\n",
        "\n",
        "###Modified by CS6493 course project Group 18\n",
        "Last modified: 2023.03.27\n"
      ],
      "metadata": {
        "id": "JyIMwF0iNqOH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ei9s-xqd4FJw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import torch\n",
        "import torch.nn\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "\n",
        "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "\n",
        "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "\n",
        "!pip install torch-geometric\n",
        "\n",
        "from torch_geometric.data import Data"
      ],
      "metadata": {
        "id": "5qY1EhKT6hPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c70e3e48-3bc3-437f-9f2f-079162c0d5b4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.9/dist-packages (2.1.1+pt113cu116)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.9/dist-packages (0.6.17+pt113cu116)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch-sparse) (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.9/dist-packages (from scipy->torch-sparse) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
            "Requirement already satisfied: torch-cluster in /usr/local/lib/python3.9/dist-packages (1.6.1+pt113cu116)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch-cluster) (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.9/dist-packages (from scipy->torch-cluster) (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
            "Requirement already satisfied: torch-spline-conv in /usr/local/lib/python3.9/dist-packages (1.2.2+pt113cu116)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.9/dist-packages (2.3.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (5.9.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (2.27.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.10.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch-geometric) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (2022.12.7)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch-geometric) (1.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Etu8RTOC6hSF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "051773a7-8cb7-4c49-9b65-0fcd76fdff53"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    FloatTensor = torch.cuda.FloatTensor\n",
        "    LongTensor = torch.cuda.LongTensor\n",
        "    ByteTensor = torch.cuda.ByteTensor\n",
        "\n",
        "else:\n",
        "    FloatTensor = torch.FloatTensor\n",
        "    LongTensor = torch.LongTensor\n",
        "    ByteTensor = torch.ByteTensor\n",
        "\n",
        "class MaskedNLLLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, weight=None):\n",
        "        super(MaskedNLLLoss, self).__init__()\n",
        "        self.weight = weight\n",
        "        self.loss = nn.NLLLoss(weight=weight,\n",
        "                               reduction='sum')\n",
        "\n",
        "    def forward(self, pred, target, mask):\n",
        "        \"\"\"\n",
        "        pred -> batch*seq_len, n_classes\n",
        "        target -> batch*seq_len\n",
        "        mask -> batch, seq_len\n",
        "        \"\"\"\n",
        "        mask_ = mask.view(-1,1) # batch*seq_len, 1\n",
        "        if type(self.weight)==type(None):\n",
        "            loss = self.loss(pred*mask_, target)/torch.sum(mask)\n",
        "        else:\n",
        "            loss = self.loss(pred*mask_, target)\\\n",
        "                            /torch.sum(self.weight[target]*mask_.squeeze())\n",
        "        return loss\n",
        "\n",
        "\n",
        "class MaskedMSELoss(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MaskedMSELoss, self).__init__()\n",
        "        self.loss = nn.MSELoss(reduction='sum')\n",
        "\n",
        "    def forward(self, pred, target, mask):\n",
        "        \"\"\"\n",
        "        pred -> batch*seq_len\n",
        "        target -> batch*seq_len\n",
        "        mask -> batch*seq_len\n",
        "        \"\"\"\n",
        "        loss = self.loss(pred*mask, target)/torch.sum(mask)\n",
        "        return loss\n",
        "\n",
        "\n",
        "class UnMaskedWeightedNLLLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, weight=None):\n",
        "        super(UnMaskedWeightedNLLLoss, self).__init__()\n",
        "        self.weight = weight\n",
        "        self.loss = nn.NLLLoss(weight=weight,\n",
        "                               reduction='sum')\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        \"\"\"\n",
        "        pred -> batch*seq_len, n_classes\n",
        "        target -> batch*seq_len\n",
        "        \"\"\"\n",
        "        if type(self.weight)==type(None):\n",
        "            loss = self.loss(pred, target)\n",
        "        else:\n",
        "            loss = self.loss(pred, target)\\\n",
        "                            /torch.sum(self.weight[target])\n",
        "        return loss\n",
        "\n",
        "\n",
        "class SimpleAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim):\n",
        "        super(SimpleAttention, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.scalar = nn.Linear(self.input_dim,1,bias=False)\n",
        "\n",
        "    def forward(self, M, x=None):\n",
        "        \"\"\"\n",
        "        M -> (seq_len, batch, vector)\n",
        "        x -> dummy argument for the compatibility with MatchingAttention\n",
        "        \"\"\"\n",
        "        scale = self.scalar(M) # seq_len, batch, 1\n",
        "        alpha = F.softmax(scale, dim=0).permute(1,2,0) # batch, 1, seq_len\n",
        "        attn_pool = torch.bmm(alpha, M.transpose(0,1))[:,0,:] # batch, vector\n",
        "        return attn_pool, alpha\n",
        "\n",
        "\n",
        "class MatchingAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, mem_dim, cand_dim, alpha_dim=None, att_type='general'):\n",
        "        super(MatchingAttention, self).__init__()\n",
        "        assert att_type!='concat' or alpha_dim!=None\n",
        "        assert att_type!='dot' or mem_dim==cand_dim\n",
        "        self.mem_dim = mem_dim\n",
        "        self.cand_dim = cand_dim\n",
        "        self.att_type = att_type\n",
        "        if att_type=='general':\n",
        "            self.transform = nn.Linear(cand_dim, mem_dim, bias=False)\n",
        "        if att_type=='general2':\n",
        "            self.transform = nn.Linear(cand_dim, mem_dim, bias=True)\n",
        "            #torch.nn.init.normal_(self.transform.weight,std=0.01)\n",
        "        elif att_type=='concat':\n",
        "            self.transform = nn.Linear(cand_dim+mem_dim, alpha_dim, bias=False)\n",
        "            self.vector_prod = nn.Linear(alpha_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, M, x, mask=None):\n",
        "        \"\"\"\n",
        "        M -> (seq_len, batch, mem_dim)\n",
        "        x -> (batch, cand_dim)\n",
        "        mask -> (batch, seq_len)\n",
        "        \"\"\"\n",
        "        if type(mask)==type(None):\n",
        "            mask = torch.ones(M.size(1), M.size(0)).type(M.type())\n",
        "\n",
        "        if self.att_type=='dot':\n",
        "            # vector = cand_dim = mem_dim\n",
        "            M_ = M.permute(1,2,0) # batch, vector, seqlen\n",
        "            x_ = x.unsqueeze(1) # batch, 1, vector\n",
        "            alpha = F.softmax(torch.bmm(x_, M_), dim=2) # batch, 1, seqlen\n",
        "        elif self.att_type=='general':\n",
        "            M_ = M.permute(1,2,0) # batch, mem_dim, seqlen\n",
        "            x_ = self.transform(x).unsqueeze(1) # batch, 1, mem_dim\n",
        "            alpha = F.softmax(torch.bmm(x_, M_), dim=2) # batch, 1, seqlen\n",
        "        elif self.att_type=='general2':\n",
        "            M_ = M.permute(1,2,0) # batch, mem_dim, seqlen\n",
        "            x_ = self.transform(x).unsqueeze(1) # batch, 1, mem_dim\n",
        "            mask_ = mask.unsqueeze(2).repeat(1, 1, self.mem_dim).transpose(1, 2) # batch, seq_len, mem_dim\n",
        "            M_ = M_ * mask_\n",
        "            alpha_ = torch.bmm(x_, M_)*mask.unsqueeze(1)\n",
        "            alpha_ = torch.tanh(alpha_)\n",
        "            alpha_ = F.softmax(alpha_, dim=2)\n",
        "            # alpha_ = F.softmax((torch.bmm(x_, M_))*mask.unsqueeze(1), dim=2) # batch, 1, seqlen\n",
        "            alpha_masked = alpha_*mask.unsqueeze(1) # batch, 1, seqlen\n",
        "            alpha_sum = torch.sum(alpha_masked, dim=2, keepdim=True) # batch, 1, 1\n",
        "            alpha = alpha_masked/alpha_sum # batch, 1, 1 ; normalized\n",
        "            #import ipdb;ipdb.set_trace()\n",
        "        else:\n",
        "            M_ = M.transpose(0,1) # batch, seqlen, mem_dim\n",
        "            x_ = x.unsqueeze(1).expand(-1,M.size()[0],-1) # batch, seqlen, cand_dim\n",
        "            M_x_ = torch.cat([M_,x_],2) # batch, seqlen, mem_dim+cand_dim\n",
        "            mx_a = F.tanh(self.transform(M_x_)) # batch, seqlen, alpha_dim\n",
        "            alpha = F.softmax(self.vector_prod(mx_a),1).transpose(1,2) # batch, 1, seqlen\n",
        "\n",
        "        attn_pool = torch.bmm(alpha, M.transpose(0,1))[:,0,:] # batch, mem_dim\n",
        "        return attn_pool, alpha\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim=None, out_dim=None, n_head=1, score_function='dot_product', dropout=0):\n",
        "        ''' Attention Mechanism\n",
        "        :param embed_dim:\n",
        "        :param hidden_dim:\n",
        "        :param out_dim:\n",
        "        :param n_head: num of head (Multi-Head Attention)\n",
        "        :param score_function: scaled_dot_product / mlp (concat) / bi_linear (general dot)\n",
        "        :return (?, q_len, out_dim,)\n",
        "        '''\n",
        "        super(Attention, self).__init__()\n",
        "        if hidden_dim is None:\n",
        "            hidden_dim = embed_dim // n_head\n",
        "        if out_dim is None:\n",
        "            out_dim = embed_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_head = n_head\n",
        "        self.score_function = score_function\n",
        "        self.w_k = nn.Linear(embed_dim, n_head * hidden_dim)\n",
        "        self.w_q = nn.Linear(embed_dim, n_head * hidden_dim)\n",
        "        self.proj = nn.Linear(n_head * hidden_dim, out_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        if score_function == 'mlp':\n",
        "            self.weight = nn.Parameter(torch.Tensor(hidden_dim*2))\n",
        "        elif self.score_function == 'bi_linear':\n",
        "            self.weight = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
        "        else:  # dot_product / scaled_dot_product\n",
        "            self.register_parameter('weight', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.hidden_dim)\n",
        "        if self.weight is not None:\n",
        "            self.weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, k, q):\n",
        "        if len(q.shape) == 2:  # q_len missing\n",
        "            q = torch.unsqueeze(q, dim=1)\n",
        "        if len(k.shape) == 2:  # k_len missing\n",
        "            k = torch.unsqueeze(k, dim=1)\n",
        "        mb_size = k.shape[0]  # ?\n",
        "        k_len = k.shape[1]\n",
        "        q_len = q.shape[1]\n",
        "        # k: (?, k_len, embed_dim,)\n",
        "        # q: (?, q_len, embed_dim,)\n",
        "        # kx: (n_head*?, k_len, hidden_dim)\n",
        "        # qx: (n_head*?, q_len, hidden_dim)\n",
        "        # score: (n_head*?, q_len, k_len,)\n",
        "        # output: (?, q_len, out_dim,)\n",
        "        kx = self.w_k(k).view(mb_size, k_len, self.n_head, self.hidden_dim)\n",
        "        kx = kx.permute(2, 0, 1, 3).contiguous().view(-1, k_len, self.hidden_dim)\n",
        "        qx = self.w_q(q).view(mb_size, q_len, self.n_head, self.hidden_dim)\n",
        "        qx = qx.permute(2, 0, 1, 3).contiguous().view(-1, q_len, self.hidden_dim)\n",
        "        if self.score_function == 'dot_product':\n",
        "            kt = kx.permute(0, 2, 1)\n",
        "            score = torch.bmm(qx, kt)\n",
        "        elif self.score_function == 'scaled_dot_product':\n",
        "            kt = kx.permute(0, 2, 1)\n",
        "            qkt = torch.bmm(qx, kt)\n",
        "            score = torch.div(qkt, math.sqrt(self.hidden_dim))\n",
        "        elif self.score_function == 'mlp':\n",
        "            kxx = torch.unsqueeze(kx, dim=1).expand(-1, q_len, -1, -1)\n",
        "            qxx = torch.unsqueeze(qx, dim=2).expand(-1, -1, k_len, -1)\n",
        "            kq = torch.cat((kxx, qxx), dim=-1)  # (n_head*?, q_len, k_len, hidden_dim*2)\n",
        "            # kq = torch.unsqueeze(kx, dim=1) + torch.unsqueeze(qx, dim=2)\n",
        "            score = torch.tanh(torch.matmul(kq, self.weight))\n",
        "        elif self.score_function == 'bi_linear':\n",
        "            qw = torch.matmul(qx, self.weight)\n",
        "            kt = kx.permute(0, 2, 1)\n",
        "            score = torch.bmm(qw, kt)\n",
        "        else:\n",
        "            raise RuntimeError('invalid score_function')\n",
        "            \n",
        "        score = F.softmax(score, dim=0)\n",
        "        output = torch.bmm(score, kx)  # (n_head*?, q_len, hidden_dim)\n",
        "        output = torch.cat(torch.split(output, mb_size, dim=0), dim=-1)  # (?, q_len, n_head*hidden_dim)\n",
        "        output = self.proj(output)  # (?, q_len, out_dim)\n",
        "        output = self.dropout(output)\n",
        "        return output, score\n",
        "    \n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "\n",
        "    def __init__(self, D_m, D_e, D_h, n_classes=7, dropout=0.5, attention=False):\n",
        "        \n",
        "        super(LSTMModel, self).__init__()\n",
        "        \n",
        "        self.dropout   = nn.Dropout(dropout)\n",
        "        self.attention = attention\n",
        "        self.lstm = nn.LSTM(input_size=D_m, hidden_size=D_e, num_layers=2, bidirectional=True, dropout=dropout)\n",
        "        \n",
        "        if self.attention:\n",
        "            self.matchatt = MatchingAttention(2*D_e, 2*D_e, att_type='general2')\n",
        "        \n",
        "        self.linear = nn.Linear(2*D_e, D_h)\n",
        "        self.smax_fc = nn.Linear(D_h, n_classes)\n",
        "\n",
        "    def forward(self, U, qmask, umask):\n",
        "        \"\"\"\n",
        "        U -> seq_len, batch, D_m\n",
        "        qmask -> seq_len, batch, party\n",
        "        \"\"\"\n",
        "        emotions, hidden = self.lstm(U)\n",
        "        alpha, alpha_f, alpha_b = [], [], []\n",
        "        \n",
        "        if self.attention:\n",
        "            att_emotions = []\n",
        "            alpha = []\n",
        "            for t in emotions:\n",
        "                att_em, alpha_ = self.matchatt(emotions, t, mask=umask)\n",
        "                att_emotions.append(att_em.unsqueeze(0))\n",
        "                alpha.append(alpha_[:, 0, :])\n",
        "            att_emotions = torch.cat(att_emotions, dim=0)\n",
        "            hidden = F.relu(self.linear(att_emotions))\n",
        "        else:\n",
        "            hidden = F.relu(self.linear(emotions))\n",
        "        \n",
        "        hidden = self.dropout(hidden)\n",
        "        log_prob = F.log_softmax(self.smax_fc(hidden), 2)\n",
        "        return log_prob, alpha, alpha_f, alpha_b\n",
        "    "
      ],
      "metadata": {
        "id": "U5PLioA58VMS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataloader\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "class IEMOCAPDataset(Dataset):\n",
        "\n",
        "    def __init__(self, path, train=True):\n",
        "        self.videoIDs, self.videoSpeakers, self.videoLabels, self.videoText,\\\n",
        "        self.videoAudio, self.videoVisual, self.videoSentence, self.trainVid,\\\n",
        "        self.testVid = pickle.load(open(path, 'rb'), encoding='latin1')\n",
        "        '''\n",
        "        label index mapping = {'happy':0, 'sad':1, 'neutral':2, 'angry':3, 'excioted':4, 'frustrated':5}\n",
        "        '''\n",
        "        self.keys = [x for x in (self.trainVid if train else self.testVid)]\n",
        "\n",
        "        self.len = len(self.keys)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        vid = self.keys[index]\n",
        "        return torch.FloatTensor(self.videoText[vid]),\\\n",
        "               torch.FloatTensor(self.videoVisual[vid]),\\\n",
        "               torch.FloatTensor(self.videoAudio[vid]),\\\n",
        "               torch.FloatTensor([[1,0] if x=='M' else [0,1] for x in\\\n",
        "                                  self.videoSpeakers[vid]]),\\\n",
        "               torch.FloatTensor([1]*len(self.videoLabels[vid])),\\\n",
        "               torch.LongTensor(self.videoLabels[vid]),\\\n",
        "               vid\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def collate_fn(self, data):\n",
        "        dat = pd.DataFrame(data)\n",
        "        return [pad_sequence(dat[i]) if i<4 else pad_sequence(dat[i], True) if i<6 else dat[i].tolist() for i in dat]\n",
        "\n",
        "class MELDDataset(Dataset):\n",
        "\n",
        "    def __init__(self, path, classify, train=True):\n",
        "        self.videoIDs, self.videoSpeakers, self.emotion_labels, self.videoText,\\\n",
        "        self.videoAudio, self.videoSentence, self.trainVid,\\\n",
        "        self.testVid, self.sentiment_labels = pickle.load(open(path, 'rb'))\n",
        "        \n",
        "        if classify == 'emotion':\n",
        "            self.videoLabels = self.emotion_labels\n",
        "        else:\n",
        "            self.videoLabels = self.sentiment_labels\n",
        "        '''\n",
        "        emotion_label_mapping = {'neutral': 0, 'surprise': 1, 'fear': 2, 'sadness': 3, 'joy': 4, 'disgust': 5, 'anger':6}\n",
        "        setiment_label_mapping = {'neutral': 0, 'positive': 1, 'negative': 2}\n",
        "        '''\n",
        "        self.keys = [x for x in (self.trainVid if train else self.testVid)]\n",
        "\n",
        "        self.len = len(self.keys)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        vid = self.keys[index]\n",
        "        return torch.FloatTensor(self.videoText[vid]),\\\n",
        "               torch.FloatTensor(self.videoAudio[vid]),\\\n",
        "               torch.FloatTensor(self.videoSpeakers[vid]),\\\n",
        "               torch.FloatTensor([1]*len(self.videoLabels[vid])),\\\n",
        "               torch.LongTensor(self.videoLabels[vid]),\\\n",
        "               vid\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def collate_fn(self, data):\n",
        "        dat = pd.DataFrame(data)\n",
        "        return [pad_sequence(dat[i]) if i<3 else pad_sequence(dat[i], True) if i<5 else dat[i].tolist() for i in dat]\n",
        "        "
      ],
      "metadata": {
        "id": "3zbpy3NY8yeG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed=2023):\n",
        "    print(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "weiCWx50ObS0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#IEMOCAP without attention\n",
        "import numpy as np\n",
        "import argparse, time, pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "def get_train_valid_sampler(trainset, valid=0.1):\n",
        "    size = len(trainset)\n",
        "    idx = list(range(size))\n",
        "    split = int(valid*size)\n",
        "    return SubsetRandomSampler(idx[split:]), SubsetRandomSampler(idx[:split])\n",
        "\n",
        "def get_IEMOCAP_loaders(path, batch_size=32, valid=0.1, num_workers=0, pin_memory=False):\n",
        "    trainset = IEMOCAPDataset(path=path)\n",
        "    testset = IEMOCAPDataset(path=path, train=False)\n",
        "    \n",
        "    train_sampler, valid_sampler = get_train_valid_sampler(trainset, valid)\n",
        "    train_loader = DataLoader(trainset,\n",
        "                              batch_size=batch_size,\n",
        "                              sampler=train_sampler,\n",
        "                              collate_fn=trainset.collate_fn,\n",
        "                              num_workers=num_workers,\n",
        "                              pin_memory=pin_memory)\n",
        "    valid_loader = DataLoader(trainset,\n",
        "                              batch_size=batch_size,\n",
        "                              sampler=valid_sampler,\n",
        "                              collate_fn=trainset.collate_fn,\n",
        "                              num_workers=num_workers,\n",
        "                              pin_memory=pin_memory)\n",
        "    test_loader = DataLoader(testset,\n",
        "                             batch_size=batch_size,\n",
        "                             collate_fn=testset.collate_fn,\n",
        "                             num_workers=num_workers,\n",
        "                             pin_memory=pin_memory)\n",
        "\n",
        "    return train_loader, valid_loader, test_loader\n",
        "\n",
        "def train_or_eval_model(model, loss_function, dataloader, epoch, optimizer=None, train=False):\n",
        "    losses = []\n",
        "    preds = []\n",
        "    labels = []\n",
        "    masks = []\n",
        "    alphas, alphas_f, alphas_b, vids = [], [], [], []\n",
        "    assert not train or optimizer!=None\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    for data in dataloader:\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "        \n",
        "        textf, visuf, acouf, qmask, umask, label =\\\n",
        "                [d.cuda() for d in data[:-1]] if cuda else data[:-1]\n",
        "        \n",
        "        # log_prob = model(torch.cat((textf, acouf, visuf), dim=-1), qmask, umask) \n",
        "        log_prob, alpha, alpha_f, alpha_b = model(textf, qmask, umask) \n",
        "        lp_ = log_prob.transpose(0, 1).contiguous().view(-1, log_prob.size()[2])\n",
        "        labels_ = label.view(-1) \n",
        "        loss = loss_function(lp_, labels_, umask)\n",
        "\n",
        "        pred_ = torch.argmax(lp_, 1) \n",
        "        preds.append(pred_.data.cpu().numpy())\n",
        "        labels.append(labels_.data.cpu().numpy())\n",
        "        masks.append(umask.view(-1).cpu().numpy())\n",
        "\n",
        "        losses.append(loss.item()*masks[-1].sum())\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            if args.tensorboard:\n",
        "                for param in model.named_parameters():\n",
        "                    writer.add_histogram(param[0], param[1].grad, epoch)\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            alphas += alpha\n",
        "            alphas_f += alpha_f\n",
        "            alphas_b += alpha_b\n",
        "            vids += data[-1]\n",
        "\n",
        "    if preds!=[]:\n",
        "        preds  = np.concatenate(preds)\n",
        "        labels = np.concatenate(labels)\n",
        "        masks  = np.concatenate(masks)\n",
        "    else:\n",
        "        return float('nan'), float('nan'), [], [], [], float('nan'), []\n",
        "\n",
        "    avg_loss = round(np.sum(losses)/np.sum(masks), 4)\n",
        "    avg_accuracy = round(accuracy_score(labels, preds, sample_weight=masks)*100, 2)\n",
        "    avg_fscore = round(f1_score(labels, preds, sample_weight=masks, average='weighted')*100, 2)\n",
        "    return avg_loss, avg_accuracy, labels, preds, masks, avg_fscore, [alphas, alphas_f, alphas_b, vids]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--no-cuda', action='store_true', default=False, help='does not use GPU')\n",
        "    parser.add_argument('--lr', type=float, default=0.0001, metavar='LR', help='learning rate')\n",
        "    parser.add_argument('--l2', type=float, default=0.00001, metavar='L2', help='L2 regularization weight')\n",
        "    parser.add_argument('--dropout', type=float, default=0.25, metavar='dropout', help='dropout rate')\n",
        "    parser.add_argument('--batch-size', type=int, default=32, metavar='BS', help='batch size')\n",
        "    parser.add_argument('--epochs', type=int, default=60, metavar='E', help='number of epochs')\n",
        "    parser.add_argument('--class-weight', action='store_true', default=False, help='use class weight')\n",
        "    parser.add_argument('--attention', action='store_true', default=False, help='use attention on top of lstm')\n",
        "    parser.add_argument('--tensorboard', action='store_true', default=False, help='Enables tensorboard log')\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    print(args)\n",
        "    seed_everything(seed=2023)\n",
        "    args.cuda = torch.cuda.is_available() and not args.no_cuda\n",
        "    if args.cuda:\n",
        "        print('Running on GPU')\n",
        "    else:\n",
        "        print('Running on CPU')\n",
        "\n",
        "    if args.tensorboard:\n",
        "        from tensorboardX import SummaryWriter\n",
        "        writer = SummaryWriter()\n",
        "\n",
        "    batch_size = args.batch_size\n",
        "    cuda       = args.cuda\n",
        "    n_epochs   = args.epochs\n",
        "    \n",
        "    n_classes  = 6\n",
        "    D_m = 100\n",
        "    D_e = 100\n",
        "    D_h = 100\n",
        "\n",
        "    model = LSTMModel(D_m, D_e, D_h,\n",
        "                      n_classes=n_classes,\n",
        "                      dropout=args.dropout,\n",
        "                      attention=args.attention)\n",
        "    if cuda:\n",
        "        model.cuda()\n",
        "        \n",
        "    loss_weights = torch.FloatTensor([1.0, 0.60072, 0.38066, 0.54019, 0.67924, 0.34332])\n",
        "    \n",
        "    if args.class_weight:\n",
        "        loss_function  = MaskedNLLLoss(loss_weights.cuda() if cuda else loss_weights)\n",
        "    else:\n",
        "        loss_function = MaskedNLLLoss()\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                           lr=args.lr,\n",
        "                           weight_decay=args.l2)\n",
        "\n",
        "    train_loader, valid_loader, test_loader = get_IEMOCAP_loaders('drive/MyDrive/IEMOCAP_features_raw.pkl',\n",
        "                                                                  batch_size=batch_size,\n",
        "                                                                  valid=0.0)\n",
        "\n",
        "    best_loss, best_label, best_pred, best_mask = None, None, None, None\n",
        "\n",
        "    for e in range(n_epochs):\n",
        "        start_time = time.time()\n",
        "        train_loss, train_acc, _, _, _, train_fscore, _ = train_or_eval_model(model, loss_function,\n",
        "                                               train_loader, e, optimizer, True)\n",
        "        valid_loss, valid_acc, _, _, _, val_fscore, _ = train_or_eval_model(model, loss_function, valid_loader, e)\n",
        "        test_loss, test_acc, test_label, test_pred, test_mask, test_fscore, attentions = train_or_eval_model(model, loss_function, test_loader, e)\n",
        "\n",
        "        if best_loss == None or best_loss > test_loss:\n",
        "            best_loss, best_label, best_pred, best_mask, best_attn =\\\n",
        "                    test_loss, test_label, test_pred, test_mask, attentions\n",
        "\n",
        "        if args.tensorboard:\n",
        "            writer.add_scalar('test: accuracy/loss', test_acc/test_loss, e)\n",
        "            writer.add_scalar('train: accuracy/loss', train_acc/train_loss, e)\n",
        "        print('epoch {} train_loss {} train_acc {} train_fscore {} valid_loss {} valid_acc {} val_fscore {} test_loss {} test_acc {} test_fscore {} time {}'.\\\n",
        "                format(e+1, train_loss, train_acc, train_fscore, valid_loss, valid_acc, val_fscore,\\\n",
        "                        test_loss, test_acc, test_fscore, round(time.time()-start_time, 2)))\n",
        "    if args.tensorboard:\n",
        "        writer.close()\n",
        "\n",
        "    print('Test performance..')\n",
        "    print('Loss {} F1-score {}'.format(best_loss,\n",
        "                                     round(f1_score(best_label, best_pred, sample_weight=best_mask, average='weighted')*100, 2)))\n",
        "    print(classification_report(best_label, best_pred, sample_weight=best_mask, digits=4))\n",
        "    print(confusion_matrix(best_label, best_pred, sample_weight=best_mask))"
      ],
      "metadata": {
        "id": "kYWUbLns6hWK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a066e97e-51d4-4459-a928-dfb17f22b332"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(no_cuda=False, lr=0.0001, l2=1e-05, dropout=0.25, batch_size=32, epochs=60, class_weight=False, attention=False, tensorboard=False)\n",
            "2023\n",
            "Running on GPU\n",
            "epoch 1 train_loss 1.7823 train_acc 21.64 train_fscore 11.49 valid_loss nan valid_acc nan val_fscore nan test_loss 1.7735 test_acc 22.98 test_fscore 13.32 time 0.48\n",
            "epoch 2 train_loss 1.7674 train_acc 25.46 train_fscore 14.4 valid_loss nan valid_acc nan val_fscore nan test_loss 1.766 test_acc 25.63 test_fscore 15.62 time 0.47\n",
            "epoch 3 train_loss 1.7515 train_acc 31.79 train_fscore 21.21 valid_loss nan valid_acc nan val_fscore nan test_loss 1.7583 test_acc 29.45 test_fscore 20.11 time 0.46\n",
            "epoch 4 train_loss 1.7364 train_acc 38.62 train_fscore 30.48 valid_loss nan valid_acc nan val_fscore nan test_loss 1.7502 test_acc 34.75 test_fscore 28.1 time 0.46\n",
            "epoch 5 train_loss 1.7198 train_acc 46.33 train_fscore 40.65 valid_loss nan valid_acc nan val_fscore nan test_loss 1.7415 test_acc 39.25 test_fscore 34.05 time 0.46\n",
            "epoch 6 train_loss 1.7026 train_acc 54.17 train_fscore 49.61 valid_loss nan valid_acc nan val_fscore nan test_loss 1.7316 test_acc 42.76 test_fscore 38.05 time 0.53\n",
            "epoch 7 train_loss 1.6823 train_acc 59.88 train_fscore 56.38 valid_loss nan valid_acc nan val_fscore nan test_loss 1.7204 test_acc 46.77 test_fscore 42.53 time 0.45\n",
            "epoch 8 train_loss 1.6599 train_acc 64.61 train_fscore 61.35 valid_loss nan valid_acc nan val_fscore nan test_loss 1.7075 test_acc 49.11 test_fscore 45.09 time 0.45\n",
            "epoch 9 train_loss 1.6342 train_acc 68.0 train_fscore 65.1 valid_loss nan valid_acc nan val_fscore nan test_loss 1.6925 test_acc 50.22 test_fscore 46.67 time 0.45\n",
            "epoch 10 train_loss 1.6054 train_acc 70.29 train_fscore 67.39 valid_loss nan valid_acc nan val_fscore nan test_loss 1.6752 test_acc 51.2 test_fscore 47.84 time 0.47\n",
            "epoch 11 train_loss 1.5733 train_acc 73.01 train_fscore 70.24 valid_loss nan valid_acc nan val_fscore nan test_loss 1.6552 test_acc 52.25 test_fscore 49.11 time 0.45\n",
            "epoch 12 train_loss 1.5345 train_acc 73.61 train_fscore 70.84 valid_loss nan valid_acc nan val_fscore nan test_loss 1.6322 test_acc 52.31 test_fscore 49.34 time 0.46\n",
            "epoch 13 train_loss 1.4932 train_acc 74.72 train_fscore 72.05 valid_loss nan valid_acc nan val_fscore nan test_loss 1.6057 test_acc 52.99 test_fscore 50.11 time 0.46\n",
            "epoch 14 train_loss 1.4432 train_acc 75.35 train_fscore 72.99 valid_loss nan valid_acc nan val_fscore nan test_loss 1.5758 test_acc 53.36 test_fscore 50.6 time 0.47\n",
            "epoch 15 train_loss 1.3919 train_acc 75.37 train_fscore 73.06 valid_loss nan valid_acc nan val_fscore nan test_loss 1.5427 test_acc 54.04 test_fscore 51.45 time 0.45\n",
            "epoch 16 train_loss 1.3337 train_acc 75.2 train_fscore 72.87 valid_loss nan valid_acc nan val_fscore nan test_loss 1.507 test_acc 54.1 test_fscore 51.66 time 0.49\n",
            "epoch 17 train_loss 1.2752 train_acc 75.3 train_fscore 73.12 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4691 test_acc 53.97 test_fscore 51.74 time 0.82\n",
            "epoch 18 train_loss 1.2115 train_acc 76.18 train_fscore 74.41 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4301 test_acc 54.22 test_fscore 52.12 time 1.01\n",
            "epoch 19 train_loss 1.1503 train_acc 76.18 train_fscore 74.33 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3913 test_acc 53.91 test_fscore 51.99 time 1.43\n",
            "epoch 20 train_loss 1.0898 train_acc 75.44 train_fscore 73.78 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3533 test_acc 53.73 test_fscore 51.71 time 2.12\n",
            "epoch 21 train_loss 1.0305 train_acc 75.83 train_fscore 74.25 valid_loss nan valid_acc nan val_fscore nan test_loss 1.317 test_acc 53.17 test_fscore 51.28 time 1.86\n",
            "epoch 22 train_loss 0.9748 train_acc 76.04 train_fscore 74.43 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2829 test_acc 53.85 test_fscore 52.22 time 1.26\n",
            "epoch 23 train_loss 0.9222 train_acc 76.32 train_fscore 74.95 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2517 test_acc 53.91 test_fscore 52.39 time 1.37\n",
            "epoch 24 train_loss 0.8754 train_acc 76.57 train_fscore 75.28 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2235 test_acc 54.1 test_fscore 52.67 time 0.83\n",
            "epoch 25 train_loss 0.8255 train_acc 77.19 train_fscore 76.08 valid_loss nan valid_acc nan val_fscore nan test_loss 1.1975 test_acc 54.84 test_fscore 53.56 time 1.0\n",
            "epoch 26 train_loss 0.7881 train_acc 77.87 train_fscore 76.92 valid_loss nan valid_acc nan val_fscore nan test_loss 1.1754 test_acc 55.33 test_fscore 54.21 time 1.13\n",
            "epoch 27 train_loss 0.7471 train_acc 79.14 train_fscore 78.26 valid_loss nan valid_acc nan val_fscore nan test_loss 1.1561 test_acc 55.95 test_fscore 54.99 time 1.51\n",
            "epoch 28 train_loss 0.7154 train_acc 79.69 train_fscore 79.03 valid_loss nan valid_acc nan val_fscore nan test_loss 1.1391 test_acc 56.13 test_fscore 55.23 time 0.62\n",
            "epoch 29 train_loss 0.6814 train_acc 80.19 train_fscore 79.61 valid_loss nan valid_acc nan val_fscore nan test_loss 1.1249 test_acc 56.87 test_fscore 55.96 time 0.46\n",
            "epoch 30 train_loss 0.6533 train_acc 81.05 train_fscore 80.62 valid_loss nan valid_acc nan val_fscore nan test_loss 1.1132 test_acc 56.87 test_fscore 56.06 time 0.45\n",
            "epoch 31 train_loss 0.6189 train_acc 82.32 train_fscore 82.03 valid_loss nan valid_acc nan val_fscore nan test_loss 1.1045 test_acc 57.67 test_fscore 56.96 time 0.46\n",
            "epoch 32 train_loss 0.5951 train_acc 83.12 train_fscore 82.88 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0978 test_acc 57.67 test_fscore 57.06 time 0.45\n",
            "epoch 33 train_loss 0.5676 train_acc 84.35 train_fscore 84.2 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0931 test_acc 57.86 test_fscore 57.29 time 0.48\n",
            "epoch 34 train_loss 0.5405 train_acc 85.23 train_fscore 85.1 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0907 test_acc 57.67 test_fscore 57.14 time 0.71\n",
            "epoch 35 train_loss 0.5228 train_acc 85.52 train_fscore 85.45 valid_loss nan valid_acc nan val_fscore nan test_loss 1.09 test_acc 58.1 test_fscore 57.68 time 0.75\n",
            "epoch 36 train_loss 0.4981 train_acc 86.06 train_fscore 85.99 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0903 test_acc 58.29 test_fscore 57.98 time 0.76\n",
            "epoch 37 train_loss 0.4799 train_acc 87.07 train_fscore 87.03 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0911 test_acc 58.29 test_fscore 58.02 time 0.77\n",
            "epoch 38 train_loss 0.4593 train_acc 87.5 train_fscore 87.47 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0924 test_acc 58.41 test_fscore 58.17 time 0.76\n",
            "epoch 39 train_loss 0.4405 train_acc 87.83 train_fscore 87.81 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0945 test_acc 58.29 test_fscore 58.1 time 0.54\n",
            "epoch 40 train_loss 0.4288 train_acc 88.24 train_fscore 88.23 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0979 test_acc 58.66 test_fscore 58.48 time 0.44\n",
            "epoch 41 train_loss 0.4086 train_acc 88.54 train_fscore 88.52 valid_loss nan valid_acc nan val_fscore nan test_loss 1.103 test_acc 58.29 test_fscore 58.11 time 0.44\n",
            "epoch 42 train_loss 0.3987 train_acc 88.9 train_fscore 88.88 valid_loss nan valid_acc nan val_fscore nan test_loss 1.1104 test_acc 58.6 test_fscore 58.38 time 0.45\n",
            "epoch 43 train_loss 0.3852 train_acc 89.29 train_fscore 89.29 valid_loss nan valid_acc nan val_fscore nan test_loss 1.1154 test_acc 58.72 test_fscore 58.52 time 0.44\n",
            "epoch 44 train_loss 0.3715 train_acc 89.91 train_fscore 89.91 valid_loss nan valid_acc nan val_fscore nan test_loss 1.1211 test_acc 58.29 test_fscore 58.11 time 0.45\n",
            "epoch 45 train_loss 0.36 train_acc 90.19 train_fscore 90.19 valid_loss nan valid_acc nan val_fscore nan test_loss 1.1278 test_acc 57.86 test_fscore 57.68 time 0.44\n",
            "epoch 46 train_loss 0.3515 train_acc 90.22 train_fscore 90.22 valid_loss nan valid_acc nan val_fscore nan test_loss 1.1357 test_acc 57.86 test_fscore 57.66 time 0.46\n",
            "epoch 47 train_loss 0.3397 train_acc 90.83 train_fscore 90.82 valid_loss nan valid_acc nan val_fscore nan test_loss 1.1437 test_acc 57.73 test_fscore 57.55 time 0.44\n",
            "epoch 48 train_loss 0.3319 train_acc 90.5 train_fscore 90.49 valid_loss nan valid_acc nan val_fscore nan test_loss 1.1504 test_acc 57.79 test_fscore 57.66 time 0.46\n",
            "epoch 49 train_loss 0.324 train_acc 90.84 train_fscore 90.84 valid_loss nan valid_acc nan val_fscore nan test_loss 1.1588 test_acc 57.86 test_fscore 57.74 time 0.45\n",
            "epoch 50 train_loss 0.3172 train_acc 91.38 train_fscore 91.38 valid_loss nan valid_acc nan val_fscore nan test_loss 1.1639 test_acc 57.86 test_fscore 57.73 time 0.44\n",
            "epoch 51 train_loss 0.303 train_acc 91.62 train_fscore 91.62 valid_loss nan valid_acc nan val_fscore nan test_loss 1.1702 test_acc 57.98 test_fscore 57.85 time 0.45\n",
            "epoch 52 train_loss 0.302 train_acc 91.48 train_fscore 91.48 valid_loss nan valid_acc nan val_fscore nan test_loss 1.1752 test_acc 58.23 test_fscore 58.08 time 0.44\n",
            "epoch 53 train_loss 0.2984 train_acc 91.74 train_fscore 91.74 valid_loss nan valid_acc nan val_fscore nan test_loss 1.1821 test_acc 58.04 test_fscore 57.89 time 0.46\n",
            "epoch 54 train_loss 0.2906 train_acc 91.93 train_fscore 91.93 valid_loss nan valid_acc nan val_fscore nan test_loss 1.1894 test_acc 58.04 test_fscore 57.89 time 0.44\n",
            "epoch 55 train_loss 0.2827 train_acc 92.2 train_fscore 92.2 valid_loss nan valid_acc nan val_fscore nan test_loss 1.1969 test_acc 58.04 test_fscore 57.88 time 0.45\n",
            "epoch 56 train_loss 0.2804 train_acc 92.0 train_fscore 92.0 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2041 test_acc 57.86 test_fscore 57.69 time 0.44\n",
            "epoch 57 train_loss 0.2709 train_acc 92.38 train_fscore 92.38 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2095 test_acc 57.98 test_fscore 57.81 time 0.45\n",
            "epoch 58 train_loss 0.2658 train_acc 92.74 train_fscore 92.74 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2168 test_acc 57.79 test_fscore 57.61 time 0.45\n",
            "epoch 59 train_loss 0.2627 train_acc 92.72 train_fscore 92.72 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2238 test_acc 57.86 test_fscore 57.72 time 0.45\n",
            "epoch 60 train_loss 0.2592 train_acc 92.75 train_fscore 92.75 valid_loss nan valid_acc nan val_fscore nan test_loss 1.232 test_acc 57.92 test_fscore 57.75 time 0.44\n",
            "Test performance..\n",
            "Loss 1.09 F1-score 57.68\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5000    0.2014    0.2871     144.0\n",
            "           1     0.8232    0.6082    0.6995     245.0\n",
            "           2     0.4667    0.6198    0.5324     384.0\n",
            "           3     0.6645    0.5941    0.6273     170.0\n",
            "           4     0.6856    0.5251    0.5947     299.0\n",
            "           5     0.5456    0.7060    0.6156     381.0\n",
            "\n",
            "    accuracy                         0.5810    1623.0\n",
            "   macro avg     0.6143    0.5424    0.5594    1623.0\n",
            "weighted avg     0.6030    0.5810    0.5768    1623.0\n",
            "\n",
            "[[ 29.   5.  48.   0.  55.   7.]\n",
            " [  2. 149.  39.   3.   0.  52.]\n",
            " [  4.  12. 238.  16.  13. 101.]\n",
            " [  0.   1.  10. 101.   0.  58.]\n",
            " [ 23.  11. 102.   0. 157.   6.]\n",
            " [  0.   3.  73.  32.   4. 269.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MELD without attention\n",
        "import numpy as np\n",
        "import argparse, time, pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "\n",
        "def get_train_valid_sampler(trainset, valid=0.1):\n",
        "    size = len(trainset)\n",
        "    idx = list(range(size))\n",
        "    split = int(valid*size)\n",
        "    return SubsetRandomSampler(idx[split:]), SubsetRandomSampler(idx[:split])\n",
        "\n",
        "def get_MELD_loaders(path, batch_size=32, valid=0.1, classify='emotion', num_workers=0, pin_memory=False):\n",
        "    trainset = MELDDataset(path, classify)\n",
        "    testset = MELDDataset(path, classify, train=False)\n",
        "    \n",
        "    train_sampler, valid_sampler = get_train_valid_sampler(trainset, valid)\n",
        "    train_loader = DataLoader(trainset,\n",
        "                              batch_size=batch_size,\n",
        "                              sampler=train_sampler,\n",
        "                              collate_fn=trainset.collate_fn,\n",
        "                              num_workers=num_workers,\n",
        "                              pin_memory=pin_memory)\n",
        "    valid_loader = DataLoader(trainset,\n",
        "                              batch_size=batch_size,\n",
        "                              sampler=valid_sampler,\n",
        "                              collate_fn=trainset.collate_fn,\n",
        "                              num_workers=num_workers,\n",
        "                              pin_memory=pin_memory)  \n",
        "    test_loader = DataLoader(testset,\n",
        "                             batch_size=batch_size,\n",
        "                             collate_fn=testset.collate_fn,\n",
        "                             num_workers=num_workers,\n",
        "                             pin_memory=pin_memory)\n",
        "\n",
        "    return train_loader, valid_loader, test_loader\n",
        "\n",
        "def train_or_eval_model(model, loss_function, dataloader, epoch, optimizer=None, train=False):\n",
        "    losses = []\n",
        "    preds = []\n",
        "    labels = []\n",
        "    masks = []\n",
        "    alphas, alphas_f, alphas_b, vids = [], [], [], []\n",
        "    assert not train or optimizer!=None\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    for data in dataloader:\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "        \n",
        "        textf, acouf, qmask, umask, label =\\\n",
        "                [d.cuda() for d in data[:-1]] if cuda else data[:-1]\n",
        "        \n",
        "        # log_prob = model(torch.cat((textf, acouf), dim=-1), qmask, umask) \n",
        "        log_prob, alpha, alpha_f, alpha_b = model(textf, qmask, umask) \n",
        "        lp_ = log_prob.transpose(0, 1).contiguous().view(-1, log_prob.size()[2])\n",
        "        labels_ = label.view(-1)\n",
        "        loss = loss_function(lp_, labels_, umask)\n",
        "\n",
        "        pred_ = torch.argmax(lp_, 1)\n",
        "        preds.append(pred_.data.cpu().numpy())\n",
        "        labels.append(labels_.data.cpu().numpy())\n",
        "        masks.append(umask.view(-1).cpu().numpy())\n",
        "\n",
        "        losses.append(loss.item()*masks[-1].sum())\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            if args.tensorboard:\n",
        "                for param in model.named_parameters():\n",
        "                    writer.add_histogram(param[0], param[1].grad, epoch)\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            alphas += alpha\n",
        "            alphas_f += alpha_f\n",
        "            alphas_b += alpha_b\n",
        "            vids += data[-1]\n",
        "\n",
        "    if preds!=[]:\n",
        "        preds  = np.concatenate(preds)\n",
        "        labels = np.concatenate(labels)\n",
        "        masks  = np.concatenate(masks)\n",
        "    else:\n",
        "        return float('nan'), float('nan'), [], [], [], float('nan'), []\n",
        "\n",
        "    avg_loss = round(np.sum(losses)/np.sum(masks), 4)\n",
        "    avg_accuracy = round(accuracy_score(labels, preds, sample_weight=masks)*100, 2)\n",
        "    avg_fscore = round(f1_score(labels, preds, sample_weight=masks, average='weighted')*100, 2)\n",
        "    return avg_loss, avg_accuracy, labels, preds, masks, avg_fscore, [alphas, alphas_f, alphas_b, vids]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--no-cuda', action='store_true', default=False, help='does not use GPU')\n",
        "    parser.add_argument('--lr', type=float, default=0.0001, metavar='LR', help='learning rate')\n",
        "    parser.add_argument('--l2', type=float, default=0.00001, metavar='L2', help='L2 regularization weight')\n",
        "    parser.add_argument('--dropout', type=float, default=0.25, metavar='dropout', help='dropout rate')\n",
        "    parser.add_argument('--batch-size', type=int, default=32, metavar='BS', help='batch size')\n",
        "    parser.add_argument('--epochs', type=int, default=60, metavar='E', help='number of epochs')\n",
        "    parser.add_argument('--attention', action='store_true', default=False, help='use attention on top of lstm')\n",
        "    parser.add_argument('--tensorboard', action='store_true', default=False, help='Enables tensorboard log')\n",
        "    parser.add_argument('--classify', default='emotion', help='classify emotion or sentiment')\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    print(args)\n",
        "    seed_everything(seed=2023)\n",
        "    args.cuda = torch.cuda.is_available() and not args.no_cuda\n",
        "    if args.cuda:\n",
        "        print('Running on GPU')\n",
        "    else:\n",
        "        print('Running on CPU')\n",
        "\n",
        "    if args.tensorboard:\n",
        "        from tensorboardX import SummaryWriter\n",
        "        writer = SummaryWriter()\n",
        "\n",
        "    batch_size = args.batch_size\n",
        "    cuda       = args.cuda\n",
        "    n_epochs   = args.epochs\n",
        "    \n",
        "    if args.classify == 'emotion':\n",
        "        n_classes  = 7\n",
        "    elif args.classify == 'sentiment':\n",
        "        n_classes  = 3\n",
        "\n",
        "    D_m = 600\n",
        "    D_e = 100\n",
        "    D_h = 100\n",
        "\n",
        "    model = LSTMModel(D_m, D_e, D_h,\n",
        "                      n_classes=n_classes,\n",
        "                      dropout=args.dropout,\n",
        "                      attention=args.attention)\n",
        "    if cuda:\n",
        "        model.cuda()\n",
        "        \n",
        "    loss_function = MaskedNLLLoss()\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                           lr=args.lr,\n",
        "                           weight_decay=args.l2)\n",
        "    \n",
        "    train_loader, valid_loader, test_loader = get_MELD_loaders('drive/MyDrive/MELD_features_raw.pkl',\n",
        "                                                                batch_size=batch_size,\n",
        "                                                                valid=0.0,\n",
        "                                                                classify=args.classify)\n",
        "\n",
        "    best_fscore, best_loss, best_label, best_pred, best_mask = None, None, None, None, None\n",
        "\n",
        "    for e in range(n_epochs):\n",
        "        start_time = time.time()\n",
        "        train_loss, train_acc, _, _, _, train_fscore, _ = train_or_eval_model(model, loss_function,\n",
        "                                               train_loader, e, optimizer, True)\n",
        "        valid_loss, valid_acc, _, _, _, val_fscore, _ = train_or_eval_model(model, loss_function, valid_loader, e)\n",
        "        test_loss, test_acc, test_label, test_pred, test_mask, test_fscore, attentions = train_or_eval_model(model, loss_function, test_loader, e)\n",
        "\n",
        "        if best_fscore == None or best_fscore < test_fscore:\n",
        "            best_fscore, best_loss, best_label, best_pred, best_mask, best_attn =\\\n",
        "                test_fscore, test_loss, test_label, test_pred, test_mask, attentions\n",
        "\n",
        "        if args.tensorboard:\n",
        "            writer.add_scalar('test: accuracy/loss', test_acc/test_loss, e)\n",
        "            writer.add_scalar('train: accuracy/loss', train_acc/train_loss, e)\n",
        "        print('epoch {} train_loss {} train_acc {} train_fscore {} valid_loss {} valid_acc {} val_fscore {} test_loss {} test_acc {} test_fscore {} time {}'.\\\n",
        "                format(e+1, train_loss, train_acc, train_fscore, valid_loss, valid_acc, val_fscore,\\\n",
        "                        test_loss, test_acc, test_fscore, round(time.time()-start_time, 2)))\n",
        "    if args.tensorboard:\n",
        "        writer.close()\n",
        "\n",
        "    print('Test performance..')\n",
        "    print('Loss {} F1-score {}'.format(best_loss,\n",
        "                                     round(f1_score(best_label, best_pred, sample_weight=best_mask, average='weighted')*100, 2)))\n",
        "    print(classification_report(best_label, best_pred, sample_weight=best_mask, digits=4))\n",
        "    print(confusion_matrix(best_label, best_pred, sample_weight=best_mask))"
      ],
      "metadata": {
        "id": "GOGQXFlF6hYZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "141d23a2-041a-416f-995b-29e2d969fe83"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(no_cuda=False, lr=0.0001, l2=1e-05, dropout=0.25, batch_size=32, epochs=60, attention=False, tensorboard=False, classify='emotion')\n",
            "2023\n",
            "Running on GPU\n",
            "epoch 1 train_loss 1.8368 train_acc 41.43 train_fscore 30.33 valid_loss nan valid_acc nan val_fscore nan test_loss 1.7023 test_acc 48.12 test_fscore 31.27 time 0.56\n",
            "epoch 2 train_loss 1.61 train_acc 46.68 train_fscore 29.71 valid_loss nan valid_acc nan val_fscore nan test_loss 1.5444 test_acc 48.12 test_fscore 31.27 time 0.52\n",
            "epoch 3 train_loss 1.5079 train_acc 46.77 train_fscore 30.0 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4607 test_acc 48.16 test_fscore 31.49 time 0.53\n",
            "epoch 4 train_loss 1.4064 train_acc 48.82 train_fscore 35.63 valid_loss nan valid_acc nan val_fscore nan test_loss 1.391 test_acc 51.38 test_fscore 39.49 time 0.52\n",
            "epoch 5 train_loss 1.3056 train_acc 54.29 train_fscore 44.44 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3309 test_acc 54.52 test_fscore 45.39 time 0.57\n",
            "epoch 6 train_loss 1.2064 train_acc 58.39 train_fscore 50.09 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2945 test_acc 57.09 test_fscore 49.48 time 0.59\n",
            "epoch 7 train_loss 1.1486 train_acc 60.19 train_fscore 53.89 valid_loss nan valid_acc nan val_fscore nan test_loss 1.296 test_acc 56.86 test_fscore 50.28 time 0.47\n",
            "epoch 8 train_loss 1.1172 train_acc 61.95 train_fscore 56.43 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2928 test_acc 57.28 test_fscore 51.77 time 0.38\n",
            "epoch 9 train_loss 1.098 train_acc 63.09 train_fscore 58.26 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2913 test_acc 57.43 test_fscore 52.94 time 0.37\n",
            "epoch 10 train_loss 1.0807 train_acc 64.44 train_fscore 60.01 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2778 test_acc 57.85 test_fscore 53.77 time 0.39\n",
            "epoch 11 train_loss 1.0618 train_acc 65.16 train_fscore 60.75 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2744 test_acc 58.2 test_fscore 54.07 time 0.37\n",
            "epoch 12 train_loss 1.0458 train_acc 65.76 train_fscore 61.63 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2675 test_acc 58.51 test_fscore 54.03 time 0.37\n",
            "epoch 13 train_loss 1.0356 train_acc 66.29 train_fscore 62.19 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2613 test_acc 58.97 test_fscore 54.57 time 0.36\n",
            "epoch 14 train_loss 1.0246 train_acc 66.25 train_fscore 62.14 valid_loss nan valid_acc nan val_fscore nan test_loss 1.259 test_acc 58.97 test_fscore 54.8 time 0.36\n",
            "epoch 15 train_loss 1.0177 train_acc 66.73 train_fscore 62.6 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2486 test_acc 58.97 test_fscore 55.19 time 0.36\n",
            "epoch 16 train_loss 1.0065 train_acc 66.77 train_fscore 62.71 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2489 test_acc 59.27 test_fscore 55.3 time 0.36\n",
            "epoch 17 train_loss 1.0043 train_acc 67.24 train_fscore 63.27 valid_loss nan valid_acc nan val_fscore nan test_loss 1.25 test_acc 58.51 test_fscore 54.79 time 0.38\n",
            "epoch 18 train_loss 0.9977 train_acc 67.52 train_fscore 63.52 valid_loss nan valid_acc nan val_fscore nan test_loss 1.243 test_acc 59.46 test_fscore 55.89 time 0.36\n",
            "epoch 19 train_loss 0.9945 train_acc 67.31 train_fscore 63.58 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2448 test_acc 59.2 test_fscore 55.58 time 0.37\n",
            "epoch 20 train_loss 0.9887 train_acc 67.61 train_fscore 63.93 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2433 test_acc 58.93 test_fscore 55.75 time 0.37\n",
            "epoch 21 train_loss 0.992 train_acc 67.64 train_fscore 64.08 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2374 test_acc 59.12 test_fscore 55.87 time 0.37\n",
            "epoch 22 train_loss 0.9834 train_acc 67.81 train_fscore 64.17 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2462 test_acc 59.43 test_fscore 55.8 time 0.36\n",
            "epoch 23 train_loss 0.9797 train_acc 68.22 train_fscore 64.78 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2413 test_acc 58.62 test_fscore 55.91 time 0.37\n",
            "epoch 24 train_loss 0.9837 train_acc 67.99 train_fscore 64.56 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2502 test_acc 58.89 test_fscore 55.21 time 0.36\n",
            "epoch 25 train_loss 0.9782 train_acc 68.06 train_fscore 64.61 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2463 test_acc 58.93 test_fscore 55.45 time 0.35\n",
            "epoch 26 train_loss 0.978 train_acc 68.28 train_fscore 64.76 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2451 test_acc 59.23 test_fscore 55.81 time 0.36\n",
            "epoch 27 train_loss 0.9768 train_acc 68.32 train_fscore 65.0 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2353 test_acc 58.85 test_fscore 56.23 time 0.37\n",
            "epoch 28 train_loss 0.9735 train_acc 68.14 train_fscore 64.94 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2383 test_acc 59.58 test_fscore 56.38 time 0.37\n",
            "epoch 29 train_loss 0.9717 train_acc 68.48 train_fscore 65.11 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2415 test_acc 59.5 test_fscore 56.22 time 0.37\n",
            "epoch 30 train_loss 0.9732 train_acc 68.53 train_fscore 65.28 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2481 test_acc 59.39 test_fscore 56.07 time 0.37\n",
            "epoch 31 train_loss 0.973 train_acc 68.39 train_fscore 65.16 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2383 test_acc 58.77 test_fscore 55.88 time 0.37\n",
            "epoch 32 train_loss 0.9712 train_acc 68.22 train_fscore 64.84 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2387 test_acc 59.54 test_fscore 56.42 time 0.37\n",
            "epoch 33 train_loss 0.9683 train_acc 68.26 train_fscore 65.14 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2359 test_acc 59.16 test_fscore 56.51 time 0.37\n",
            "epoch 34 train_loss 0.9662 train_acc 68.81 train_fscore 65.78 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2316 test_acc 58.47 test_fscore 56.27 time 0.42\n",
            "epoch 35 train_loss 0.9644 train_acc 68.74 train_fscore 65.71 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2551 test_acc 59.54 test_fscore 56.06 time 0.5\n",
            "epoch 36 train_loss 0.9613 train_acc 68.74 train_fscore 65.68 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2386 test_acc 59.04 test_fscore 56.38 time 0.5\n",
            "epoch 37 train_loss 0.964 train_acc 68.72 train_fscore 65.77 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2428 test_acc 59.5 test_fscore 56.52 time 0.5\n",
            "epoch 38 train_loss 0.963 train_acc 68.8 train_fscore 65.82 valid_loss nan valid_acc nan val_fscore nan test_loss 1.237 test_acc 59.81 test_fscore 56.92 time 0.52\n",
            "epoch 39 train_loss 0.9599 train_acc 68.76 train_fscore 65.74 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2496 test_acc 59.62 test_fscore 56.2 time 0.51\n",
            "epoch 40 train_loss 0.9558 train_acc 68.8 train_fscore 65.84 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2449 test_acc 59.23 test_fscore 56.41 time 0.52\n",
            "epoch 41 train_loss 0.9551 train_acc 68.73 train_fscore 65.87 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2398 test_acc 59.12 test_fscore 56.26 time 0.54\n",
            "epoch 42 train_loss 0.9558 train_acc 69.0 train_fscore 66.1 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2363 test_acc 59.08 test_fscore 56.49 time 0.39\n",
            "epoch 43 train_loss 0.9511 train_acc 69.32 train_fscore 66.4 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2384 test_acc 59.04 test_fscore 56.62 time 0.38\n",
            "epoch 44 train_loss 0.9524 train_acc 69.03 train_fscore 66.04 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2341 test_acc 59.0 test_fscore 56.81 time 0.37\n",
            "epoch 45 train_loss 0.9519 train_acc 69.22 train_fscore 66.29 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2455 test_acc 59.5 test_fscore 56.42 time 0.36\n",
            "epoch 46 train_loss 0.9478 train_acc 69.21 train_fscore 66.49 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2341 test_acc 59.39 test_fscore 57.0 time 0.37\n",
            "epoch 47 train_loss 0.9507 train_acc 68.9 train_fscore 66.1 valid_loss nan valid_acc nan val_fscore nan test_loss 1.233 test_acc 58.7 test_fscore 56.64 time 0.35\n",
            "epoch 48 train_loss 0.9455 train_acc 69.03 train_fscore 66.27 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2411 test_acc 59.43 test_fscore 56.69 time 0.36\n",
            "epoch 49 train_loss 0.9435 train_acc 69.28 train_fscore 66.39 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2357 test_acc 58.97 test_fscore 56.77 time 0.36\n",
            "epoch 50 train_loss 0.9456 train_acc 69.11 train_fscore 66.39 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2389 test_acc 59.08 test_fscore 56.45 time 0.36\n",
            "epoch 51 train_loss 0.942 train_acc 69.28 train_fscore 66.56 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2439 test_acc 59.04 test_fscore 56.34 time 0.36\n",
            "epoch 52 train_loss 0.9425 train_acc 69.32 train_fscore 66.57 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2499 test_acc 58.89 test_fscore 56.26 time 0.37\n",
            "epoch 53 train_loss 0.9415 train_acc 69.21 train_fscore 66.55 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2435 test_acc 59.66 test_fscore 56.97 time 0.37\n",
            "epoch 54 train_loss 0.9398 train_acc 69.14 train_fscore 66.32 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2391 test_acc 58.85 test_fscore 56.71 time 0.38\n",
            "epoch 55 train_loss 0.9379 train_acc 69.41 train_fscore 66.72 valid_loss nan valid_acc nan val_fscore nan test_loss 1.242 test_acc 59.16 test_fscore 56.75 time 0.37\n",
            "epoch 56 train_loss 0.9399 train_acc 69.24 train_fscore 66.49 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2353 test_acc 57.59 test_fscore 56.15 time 0.36\n",
            "epoch 57 train_loss 0.9382 train_acc 69.54 train_fscore 66.88 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2365 test_acc 59.27 test_fscore 56.68 time 0.38\n",
            "epoch 58 train_loss 0.9325 train_acc 69.64 train_fscore 67.01 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2392 test_acc 58.85 test_fscore 56.81 time 0.37\n",
            "epoch 59 train_loss 0.9348 train_acc 69.44 train_fscore 66.81 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2341 test_acc 59.35 test_fscore 56.89 time 0.38\n",
            "epoch 60 train_loss 0.9315 train_acc 69.49 train_fscore 66.9 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2536 test_acc 59.43 test_fscore 56.5 time 0.37\n",
            "Test performance..\n",
            "Loss 1.2341 F1-score 57.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7207    0.8033    0.7598    1256.0\n",
            "           1     0.4633    0.5160    0.4882     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2857    0.1827    0.2229     208.0\n",
            "           4     0.4823    0.5746    0.5244     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4456    0.3681    0.4032     345.0\n",
            "\n",
            "    accuracy                         0.5939    2610.0\n",
            "   macro avg     0.3425    0.3493    0.3426    2610.0\n",
            "weighted avg     0.5527    0.5939    0.5700    2610.0\n",
            "\n",
            "[[1.009e+03 5.800e+01 0.000e+00 5.700e+01 1.040e+02 0.000e+00 2.800e+01]\n",
            " [6.300e+01 1.450e+02 0.000e+00 3.000e+00 4.100e+01 0.000e+00 2.900e+01]\n",
            " [2.700e+01 4.000e+00 0.000e+00 1.000e+00 7.000e+00 0.000e+00 1.100e+01]\n",
            " [1.010e+02 1.500e+01 0.000e+00 3.800e+01 1.900e+01 0.000e+00 3.500e+01]\n",
            " [9.700e+01 3.200e+01 0.000e+00 5.000e+00 2.310e+02 0.000e+00 3.700e+01]\n",
            " [2.700e+01 1.100e+01 0.000e+00 4.000e+00 8.000e+00 0.000e+00 1.800e+01]\n",
            " [7.600e+01 4.800e+01 0.000e+00 2.500e+01 6.900e+01 0.000e+00 1.270e+02]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#IEMOCAP with matching attention\n",
        "import numpy as np\n",
        "import argparse, time, pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "def get_train_valid_sampler(trainset, valid=0.1):\n",
        "    size = len(trainset)\n",
        "    idx = list(range(size))\n",
        "    split = int(valid*size)\n",
        "    return SubsetRandomSampler(idx[split:]), SubsetRandomSampler(idx[:split])\n",
        "\n",
        "def get_IEMOCAP_loaders(path, batch_size=32, valid=0.1, num_workers=0, pin_memory=False):\n",
        "    trainset = IEMOCAPDataset(path=path)\n",
        "    testset = IEMOCAPDataset(path=path, train=False)\n",
        "    \n",
        "    train_sampler, valid_sampler = get_train_valid_sampler(trainset, valid)\n",
        "    train_loader = DataLoader(trainset,\n",
        "                              batch_size=batch_size,\n",
        "                              sampler=train_sampler,\n",
        "                              collate_fn=trainset.collate_fn,\n",
        "                              num_workers=num_workers,\n",
        "                              pin_memory=pin_memory)\n",
        "    valid_loader = DataLoader(trainset,\n",
        "                              batch_size=batch_size,\n",
        "                              sampler=valid_sampler,\n",
        "                              collate_fn=trainset.collate_fn,\n",
        "                              num_workers=num_workers,\n",
        "                              pin_memory=pin_memory)\n",
        "    test_loader = DataLoader(testset,\n",
        "                             batch_size=batch_size,\n",
        "                             collate_fn=testset.collate_fn,\n",
        "                             num_workers=num_workers,\n",
        "                             pin_memory=pin_memory)\n",
        "\n",
        "    return train_loader, valid_loader, test_loader\n",
        "\n",
        "def train_or_eval_model(model, loss_function, dataloader, epoch, optimizer=None, train=False):\n",
        "    losses = []\n",
        "    preds = []\n",
        "    labels = []\n",
        "    masks = []\n",
        "    alphas, alphas_f, alphas_b, vids = [], [], [], []\n",
        "    assert not train or optimizer!=None\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    for data in dataloader:\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "        \n",
        "        textf, visuf, acouf, qmask, umask, label =\\\n",
        "                [d.cuda() for d in data[:-1]] if cuda else data[:-1]\n",
        "        \n",
        "        # log_prob = model(torch.cat((textf, acouf, visuf), dim=-1), qmask, umask) \n",
        "        log_prob, alpha, alpha_f, alpha_b = model(textf, qmask, umask) \n",
        "        lp_ = log_prob.transpose(0, 1).contiguous().view(-1, log_prob.size()[2])\n",
        "        labels_ = label.view(-1) \n",
        "        loss = loss_function(lp_, labels_, umask)\n",
        "\n",
        "        pred_ = torch.argmax(lp_, 1) \n",
        "        preds.append(pred_.data.cpu().numpy())\n",
        "        labels.append(labels_.data.cpu().numpy())\n",
        "        masks.append(umask.view(-1).cpu().numpy())\n",
        "\n",
        "        losses.append(loss.item()*masks[-1].sum())\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            if args.tensorboard:\n",
        "                for param in model.named_parameters():\n",
        "                    writer.add_histogram(param[0], param[1].grad, epoch)\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            alphas += alpha\n",
        "            alphas_f += alpha_f\n",
        "            alphas_b += alpha_b\n",
        "            vids += data[-1]\n",
        "\n",
        "    if preds!=[]:\n",
        "        preds  = np.concatenate(preds)\n",
        "        labels = np.concatenate(labels)\n",
        "        masks  = np.concatenate(masks)\n",
        "    else:\n",
        "        return float('nan'), float('nan'), [], [], [], float('nan'), []\n",
        "\n",
        "    avg_loss = round(np.sum(losses)/np.sum(masks), 4)\n",
        "    avg_accuracy = round(accuracy_score(labels, preds, sample_weight=masks)*100, 2)\n",
        "    avg_fscore = round(f1_score(labels, preds, sample_weight=masks, average='weighted')*100, 2)\n",
        "    return avg_loss, avg_accuracy, labels, preds, masks, avg_fscore, [alphas, alphas_f, alphas_b, vids]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--no-cuda', action='store_true', default=False, help='does not use GPU')\n",
        "    parser.add_argument('--lr', type=float, default=0.0001, metavar='LR', help='learning rate')\n",
        "    parser.add_argument('--l2', type=float, default=0.00001, metavar='L2', help='L2 regularization weight')\n",
        "    parser.add_argument('--dropout', type=float, default=0.25, metavar='dropout', help='dropout rate')\n",
        "    parser.add_argument('--batch-size', type=int, default=32, metavar='BS', help='batch size')\n",
        "    parser.add_argument('--epochs', type=int, default=60, metavar='E', help='number of epochs')\n",
        "    parser.add_argument('--class-weight', action='store_true', default=False, help='use class weight')\n",
        "    parser.add_argument('--attention', action='store_true', default=True, help='use attention on top of lstm')\n",
        "    parser.add_argument('--tensorboard', action='store_true', default=False, help='Enables tensorboard log')\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    print(args)\n",
        "    seed_everything(seed=2023)\n",
        "    args.cuda = torch.cuda.is_available() and not args.no_cuda\n",
        "    if args.cuda:\n",
        "        print('Running on GPU')\n",
        "    else:\n",
        "        print('Running on CPU')\n",
        "\n",
        "    if args.tensorboard:\n",
        "        from tensorboardX import SummaryWriter\n",
        "        writer = SummaryWriter()\n",
        "\n",
        "    batch_size = args.batch_size\n",
        "    cuda       = args.cuda\n",
        "    n_epochs   = args.epochs\n",
        "    \n",
        "    n_classes  = 6\n",
        "    D_m = 100\n",
        "    D_e = 100\n",
        "    D_h = 100\n",
        "\n",
        "    model = LSTMModel(D_m, D_e, D_h,\n",
        "                      n_classes=n_classes,\n",
        "                      dropout=args.dropout,\n",
        "                      attention=args.attention)\n",
        "    if cuda:\n",
        "        model.cuda()\n",
        "        \n",
        "    loss_weights = torch.FloatTensor([1.0, 0.60072, 0.38066, 0.54019, 0.67924, 0.34332])\n",
        "    \n",
        "    if args.class_weight:\n",
        "        loss_function  = MaskedNLLLoss(loss_weights.cuda() if cuda else loss_weights)\n",
        "    else:\n",
        "        loss_function = MaskedNLLLoss()\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                           lr=args.lr,\n",
        "                           weight_decay=args.l2)\n",
        "\n",
        "    train_loader, valid_loader, test_loader = get_IEMOCAP_loaders('drive/MyDrive/IEMOCAP_features_raw.pkl',\n",
        "                                                                  batch_size=batch_size,\n",
        "                                                                  valid=0.0)\n",
        "\n",
        "    best_loss, best_label, best_pred, best_mask = None, None, None, None\n",
        "\n",
        "    for e in range(n_epochs):\n",
        "        start_time = time.time()\n",
        "        train_loss, train_acc, _, _, _, train_fscore, _ = train_or_eval_model(model, loss_function,\n",
        "                                               train_loader, e, optimizer, True)\n",
        "        valid_loss, valid_acc, _, _, _, val_fscore, _ = train_or_eval_model(model, loss_function, valid_loader, e)\n",
        "        test_loss, test_acc, test_label, test_pred, test_mask, test_fscore, attentions = train_or_eval_model(model, loss_function, test_loader, e)\n",
        "\n",
        "        if best_loss == None or best_loss > test_loss:\n",
        "            best_loss, best_label, best_pred, best_mask, best_attn =\\\n",
        "                    test_loss, test_label, test_pred, test_mask, attentions\n",
        "\n",
        "        if args.tensorboard:\n",
        "            writer.add_scalar('test: accuracy/loss', test_acc/test_loss, e)\n",
        "            writer.add_scalar('train: accuracy/loss', train_acc/train_loss, e)\n",
        "        print('epoch {} train_loss {} train_acc {} train_fscore {} valid_loss {} valid_acc {} val_fscore {} test_loss {} test_acc {} test_fscore {} time {}'.\\\n",
        "                format(e+1, train_loss, train_acc, train_fscore, valid_loss, valid_acc, val_fscore,\\\n",
        "                        test_loss, test_acc, test_fscore, round(time.time()-start_time, 2)))\n",
        "    if args.tensorboard:\n",
        "        writer.close()\n",
        "\n",
        "    print('Test performance..')\n",
        "    print('Loss {} F1-score {}'.format(best_loss,\n",
        "                                     round(f1_score(best_label, best_pred, sample_weight=best_mask, average='weighted')*100, 2)))\n",
        "    print(classification_report(best_label, best_pred, sample_weight=best_mask, digits=4))\n",
        "    print(confusion_matrix(best_label, best_pred, sample_weight=best_mask))"
      ],
      "metadata": {
        "id": "9rGSWh2p7n6O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55cbd2aa-432f-4f68-9383-495711899466"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(no_cuda=False, lr=0.0001, l2=1e-05, dropout=0.25, batch_size=32, epochs=60, class_weight=False, attention=True, tensorboard=False)\n",
            "2023\n",
            "Running on GPU\n",
            "epoch 1 train_loss 1.7942 train_acc 19.98 train_fscore 8.62 valid_loss nan valid_acc nan val_fscore nan test_loss 1.7897 test_acc 23.66 test_fscore 9.05 time 0.76\n",
            "epoch 2 train_loss 1.784 train_acc 22.62 train_fscore 8.93 valid_loss nan valid_acc nan val_fscore nan test_loss 1.784 test_acc 23.66 test_fscore 9.05 time 0.73\n",
            "epoch 3 train_loss 1.774 train_acc 22.98 train_fscore 9.29 valid_loss nan valid_acc nan val_fscore nan test_loss 1.7781 test_acc 23.66 test_fscore 9.05 time 0.75\n",
            "epoch 4 train_loss 1.7637 train_acc 24.13 train_fscore 11.33 valid_loss nan valid_acc nan val_fscore nan test_loss 1.7719 test_acc 23.66 test_fscore 9.05 time 1.19\n",
            "epoch 5 train_loss 1.7534 train_acc 27.13 train_fscore 16.43 valid_loss nan valid_acc nan val_fscore nan test_loss 1.7651 test_acc 23.66 test_fscore 9.05 time 1.23\n",
            "epoch 6 train_loss 1.7421 train_acc 30.81 train_fscore 22.25 valid_loss nan valid_acc nan val_fscore nan test_loss 1.7576 test_acc 24.89 test_fscore 11.58 time 1.2\n",
            "epoch 7 train_loss 1.7293 train_acc 35.66 train_fscore 28.85 valid_loss nan valid_acc nan val_fscore nan test_loss 1.749 test_acc 25.51 test_fscore 12.99 time 0.86\n",
            "epoch 8 train_loss 1.7142 train_acc 39.66 train_fscore 34.92 valid_loss nan valid_acc nan val_fscore nan test_loss 1.739 test_acc 28.71 test_fscore 19.55 time 0.76\n",
            "epoch 9 train_loss 1.697 train_acc 43.61 train_fscore 40.37 valid_loss nan valid_acc nan val_fscore nan test_loss 1.727 test_acc 35.3 test_fscore 27.86 time 0.77\n",
            "epoch 10 train_loss 1.6769 train_acc 46.83 train_fscore 44.57 valid_loss nan valid_acc nan val_fscore nan test_loss 1.7128 test_acc 39.56 test_fscore 33.51 time 0.75\n",
            "epoch 11 train_loss 1.6553 train_acc 49.66 train_fscore 47.85 valid_loss nan valid_acc nan val_fscore nan test_loss 1.6959 test_acc 39.8 test_fscore 33.63 time 0.76\n",
            "epoch 12 train_loss 1.627 train_acc 51.7 train_fscore 49.65 valid_loss nan valid_acc nan val_fscore nan test_loss 1.6758 test_acc 39.19 test_fscore 32.82 time 0.73\n",
            "epoch 13 train_loss 1.597 train_acc 52.7 train_fscore 50.03 valid_loss nan valid_acc nan val_fscore nan test_loss 1.6522 test_acc 38.14 test_fscore 31.79 time 0.74\n",
            "epoch 14 train_loss 1.5615 train_acc 53.61 train_fscore 50.2 valid_loss nan valid_acc nan val_fscore nan test_loss 1.6244 test_acc 40.11 test_fscore 34.01 time 0.74\n",
            "epoch 15 train_loss 1.5211 train_acc 54.32 train_fscore 50.52 valid_loss nan valid_acc nan val_fscore nan test_loss 1.5921 test_acc 41.16 test_fscore 36.43 time 0.76\n",
            "epoch 16 train_loss 1.4764 train_acc 53.72 train_fscore 49.19 valid_loss nan valid_acc nan val_fscore nan test_loss 1.5573 test_acc 45.16 test_fscore 42.44 time 0.74\n",
            "epoch 17 train_loss 1.4319 train_acc 52.98 train_fscore 47.81 valid_loss nan valid_acc nan val_fscore nan test_loss 1.5206 test_acc 46.52 test_fscore 43.37 time 0.74\n",
            "epoch 18 train_loss 1.3862 train_acc 52.87 train_fscore 47.26 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4822 test_acc 47.13 test_fscore 43.18 time 0.76\n",
            "epoch 19 train_loss 1.3373 train_acc 53.32 train_fscore 47.47 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4416 test_acc 47.13 test_fscore 42.61 time 0.75\n",
            "epoch 20 train_loss 1.2937 train_acc 53.48 train_fscore 47.49 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3983 test_acc 48.55 test_fscore 43.98 time 0.92\n",
            "epoch 21 train_loss 1.2535 train_acc 54.44 train_fscore 48.75 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3558 test_acc 50.4 test_fscore 46.48 time 1.18\n",
            "epoch 22 train_loss 1.2149 train_acc 54.65 train_fscore 49.66 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3157 test_acc 50.89 test_fscore 47.35 time 1.19\n",
            "epoch 23 train_loss 1.1742 train_acc 56.54 train_fscore 51.97 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2773 test_acc 53.79 test_fscore 51.17 time 1.08\n",
            "epoch 24 train_loss 1.1438 train_acc 56.7 train_fscore 52.66 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2464 test_acc 55.95 test_fscore 53.29 time 0.75\n",
            "epoch 25 train_loss 1.1145 train_acc 56.97 train_fscore 53.27 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2162 test_acc 56.69 test_fscore 54.0 time 0.75\n",
            "epoch 26 train_loss 1.0898 train_acc 58.55 train_fscore 54.93 valid_loss nan valid_acc nan val_fscore nan test_loss 1.1898 test_acc 57.3 test_fscore 54.58 time 0.73\n",
            "epoch 27 train_loss 1.0658 train_acc 58.28 train_fscore 54.98 valid_loss nan valid_acc nan val_fscore nan test_loss 1.1653 test_acc 56.93 test_fscore 54.14 time 0.76\n",
            "epoch 28 train_loss 1.0426 train_acc 59.31 train_fscore 56.05 valid_loss nan valid_acc nan val_fscore nan test_loss 1.1485 test_acc 56.75 test_fscore 53.94 time 0.76\n",
            "epoch 29 train_loss 1.0266 train_acc 59.64 train_fscore 56.61 valid_loss nan valid_acc nan val_fscore nan test_loss 1.132 test_acc 56.81 test_fscore 54.09 time 0.75\n",
            "epoch 30 train_loss 1.0076 train_acc 60.12 train_fscore 57.38 valid_loss nan valid_acc nan val_fscore nan test_loss 1.1186 test_acc 56.38 test_fscore 53.75 time 0.74\n",
            "epoch 31 train_loss 0.9915 train_acc 61.0 train_fscore 58.52 valid_loss nan valid_acc nan val_fscore nan test_loss 1.1057 test_acc 56.44 test_fscore 53.91 time 0.75\n",
            "epoch 32 train_loss 0.9723 train_acc 62.05 train_fscore 59.83 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0943 test_acc 56.25 test_fscore 53.72 time 0.78\n",
            "epoch 33 train_loss 0.9552 train_acc 62.19 train_fscore 60.12 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0842 test_acc 56.25 test_fscore 53.78 time 0.76\n",
            "epoch 34 train_loss 0.9374 train_acc 63.77 train_fscore 62.07 valid_loss nan valid_acc nan val_fscore nan test_loss 1.074 test_acc 56.81 test_fscore 54.92 time 0.75\n",
            "epoch 35 train_loss 0.9227 train_acc 64.56 train_fscore 62.87 valid_loss nan valid_acc nan val_fscore nan test_loss 1.067 test_acc 56.81 test_fscore 55.61 time 0.76\n",
            "epoch 36 train_loss 0.9045 train_acc 65.01 train_fscore 63.59 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0608 test_acc 56.75 test_fscore 55.71 time 0.8\n",
            "epoch 37 train_loss 0.8882 train_acc 65.9 train_fscore 64.53 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0599 test_acc 55.82 test_fscore 54.95 time 1.21\n",
            "epoch 38 train_loss 0.8672 train_acc 67.25 train_fscore 66.01 valid_loss nan valid_acc nan val_fscore nan test_loss 1.052 test_acc 56.13 test_fscore 55.27 time 1.32\n",
            "epoch 39 train_loss 0.8544 train_acc 67.71 train_fscore 66.62 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0483 test_acc 56.19 test_fscore 55.41 time 1.21\n",
            "epoch 40 train_loss 0.8348 train_acc 69.02 train_fscore 68.16 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0482 test_acc 56.32 test_fscore 55.49 time 0.79\n",
            "epoch 41 train_loss 0.8216 train_acc 70.14 train_fscore 69.26 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0532 test_acc 55.51 test_fscore 54.83 time 0.76\n",
            "epoch 42 train_loss 0.806 train_acc 70.64 train_fscore 69.93 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0561 test_acc 55.58 test_fscore 54.96 time 0.74\n",
            "epoch 43 train_loss 0.7905 train_acc 71.72 train_fscore 71.19 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0527 test_acc 56.38 test_fscore 55.63 time 0.76\n",
            "epoch 44 train_loss 0.7676 train_acc 72.38 train_fscore 71.82 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0504 test_acc 56.99 test_fscore 56.18 time 0.76\n",
            "epoch 45 train_loss 0.7523 train_acc 73.08 train_fscore 72.56 valid_loss nan valid_acc nan val_fscore nan test_loss 1.05 test_acc 56.38 test_fscore 55.76 time 0.79\n",
            "epoch 46 train_loss 0.7384 train_acc 73.58 train_fscore 73.1 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0443 test_acc 56.38 test_fscore 55.82 time 0.74\n",
            "epoch 47 train_loss 0.7247 train_acc 74.03 train_fscore 73.57 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0459 test_acc 56.75 test_fscore 56.01 time 0.76\n",
            "epoch 48 train_loss 0.7103 train_acc 74.92 train_fscore 74.53 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0432 test_acc 57.12 test_fscore 56.27 time 0.73\n",
            "epoch 49 train_loss 0.6971 train_acc 75.58 train_fscore 75.19 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0397 test_acc 57.3 test_fscore 56.48 time 0.76\n",
            "epoch 50 train_loss 0.6858 train_acc 76.01 train_fscore 75.68 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0352 test_acc 56.62 test_fscore 55.9 time 0.75\n",
            "epoch 51 train_loss 0.6747 train_acc 76.85 train_fscore 76.57 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0335 test_acc 57.24 test_fscore 56.45 time 0.77\n",
            "epoch 52 train_loss 0.6614 train_acc 76.88 train_fscore 76.6 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0403 test_acc 56.44 test_fscore 55.7 time 0.75\n",
            "epoch 53 train_loss 0.6459 train_acc 77.75 train_fscore 77.49 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0465 test_acc 56.07 test_fscore 55.36 time 1.15\n",
            "epoch 54 train_loss 0.6337 train_acc 78.06 train_fscore 77.79 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0391 test_acc 56.25 test_fscore 55.68 time 1.21\n",
            "epoch 55 train_loss 0.6303 train_acc 78.28 train_fscore 78.1 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0437 test_acc 56.75 test_fscore 56.31 time 1.26\n",
            "epoch 56 train_loss 0.6161 train_acc 78.62 train_fscore 78.41 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0441 test_acc 56.62 test_fscore 56.24 time 0.83\n",
            "epoch 57 train_loss 0.6031 train_acc 79.54 train_fscore 79.37 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0487 test_acc 56.56 test_fscore 56.22 time 0.74\n",
            "epoch 58 train_loss 0.5931 train_acc 79.47 train_fscore 79.28 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0408 test_acc 57.61 test_fscore 57.23 time 0.75\n",
            "epoch 59 train_loss 0.5804 train_acc 80.65 train_fscore 80.51 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0351 test_acc 58.23 test_fscore 58.11 time 0.75\n",
            "epoch 60 train_loss 0.5749 train_acc 80.98 train_fscore 80.79 valid_loss nan valid_acc nan val_fscore nan test_loss 1.0342 test_acc 58.29 test_fscore 58.16 time 0.77\n",
            "Test performance..\n",
            "Loss 1.0335 F1-score 56.45\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5161    0.1111    0.1829     144.0\n",
            "           1     0.8759    0.5184    0.6513     245.0\n",
            "           2     0.4281    0.6198    0.5064     384.0\n",
            "           3     0.6547    0.5353    0.5890     170.0\n",
            "           4     0.7244    0.6154    0.6655     299.0\n",
            "           5     0.5482    0.7165    0.6212     381.0\n",
            "\n",
            "    accuracy                         0.5724    1623.0\n",
            "   macro avg     0.6246    0.5194    0.5360    1623.0\n",
            "weighted avg     0.6100    0.5724    0.5645    1623.0\n",
            "\n",
            "[[ 16.   4.  78.   0.  46.   0.]\n",
            " [  1. 127.  55.   0.   1.  61.]\n",
            " [  3.   8. 238.  20.  22.  93.]\n",
            " [  0.   1.   9.  91.   0.  69.]\n",
            " [ 11.   2. 100.   0. 184.   2.]\n",
            " [  0.   3.  76.  28.   1. 273.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MELD with matching attention\n",
        "import numpy as np\n",
        "import argparse, time, pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "\n",
        "def get_train_valid_sampler(trainset, valid=0.1):\n",
        "    size = len(trainset)\n",
        "    idx = list(range(size))\n",
        "    split = int(valid*size)\n",
        "    return SubsetRandomSampler(idx[split:]), SubsetRandomSampler(idx[:split])\n",
        "\n",
        "def get_MELD_loaders(path, batch_size=32, valid=0.1, classify='emotion', num_workers=0, pin_memory=False):\n",
        "    trainset = MELDDataset(path, classify)\n",
        "    testset = MELDDataset(path, classify, train=False)\n",
        "    \n",
        "    train_sampler, valid_sampler = get_train_valid_sampler(trainset, valid)\n",
        "    train_loader = DataLoader(trainset,\n",
        "                              batch_size=batch_size,\n",
        "                              sampler=train_sampler,\n",
        "                              collate_fn=trainset.collate_fn,\n",
        "                              num_workers=num_workers,\n",
        "                              pin_memory=pin_memory)\n",
        "    valid_loader = DataLoader(trainset,\n",
        "                              batch_size=batch_size,\n",
        "                              sampler=valid_sampler,\n",
        "                              collate_fn=trainset.collate_fn,\n",
        "                              num_workers=num_workers,\n",
        "                              pin_memory=pin_memory)  \n",
        "    test_loader = DataLoader(testset,\n",
        "                             batch_size=batch_size,\n",
        "                             collate_fn=testset.collate_fn,\n",
        "                             num_workers=num_workers,\n",
        "                             pin_memory=pin_memory)\n",
        "\n",
        "    return train_loader, valid_loader, test_loader\n",
        "\n",
        "def train_or_eval_model(model, loss_function, dataloader, epoch, optimizer=None, train=False):\n",
        "    losses = []\n",
        "    preds = []\n",
        "    labels = []\n",
        "    masks = []\n",
        "    alphas, alphas_f, alphas_b, vids = [], [], [], []\n",
        "    assert not train or optimizer!=None\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    for data in dataloader:\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "        \n",
        "        textf, acouf, qmask, umask, label =\\\n",
        "                [d.cuda() for d in data[:-1]] if cuda else data[:-1]\n",
        "        \n",
        "        # log_prob = model(torch.cat((textf, acouf), dim=-1), qmask, umask) \n",
        "        log_prob, alpha, alpha_f, alpha_b = model(textf, qmask, umask) \n",
        "        lp_ = log_prob.transpose(0, 1).contiguous().view(-1, log_prob.size()[2])\n",
        "        labels_ = label.view(-1)\n",
        "        loss = loss_function(lp_, labels_, umask)\n",
        "\n",
        "        pred_ = torch.argmax(lp_, 1)\n",
        "        preds.append(pred_.data.cpu().numpy())\n",
        "        labels.append(labels_.data.cpu().numpy())\n",
        "        masks.append(umask.view(-1).cpu().numpy())\n",
        "\n",
        "        losses.append(loss.item()*masks[-1].sum())\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            if args.tensorboard:\n",
        "                for param in model.named_parameters():\n",
        "                    writer.add_histogram(param[0], param[1].grad, epoch)\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            alphas += alpha\n",
        "            alphas_f += alpha_f\n",
        "            alphas_b += alpha_b\n",
        "            vids += data[-1]\n",
        "\n",
        "    if preds!=[]:\n",
        "        preds  = np.concatenate(preds)\n",
        "        labels = np.concatenate(labels)\n",
        "        masks  = np.concatenate(masks)\n",
        "    else:\n",
        "        return float('nan'), float('nan'), [], [], [], float('nan'), []\n",
        "\n",
        "    avg_loss = round(np.sum(losses)/np.sum(masks), 4)\n",
        "    avg_accuracy = round(accuracy_score(labels, preds, sample_weight=masks)*100, 2)\n",
        "    avg_fscore = round(f1_score(labels, preds, sample_weight=masks, average='weighted')*100, 2)\n",
        "    return avg_loss, avg_accuracy, labels, preds, masks, avg_fscore, [alphas, alphas_f, alphas_b, vids]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--no-cuda', action='store_true', default=False, help='does not use GPU')\n",
        "    parser.add_argument('--lr', type=float, default=0.0001, metavar='LR', help='learning rate')\n",
        "    parser.add_argument('--l2', type=float, default=0.00001, metavar='L2', help='L2 regularization weight')\n",
        "    parser.add_argument('--dropout', type=float, default=0.25, metavar='dropout', help='dropout rate')\n",
        "    parser.add_argument('--batch-size', type=int, default=32, metavar='BS', help='batch size')\n",
        "    parser.add_argument('--epochs', type=int, default=60, metavar='E', help='number of epochs')\n",
        "    parser.add_argument('--attention', action='store_true', default=True, help='use attention on top of lstm')\n",
        "    parser.add_argument('--tensorboard', action='store_true', default=False, help='Enables tensorboard log')\n",
        "    parser.add_argument('--classify', default='emotion', help='classify emotion or sentiment')\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    print(args)\n",
        "    seed_everything(seed=2023)\n",
        "    args.cuda = torch.cuda.is_available() and not args.no_cuda\n",
        "    if args.cuda:\n",
        "        print('Running on GPU')\n",
        "    else:\n",
        "        print('Running on CPU')\n",
        "\n",
        "    if args.tensorboard:\n",
        "        from tensorboardX import SummaryWriter\n",
        "        writer = SummaryWriter()\n",
        "\n",
        "    batch_size = args.batch_size\n",
        "    cuda       = args.cuda\n",
        "    n_epochs   = args.epochs\n",
        "    \n",
        "    if args.classify == 'emotion':\n",
        "        n_classes  = 7\n",
        "    elif args.classify == 'sentiment':\n",
        "        n_classes  = 3\n",
        "\n",
        "    D_m = 600\n",
        "    D_e = 100\n",
        "    D_h = 100\n",
        "\n",
        "    model = LSTMModel(D_m, D_e, D_h,\n",
        "                      n_classes=n_classes,\n",
        "                      dropout=args.dropout,\n",
        "                      attention=args.attention)\n",
        "    if cuda:\n",
        "        model.cuda()\n",
        "        \n",
        "    loss_function = MaskedNLLLoss()\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                           lr=args.lr,\n",
        "                           weight_decay=args.l2)\n",
        "    \n",
        "    train_loader, valid_loader, test_loader = get_MELD_loaders('drive/MyDrive/MELD_features_raw.pkl',\n",
        "                                                                batch_size=batch_size,\n",
        "                                                                valid=0.0,\n",
        "                                                                classify=args.classify)\n",
        "\n",
        "    best_fscore, best_loss, best_label, best_pred, best_mask = None, None, None, None, None\n",
        "\n",
        "    for e in range(n_epochs):\n",
        "        start_time = time.time()\n",
        "        train_loss, train_acc, _, _, _, train_fscore, _ = train_or_eval_model(model, loss_function,\n",
        "                                               train_loader, e, optimizer, True)\n",
        "        valid_loss, valid_acc, _, _, _, val_fscore, _ = train_or_eval_model(model, loss_function, valid_loader, e)\n",
        "        test_loss, test_acc, test_label, test_pred, test_mask, test_fscore, attentions = train_or_eval_model(model, loss_function, test_loader, e)\n",
        "\n",
        "        if best_fscore == None or best_fscore < test_fscore:\n",
        "            best_fscore, best_loss, best_label, best_pred, best_mask, best_attn =\\\n",
        "                test_fscore, test_loss, test_label, test_pred, test_mask, attentions\n",
        "\n",
        "        if args.tensorboard:\n",
        "            writer.add_scalar('test: accuracy/loss', test_acc/test_loss, e)\n",
        "            writer.add_scalar('train: accuracy/loss', train_acc/train_loss, e)\n",
        "        print('epoch {} train_loss {} train_acc {} train_fscore {} valid_loss {} valid_acc {} val_fscore {} test_loss {} test_acc {} test_fscore {} time {}'.\\\n",
        "                format(e+1, train_loss, train_acc, train_fscore, valid_loss, valid_acc, val_fscore,\\\n",
        "                        test_loss, test_acc, test_fscore, round(time.time()-start_time, 2)))\n",
        "    if args.tensorboard:\n",
        "        writer.close()\n",
        "\n",
        "    print('Test performance..')\n",
        "    print('Loss {} F1-score {}'.format(best_loss,\n",
        "                                     round(f1_score(best_label, best_pred, sample_weight=best_mask, average='weighted')*100, 2)))\n",
        "    print(classification_report(best_label, best_pred, sample_weight=best_mask, digits=4))\n",
        "    print(confusion_matrix(best_label, best_pred, sample_weight=best_mask))"
      ],
      "metadata": {
        "id": "030b8d5T7n8I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a840be37-4d2c-4ce9-e674-e87b0c2aaa74"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(no_cuda=False, lr=0.0001, l2=1e-05, dropout=0.25, batch_size=32, epochs=60, attention=True, tensorboard=False, classify='emotion')\n",
            "2023\n",
            "Running on GPU\n",
            "epoch 1 train_loss 1.8577 train_acc 36.5 train_fscore 28.08 valid_loss nan valid_acc nan val_fscore nan test_loss 1.6829 test_acc 48.12 test_fscore 31.27 time 1.1\n",
            "epoch 2 train_loss 1.6077 train_acc 46.68 train_fscore 29.71 valid_loss nan valid_acc nan val_fscore nan test_loss 1.5516 test_acc 48.12 test_fscore 31.27 time 1.06\n",
            "epoch 3 train_loss 1.56 train_acc 46.68 train_fscore 29.71 valid_loss nan valid_acc nan val_fscore nan test_loss 1.5229 test_acc 48.12 test_fscore 31.27 time 1.06\n",
            "epoch 4 train_loss 1.5346 train_acc 46.68 train_fscore 29.71 valid_loss nan valid_acc nan val_fscore nan test_loss 1.5024 test_acc 48.12 test_fscore 31.27 time 1.34\n",
            "epoch 5 train_loss 1.5104 train_acc 46.68 train_fscore 29.71 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4936 test_acc 48.12 test_fscore 31.27 time 1.08\n",
            "epoch 6 train_loss 1.499 train_acc 46.81 train_fscore 30.29 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4879 test_acc 48.43 test_fscore 31.97 time 1.4\n",
            "epoch 7 train_loss 1.4868 train_acc 47.18 train_fscore 31.68 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4738 test_acc 48.39 test_fscore 32.05 time 1.54\n",
            "epoch 8 train_loss 1.4702 train_acc 47.63 train_fscore 32.8 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4597 test_acc 49.23 test_fscore 35.14 time 1.46\n",
            "epoch 9 train_loss 1.4484 train_acc 48.12 train_fscore 34.02 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4415 test_acc 49.73 test_fscore 37.22 time 1.11\n",
            "epoch 10 train_loss 1.4227 train_acc 49.95 train_fscore 38.36 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4154 test_acc 51.03 test_fscore 39.42 time 1.09\n",
            "epoch 11 train_loss 1.3916 train_acc 51.21 train_fscore 40.78 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3922 test_acc 52.26 test_fscore 42.09 time 1.12\n",
            "epoch 12 train_loss 1.3462 train_acc 52.66 train_fscore 42.9 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3647 test_acc 53.49 test_fscore 44.27 time 1.1\n",
            "epoch 13 train_loss 1.3006 train_acc 54.24 train_fscore 45.02 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3465 test_acc 53.6 test_fscore 44.74 time 1.06\n",
            "epoch 14 train_loss 1.2528 train_acc 56.09 train_fscore 48.06 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3349 test_acc 55.06 test_fscore 46.35 time 1.11\n",
            "epoch 15 train_loss 1.2219 train_acc 57.2 train_fscore 50.2 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3326 test_acc 54.94 test_fscore 48.64 time 1.1\n",
            "epoch 16 train_loss 1.1958 train_acc 58.56 train_fscore 52.46 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3322 test_acc 55.02 test_fscore 48.74 time 1.11\n",
            "epoch 17 train_loss 1.1814 train_acc 59.17 train_fscore 53.53 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3502 test_acc 53.37 test_fscore 49.32 time 1.24\n",
            "epoch 18 train_loss 1.1738 train_acc 59.92 train_fscore 54.71 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3432 test_acc 54.67 test_fscore 50.32 time 1.47\n",
            "epoch 19 train_loss 1.1614 train_acc 60.12 train_fscore 55.39 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3429 test_acc 55.36 test_fscore 50.25 time 1.52\n",
            "epoch 20 train_loss 1.1566 train_acc 60.35 train_fscore 55.61 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3547 test_acc 54.6 test_fscore 50.51 time 1.08\n",
            "epoch 21 train_loss 1.1509 train_acc 60.41 train_fscore 55.88 valid_loss nan valid_acc nan val_fscore nan test_loss 1.368 test_acc 53.03 test_fscore 49.67 time 1.07\n",
            "epoch 22 train_loss 1.1463 train_acc 60.87 train_fscore 56.37 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3503 test_acc 54.71 test_fscore 50.54 time 1.02\n",
            "epoch 23 train_loss 1.1432 train_acc 61.07 train_fscore 56.65 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3673 test_acc 53.79 test_fscore 50.63 time 1.06\n",
            "epoch 24 train_loss 1.1389 train_acc 61.12 train_fscore 56.77 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3617 test_acc 53.98 test_fscore 50.11 time 1.06\n",
            "epoch 25 train_loss 1.1378 train_acc 61.13 train_fscore 56.86 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3614 test_acc 54.67 test_fscore 49.94 time 1.04\n",
            "epoch 26 train_loss 1.1329 train_acc 60.94 train_fscore 56.71 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3626 test_acc 54.18 test_fscore 49.78 time 1.04\n",
            "epoch 27 train_loss 1.1304 train_acc 61.34 train_fscore 57.07 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3692 test_acc 53.79 test_fscore 50.19 time 1.03\n",
            "epoch 28 train_loss 1.1283 train_acc 61.49 train_fscore 57.25 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3537 test_acc 54.79 test_fscore 50.68 time 1.09\n",
            "epoch 29 train_loss 1.1256 train_acc 61.42 train_fscore 57.07 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3681 test_acc 53.52 test_fscore 50.0 time 1.21\n",
            "epoch 30 train_loss 1.1221 train_acc 61.78 train_fscore 57.61 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3643 test_acc 54.1 test_fscore 50.11 time 1.42\n",
            "epoch 31 train_loss 1.1253 train_acc 61.54 train_fscore 57.36 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3553 test_acc 55.17 test_fscore 50.43 time 1.54\n",
            "epoch 32 train_loss 1.1191 train_acc 61.87 train_fscore 57.61 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3607 test_acc 54.75 test_fscore 50.5 time 1.14\n",
            "epoch 33 train_loss 1.1137 train_acc 61.76 train_fscore 57.63 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3783 test_acc 53.07 test_fscore 49.89 time 1.22\n",
            "epoch 34 train_loss 1.1189 train_acc 61.89 train_fscore 57.86 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3597 test_acc 54.98 test_fscore 50.68 time 1.03\n",
            "epoch 35 train_loss 1.112 train_acc 61.79 train_fscore 57.61 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3581 test_acc 54.94 test_fscore 50.76 time 1.04\n",
            "epoch 36 train_loss 1.1115 train_acc 62.08 train_fscore 58.02 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3637 test_acc 54.56 test_fscore 50.53 time 1.03\n",
            "epoch 37 train_loss 1.109 train_acc 62.18 train_fscore 58.01 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3657 test_acc 54.75 test_fscore 51.23 time 1.02\n",
            "epoch 38 train_loss 1.1118 train_acc 62.17 train_fscore 58.17 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3563 test_acc 55.25 test_fscore 50.88 time 1.05\n",
            "epoch 39 train_loss 1.107 train_acc 62.34 train_fscore 58.29 valid_loss nan valid_acc nan val_fscore nan test_loss 1.366 test_acc 54.98 test_fscore 51.17 time 1.01\n",
            "epoch 40 train_loss 1.1057 train_acc 61.98 train_fscore 57.87 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3687 test_acc 53.49 test_fscore 50.07 time 1.05\n",
            "epoch 41 train_loss 1.1036 train_acc 62.46 train_fscore 58.42 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3671 test_acc 54.33 test_fscore 50.59 time 1.26\n",
            "epoch 42 train_loss 1.1031 train_acc 62.19 train_fscore 58.22 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3666 test_acc 54.67 test_fscore 50.58 time 1.53\n",
            "epoch 43 train_loss 1.1008 train_acc 62.52 train_fscore 58.48 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3755 test_acc 53.49 test_fscore 50.27 time 1.55\n",
            "epoch 44 train_loss 1.0983 train_acc 62.43 train_fscore 58.34 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3811 test_acc 52.72 test_fscore 50.05 time 1.08\n",
            "epoch 45 train_loss 1.0959 train_acc 62.62 train_fscore 58.69 valid_loss nan valid_acc nan val_fscore nan test_loss 1.363 test_acc 54.52 test_fscore 50.8 time 1.03\n",
            "epoch 46 train_loss 1.0967 train_acc 62.38 train_fscore 58.37 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3699 test_acc 52.68 test_fscore 49.43 time 1.04\n",
            "epoch 47 train_loss 1.0962 train_acc 62.52 train_fscore 58.6 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3681 test_acc 54.56 test_fscore 50.93 time 1.01\n",
            "epoch 48 train_loss 1.0901 train_acc 62.5 train_fscore 58.53 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3685 test_acc 53.37 test_fscore 50.01 time 1.06\n",
            "epoch 49 train_loss 1.089 train_acc 62.76 train_fscore 58.94 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3716 test_acc 53.52 test_fscore 50.19 time 1.02\n",
            "epoch 50 train_loss 1.0925 train_acc 62.6 train_fscore 58.92 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3644 test_acc 54.94 test_fscore 51.2 time 1.05\n",
            "epoch 51 train_loss 1.0885 train_acc 62.94 train_fscore 59.07 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3813 test_acc 54.71 test_fscore 51.26 time 1.05\n",
            "epoch 52 train_loss 1.0851 train_acc 62.75 train_fscore 58.86 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3596 test_acc 53.6 test_fscore 50.03 time 1.06\n",
            "epoch 53 train_loss 1.0844 train_acc 62.89 train_fscore 59.11 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3768 test_acc 54.9 test_fscore 51.3 time 1.18\n",
            "epoch 54 train_loss 1.0858 train_acc 62.85 train_fscore 59.09 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3809 test_acc 54.79 test_fscore 51.46 time 1.46\n",
            "epoch 55 train_loss 1.0805 train_acc 63.14 train_fscore 59.37 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3747 test_acc 53.18 test_fscore 50.07 time 1.45\n",
            "epoch 56 train_loss 1.0798 train_acc 63.31 train_fscore 59.4 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3742 test_acc 54.25 test_fscore 51.07 time 1.09\n",
            "epoch 57 train_loss 1.0808 train_acc 63.13 train_fscore 59.41 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3723 test_acc 53.98 test_fscore 50.54 time 1.04\n",
            "epoch 58 train_loss 1.0769 train_acc 63.35 train_fscore 59.59 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3678 test_acc 54.6 test_fscore 51.29 time 1.02\n",
            "epoch 59 train_loss 1.0777 train_acc 62.93 train_fscore 59.09 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3794 test_acc 53.1 test_fscore 50.14 time 1.03\n",
            "epoch 60 train_loss 1.0755 train_acc 63.05 train_fscore 59.39 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3768 test_acc 53.26 test_fscore 50.38 time 1.01\n",
            "Test performance..\n",
            "Loss 1.3809 F1-score 51.46\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6980    0.8225    0.7551    1256.0\n",
            "           1     0.2407    0.2989    0.2667     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2564    0.0481    0.0810     208.0\n",
            "           4     0.4233    0.4527    0.4375     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3878    0.3507    0.3683     345.0\n",
            "\n",
            "    accuracy                         0.5479    2610.0\n",
            "   macro avg     0.2866    0.2818    0.2727    2610.0\n",
            "weighted avg     0.4987    0.5479    0.5146    2610.0\n",
            "\n",
            "[[1.033e+03 6.800e+01 0.000e+00 1.700e+01 9.800e+01 0.000e+00 4.000e+01]\n",
            " [6.800e+01 8.400e+01 0.000e+00 5.000e+00 7.500e+01 0.000e+00 4.900e+01]\n",
            " [2.600e+01 6.000e+00 0.000e+00 1.000e+00 9.000e+00 0.000e+00 8.000e+00]\n",
            " [1.180e+02 2.500e+01 0.000e+00 1.000e+01 2.000e+01 0.000e+00 3.500e+01]\n",
            " [1.180e+02 5.900e+01 0.000e+00 2.000e+00 1.820e+02 0.000e+00 4.100e+01]\n",
            " [2.900e+01 1.100e+01 0.000e+00 0.000e+00 1.000e+01 0.000e+00 1.800e+01]\n",
            " [8.800e+01 9.600e+01 0.000e+00 4.000e+00 3.600e+01 0.000e+00 1.210e+02]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dKUIlkFQ6hav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AKztVCyy6hc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8iOTE_6-6hfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2IeXc0vZ6hhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "twnU-xCr6hkI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}