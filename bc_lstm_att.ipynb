{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPCXShUtAkCm1wcyT48WXHH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kashiwagi-Eri/HelloWorld/blob/master/bc_lstm_att.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The code below is modified from original repo:\n",
        "https://github.com/declare-lab/conv-emotion/tree/master/bc-LSTM-pytorch\n",
        "\n",
        "###Modified by CS6493 course project Group 18\n",
        "Last modified: 2023.03.27\n"
      ],
      "metadata": {
        "id": "JyIMwF0iNqOH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ei9s-xqd4FJw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import torch\n",
        "import torch.nn\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "\n",
        "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "\n",
        "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "\n",
        "!pip install torch-geometric\n",
        "\n",
        "from torch_geometric.data import Data"
      ],
      "metadata": {
        "id": "5qY1EhKT6hPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Etu8RTOC6hSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    FloatTensor = torch.cuda.FloatTensor\n",
        "    LongTensor = torch.cuda.LongTensor\n",
        "    ByteTensor = torch.cuda.ByteTensor\n",
        "\n",
        "else:\n",
        "    FloatTensor = torch.FloatTensor\n",
        "    LongTensor = torch.LongTensor\n",
        "    ByteTensor = torch.ByteTensor\n",
        "\n",
        "class MaskedNLLLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, weight=None):\n",
        "        super(MaskedNLLLoss, self).__init__()\n",
        "        self.weight = weight\n",
        "        self.loss = nn.NLLLoss(weight=weight,\n",
        "                               reduction='sum')\n",
        "\n",
        "    def forward(self, pred, target, mask):\n",
        "        \"\"\"\n",
        "        pred -> batch*seq_len, n_classes\n",
        "        target -> batch*seq_len\n",
        "        mask -> batch, seq_len\n",
        "        \"\"\"\n",
        "        mask_ = mask.view(-1,1) # batch*seq_len, 1\n",
        "        if type(self.weight)==type(None):\n",
        "            loss = self.loss(pred*mask_, target)/torch.sum(mask)\n",
        "        else:\n",
        "            loss = self.loss(pred*mask_, target)\\\n",
        "                            /torch.sum(self.weight[target]*mask_.squeeze())\n",
        "        return loss\n",
        "\n",
        "\n",
        "class MaskedMSELoss(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MaskedMSELoss, self).__init__()\n",
        "        self.loss = nn.MSELoss(reduction='sum')\n",
        "\n",
        "    def forward(self, pred, target, mask):\n",
        "        \"\"\"\n",
        "        pred -> batch*seq_len\n",
        "        target -> batch*seq_len\n",
        "        mask -> batch*seq_len\n",
        "        \"\"\"\n",
        "        loss = self.loss(pred*mask, target)/torch.sum(mask)\n",
        "        return loss\n",
        "\n",
        "\n",
        "class UnMaskedWeightedNLLLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, weight=None):\n",
        "        super(UnMaskedWeightedNLLLoss, self).__init__()\n",
        "        self.weight = weight\n",
        "        self.loss = nn.NLLLoss(weight=weight,\n",
        "                               reduction='sum')\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        \"\"\"\n",
        "        pred -> batch*seq_len, n_classes\n",
        "        target -> batch*seq_len\n",
        "        \"\"\"\n",
        "        if type(self.weight)==type(None):\n",
        "            loss = self.loss(pred, target)\n",
        "        else:\n",
        "            loss = self.loss(pred, target)\\\n",
        "                            /torch.sum(self.weight[target])\n",
        "        return loss\n",
        "\n",
        "\n",
        "class SimpleAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim):\n",
        "        super(SimpleAttention, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.scalar = nn.Linear(self.input_dim,1,bias=False)\n",
        "\n",
        "    def forward(self, M, x=None):\n",
        "        \"\"\"\n",
        "        M -> (seq_len, batch, vector)\n",
        "        x -> dummy argument for the compatibility with MatchingAttention\n",
        "        \"\"\"\n",
        "        scale = self.scalar(M) # seq_len, batch, 1\n",
        "        alpha = F.softmax(scale, dim=0).permute(1,2,0) # batch, 1, seq_len\n",
        "        attn_pool = torch.bmm(alpha, M.transpose(0,1))[:,0,:] # batch, vector\n",
        "        return attn_pool, alpha\n",
        "\n",
        "\n",
        "class MatchingAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, mem_dim, cand_dim, alpha_dim=None, att_type='general'):\n",
        "        super(MatchingAttention, self).__init__()\n",
        "        assert att_type!='concat' or alpha_dim!=None\n",
        "        assert att_type!='dot' or mem_dim==cand_dim\n",
        "        self.mem_dim = mem_dim\n",
        "        self.cand_dim = cand_dim\n",
        "        self.att_type = att_type\n",
        "        if att_type=='general':\n",
        "            self.transform = nn.Linear(cand_dim, mem_dim, bias=False)\n",
        "        if att_type=='general2':\n",
        "            self.transform = nn.Linear(cand_dim, mem_dim, bias=True)\n",
        "            #torch.nn.init.normal_(self.transform.weight,std=0.01)\n",
        "        elif att_type=='concat':\n",
        "            self.transform = nn.Linear(cand_dim+mem_dim, alpha_dim, bias=False)\n",
        "            self.vector_prod = nn.Linear(alpha_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, M, x, mask=None):\n",
        "        \"\"\"\n",
        "        M -> (seq_len, batch, mem_dim)\n",
        "        x -> (batch, cand_dim)\n",
        "        mask -> (batch, seq_len)\n",
        "        \"\"\"\n",
        "        if type(mask)==type(None):\n",
        "            mask = torch.ones(M.size(1), M.size(0)).type(M.type())\n",
        "\n",
        "        if self.att_type=='dot':\n",
        "            # vector = cand_dim = mem_dim\n",
        "            M_ = M.permute(1,2,0) # batch, vector, seqlen\n",
        "            x_ = x.unsqueeze(1) # batch, 1, vector\n",
        "            alpha = F.softmax(torch.bmm(x_, M_), dim=2) # batch, 1, seqlen\n",
        "        elif self.att_type=='general':\n",
        "            M_ = M.permute(1,2,0) # batch, mem_dim, seqlen\n",
        "            x_ = self.transform(x).unsqueeze(1) # batch, 1, mem_dim\n",
        "            alpha = F.softmax(torch.bmm(x_, M_), dim=2) # batch, 1, seqlen\n",
        "        elif self.att_type=='general2':\n",
        "            M_ = M.permute(1,2,0) # batch, mem_dim, seqlen\n",
        "            x_ = self.transform(x).unsqueeze(1) # batch, 1, mem_dim\n",
        "            mask_ = mask.unsqueeze(2).repeat(1, 1, self.mem_dim).transpose(1, 2) # batch, seq_len, mem_dim\n",
        "            M_ = M_ * mask_\n",
        "            alpha_ = torch.bmm(x_, M_)*mask.unsqueeze(1)\n",
        "            alpha_ = torch.tanh(alpha_)\n",
        "            alpha_ = F.softmax(alpha_, dim=2)\n",
        "            # alpha_ = F.softmax((torch.bmm(x_, M_))*mask.unsqueeze(1), dim=2) # batch, 1, seqlen\n",
        "            alpha_masked = alpha_*mask.unsqueeze(1) # batch, 1, seqlen\n",
        "            alpha_sum = torch.sum(alpha_masked, dim=2, keepdim=True) # batch, 1, 1\n",
        "            alpha = alpha_masked/alpha_sum # batch, 1, 1 ; normalized\n",
        "            #import ipdb;ipdb.set_trace()\n",
        "        else:\n",
        "            M_ = M.transpose(0,1) # batch, seqlen, mem_dim\n",
        "            x_ = x.unsqueeze(1).expand(-1,M.size()[0],-1) # batch, seqlen, cand_dim\n",
        "            M_x_ = torch.cat([M_,x_],2) # batch, seqlen, mem_dim+cand_dim\n",
        "            mx_a = F.tanh(self.transform(M_x_)) # batch, seqlen, alpha_dim\n",
        "            alpha = F.softmax(self.vector_prod(mx_a),1).transpose(1,2) # batch, 1, seqlen\n",
        "\n",
        "        attn_pool = torch.bmm(alpha, M.transpose(0,1))[:,0,:] # batch, mem_dim\n",
        "        return attn_pool, alpha\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim=None, out_dim=None, n_head=1, score_function='dot_product', dropout=0):\n",
        "        ''' Attention Mechanism\n",
        "        :param embed_dim:\n",
        "        :param hidden_dim:\n",
        "        :param out_dim:\n",
        "        :param n_head: num of head (Multi-Head Attention)\n",
        "        :param score_function: scaled_dot_product / mlp (concat) / bi_linear (general dot)\n",
        "        :return (?, q_len, out_dim,)\n",
        "        '''\n",
        "        super(Attention, self).__init__()\n",
        "        if hidden_dim is None:\n",
        "            hidden_dim = embed_dim // n_head\n",
        "        if out_dim is None:\n",
        "            out_dim = embed_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_head = n_head\n",
        "        self.score_function = score_function\n",
        "        self.w_k = nn.Linear(embed_dim, n_head * hidden_dim)\n",
        "        self.w_q = nn.Linear(embed_dim, n_head * hidden_dim)\n",
        "        self.proj = nn.Linear(n_head * hidden_dim, out_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        if score_function == 'mlp':\n",
        "            self.weight = nn.Parameter(torch.Tensor(hidden_dim*2))\n",
        "        elif self.score_function == 'bi_linear':\n",
        "            self.weight = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
        "        else:  # dot_product / scaled_dot_product\n",
        "            self.register_parameter('weight', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.hidden_dim)\n",
        "        if self.weight is not None:\n",
        "            self.weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, k, q):\n",
        "        if len(q.shape) == 2:  # q_len missing\n",
        "            q = torch.unsqueeze(q, dim=1)\n",
        "        if len(k.shape) == 2:  # k_len missing\n",
        "            k = torch.unsqueeze(k, dim=1)\n",
        "        mb_size = k.shape[0]  # ?\n",
        "        k_len = k.shape[1]\n",
        "        q_len = q.shape[1]\n",
        "        # k: (?, k_len, embed_dim,)\n",
        "        # q: (?, q_len, embed_dim,)\n",
        "        # kx: (n_head*?, k_len, hidden_dim)\n",
        "        # qx: (n_head*?, q_len, hidden_dim)\n",
        "        # score: (n_head*?, q_len, k_len,)\n",
        "        # output: (?, q_len, out_dim,)\n",
        "        kx = self.w_k(k).view(mb_size, k_len, self.n_head, self.hidden_dim)\n",
        "        kx = kx.permute(2, 0, 1, 3).contiguous().view(-1, k_len, self.hidden_dim)\n",
        "        qx = self.w_q(q).view(mb_size, q_len, self.n_head, self.hidden_dim)\n",
        "        qx = qx.permute(2, 0, 1, 3).contiguous().view(-1, q_len, self.hidden_dim)\n",
        "        if self.score_function == 'dot_product':\n",
        "            kt = kx.permute(0, 2, 1)\n",
        "            score = torch.bmm(qx, kt)\n",
        "        elif self.score_function == 'scaled_dot_product':\n",
        "            kt = kx.permute(0, 2, 1)\n",
        "            qkt = torch.bmm(qx, kt)\n",
        "            score = torch.div(qkt, math.sqrt(self.hidden_dim))\n",
        "        elif self.score_function == 'mlp':\n",
        "            kxx = torch.unsqueeze(kx, dim=1).expand(-1, q_len, -1, -1)\n",
        "            qxx = torch.unsqueeze(qx, dim=2).expand(-1, -1, k_len, -1)\n",
        "            kq = torch.cat((kxx, qxx), dim=-1)  # (n_head*?, q_len, k_len, hidden_dim*2)\n",
        "            # kq = torch.unsqueeze(kx, dim=1) + torch.unsqueeze(qx, dim=2)\n",
        "            score = torch.tanh(torch.matmul(kq, self.weight))\n",
        "        elif self.score_function == 'bi_linear':\n",
        "            qw = torch.matmul(qx, self.weight)\n",
        "            kt = kx.permute(0, 2, 1)\n",
        "            score = torch.bmm(qw, kt)\n",
        "        else:\n",
        "            raise RuntimeError('invalid score_function')\n",
        "            \n",
        "        score = F.softmax(score, dim=0)\n",
        "        output = torch.bmm(score, kx)  # (n_head*?, q_len, hidden_dim)\n",
        "        output = torch.cat(torch.split(output, mb_size, dim=0), dim=-1)  # (?, q_len, n_head*hidden_dim)\n",
        "        output = self.proj(output)  # (?, q_len, out_dim)\n",
        "        output = self.dropout(output)\n",
        "        return output, score\n",
        "    \n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "\n",
        "    def __init__(self, D_m, D_e, D_h, n_classes=7, dropout=0.5, attention=False):\n",
        "        \n",
        "        super(LSTMModel, self).__init__()\n",
        "        \n",
        "        self.dropout   = nn.Dropout(dropout)\n",
        "        self.attention = attention\n",
        "        self.lstm = nn.LSTM(input_size=D_m, hidden_size=D_e, num_layers=2, bidirectional=True, dropout=dropout)\n",
        "        \n",
        "        if self.attention:\n",
        "            self.matchatt = MatchingAttention(2*D_e, 2*D_e, att_type='general2')\n",
        "        \n",
        "        self.linear = nn.Linear(2*D_e, D_h)\n",
        "        self.smax_fc = nn.Linear(D_h, n_classes)\n",
        "\n",
        "    def forward(self, U, qmask, umask):\n",
        "        \"\"\"\n",
        "        U -> seq_len, batch, D_m\n",
        "        qmask -> seq_len, batch, party\n",
        "        \"\"\"\n",
        "        emotions, hidden = self.lstm(U)\n",
        "        alpha, alpha_f, alpha_b = [], [], []\n",
        "        \n",
        "        if self.attention:\n",
        "            att_emotions = []\n",
        "            alpha = []\n",
        "            for t in emotions:\n",
        "                att_em, alpha_ = self.matchatt(emotions, t, mask=umask)\n",
        "                att_emotions.append(att_em.unsqueeze(0))\n",
        "                alpha.append(alpha_[:, 0, :])\n",
        "            att_emotions = torch.cat(att_emotions, dim=0)\n",
        "            hidden = F.relu(self.linear(att_emotions))\n",
        "        else:\n",
        "            hidden = F.relu(self.linear(emotions))\n",
        "        \n",
        "        hidden = self.dropout(hidden)\n",
        "        log_prob = F.log_softmax(self.smax_fc(hidden), 2)\n",
        "        return log_prob, alpha, alpha_f, alpha_b\n",
        "    "
      ],
      "metadata": {
        "id": "U5PLioA58VMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataloader\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "class IEMOCAPDataset(Dataset):\n",
        "\n",
        "    def __init__(self, path, train=True):\n",
        "        self.videoIDs, self.videoSpeakers, self.videoLabels, self.videoText,\\\n",
        "        self.videoAudio, self.videoVisual, self.videoSentence, self.trainVid,\\\n",
        "        self.testVid = pickle.load(open(path, 'rb'), encoding='latin1')\n",
        "        '''\n",
        "        label index mapping = {'happy':0, 'sad':1, 'neutral':2, 'angry':3, 'excioted':4, 'frustrated':5}\n",
        "        '''\n",
        "        self.keys = [x for x in (self.trainVid if train else self.testVid)]\n",
        "\n",
        "        self.len = len(self.keys)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        vid = self.keys[index]\n",
        "        return torch.FloatTensor(self.videoText[vid]),\\\n",
        "               torch.FloatTensor(self.videoVisual[vid]),\\\n",
        "               torch.FloatTensor(self.videoAudio[vid]),\\\n",
        "               torch.FloatTensor([[1,0] if x=='M' else [0,1] for x in\\\n",
        "                                  self.videoSpeakers[vid]]),\\\n",
        "               torch.FloatTensor([1]*len(self.videoLabels[vid])),\\\n",
        "               torch.LongTensor(self.videoLabels[vid]),\\\n",
        "               vid\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def collate_fn(self, data):\n",
        "        dat = pd.DataFrame(data)\n",
        "        return [pad_sequence(dat[i]) if i<4 else pad_sequence(dat[i], True) if i<6 else dat[i].tolist() for i in dat]\n",
        "\n",
        "class MELDDataset(Dataset):\n",
        "\n",
        "    def __init__(self, path, classify, train=True):\n",
        "        self.videoIDs, self.videoSpeakers, self.emotion_labels, self.videoText,\\\n",
        "        self.videoAudio, self.videoSentence, self.trainVid,\\\n",
        "        self.testVid, self.sentiment_labels = pickle.load(open(path, 'rb'))\n",
        "        \n",
        "        if classify == 'emotion':\n",
        "            self.videoLabels = self.emotion_labels\n",
        "        else:\n",
        "            self.videoLabels = self.sentiment_labels\n",
        "        '''\n",
        "        emotion_label_mapping = {'neutral': 0, 'surprise': 1, 'fear': 2, 'sadness': 3, 'joy': 4, 'disgust': 5, 'anger':6}\n",
        "        setiment_label_mapping = {'neutral': 0, 'positive': 1, 'negative': 2}\n",
        "        '''\n",
        "        self.keys = [x for x in (self.trainVid if train else self.testVid)]\n",
        "\n",
        "        self.len = len(self.keys)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        vid = self.keys[index]\n",
        "        return torch.FloatTensor(self.videoText[vid]),\\\n",
        "               torch.FloatTensor(self.videoAudio[vid]),\\\n",
        "               torch.FloatTensor(self.videoSpeakers[vid]),\\\n",
        "               torch.FloatTensor([1]*len(self.videoLabels[vid])),\\\n",
        "               torch.LongTensor(self.videoLabels[vid]),\\\n",
        "               vid\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def collate_fn(self, data):\n",
        "        dat = pd.DataFrame(data)\n",
        "        return [pad_sequence(dat[i]) if i<3 else pad_sequence(dat[i], True) if i<5 else dat[i].tolist() for i in dat]\n",
        "        "
      ],
      "metadata": {
        "id": "3zbpy3NY8yeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed=2023):\n",
        "    print(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "weiCWx50ObS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#IEMOCAP without attention\n",
        "import numpy as np\n",
        "import argparse, time, pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "def get_train_valid_sampler(trainset, valid=0.1):\n",
        "    size = len(trainset)\n",
        "    idx = list(range(size))\n",
        "    split = int(valid*size)\n",
        "    return SubsetRandomSampler(idx[split:]), SubsetRandomSampler(idx[:split])\n",
        "\n",
        "def get_IEMOCAP_loaders(path, batch_size=32, valid=0.1, num_workers=0, pin_memory=False):\n",
        "    trainset = IEMOCAPDataset(path=path)\n",
        "    testset = IEMOCAPDataset(path=path, train=False)\n",
        "    \n",
        "    train_sampler, valid_sampler = get_train_valid_sampler(trainset, valid)\n",
        "    train_loader = DataLoader(trainset,\n",
        "                              batch_size=batch_size,\n",
        "                              sampler=train_sampler,\n",
        "                              collate_fn=trainset.collate_fn,\n",
        "                              num_workers=num_workers,\n",
        "                              pin_memory=pin_memory)\n",
        "    valid_loader = DataLoader(trainset,\n",
        "                              batch_size=batch_size,\n",
        "                              sampler=valid_sampler,\n",
        "                              collate_fn=trainset.collate_fn,\n",
        "                              num_workers=num_workers,\n",
        "                              pin_memory=pin_memory)\n",
        "    test_loader = DataLoader(testset,\n",
        "                             batch_size=batch_size,\n",
        "                             collate_fn=testset.collate_fn,\n",
        "                             num_workers=num_workers,\n",
        "                             pin_memory=pin_memory)\n",
        "\n",
        "    return train_loader, valid_loader, test_loader\n",
        "\n",
        "def train_or_eval_model(model, loss_function, dataloader, epoch, optimizer=None, train=False):\n",
        "    losses = []\n",
        "    preds = []\n",
        "    labels = []\n",
        "    masks = []\n",
        "    alphas, alphas_f, alphas_b, vids = [], [], [], []\n",
        "    assert not train or optimizer!=None\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    for data in dataloader:\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "        \n",
        "        textf, visuf, acouf, qmask, umask, label =\\\n",
        "                [d.cuda() for d in data[:-1]] if cuda else data[:-1]\n",
        "        \n",
        "        # log_prob = model(torch.cat((textf, acouf, visuf), dim=-1), qmask, umask) \n",
        "        log_prob, alpha, alpha_f, alpha_b = model(textf, qmask, umask) \n",
        "        lp_ = log_prob.transpose(0, 1).contiguous().view(-1, log_prob.size()[2])\n",
        "        labels_ = label.view(-1) \n",
        "        loss = loss_function(lp_, labels_, umask)\n",
        "\n",
        "        pred_ = torch.argmax(lp_, 1) \n",
        "        preds.append(pred_.data.cpu().numpy())\n",
        "        labels.append(labels_.data.cpu().numpy())\n",
        "        masks.append(umask.view(-1).cpu().numpy())\n",
        "\n",
        "        losses.append(loss.item()*masks[-1].sum())\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            if args.tensorboard:\n",
        "                for param in model.named_parameters():\n",
        "                    writer.add_histogram(param[0], param[1].grad, epoch)\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            alphas += alpha\n",
        "            alphas_f += alpha_f\n",
        "            alphas_b += alpha_b\n",
        "            vids += data[-1]\n",
        "\n",
        "    if preds!=[]:\n",
        "        preds  = np.concatenate(preds)\n",
        "        labels = np.concatenate(labels)\n",
        "        masks  = np.concatenate(masks)\n",
        "    else:\n",
        "        return float('nan'), float('nan'), [], [], [], float('nan'), []\n",
        "\n",
        "    avg_loss = round(np.sum(losses)/np.sum(masks), 4)\n",
        "    avg_accuracy = round(accuracy_score(labels, preds, sample_weight=masks)*100, 2)\n",
        "    avg_fscore = round(f1_score(labels, preds, sample_weight=masks, average='weighted')*100, 2)\n",
        "    return avg_loss, avg_accuracy, labels, preds, masks, avg_fscore, [alphas, alphas_f, alphas_b, vids]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--no-cuda', action='store_true', default=False, help='does not use GPU')\n",
        "    parser.add_argument('--lr', type=float, default=0.0001, metavar='LR', help='learning rate')\n",
        "    parser.add_argument('--l2', type=float, default=0.00001, metavar='L2', help='L2 regularization weight')\n",
        "    parser.add_argument('--dropout', type=float, default=0.25, metavar='dropout', help='dropout rate')\n",
        "    parser.add_argument('--batch-size', type=int, default=32, metavar='BS', help='batch size')\n",
        "    parser.add_argument('--epochs', type=int, default=60, metavar='E', help='number of epochs')\n",
        "    parser.add_argument('--class-weight', action='store_true', default=False, help='use class weight')\n",
        "    parser.add_argument('--attention', action='store_true', default=False, help='use attention on top of lstm')\n",
        "    parser.add_argument('--tensorboard', action='store_true', default=False, help='Enables tensorboard log')\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    print(args)\n",
        "    seed_everything(seed=2023)\n",
        "    args.cuda = torch.cuda.is_available() and not args.no_cuda\n",
        "    if args.cuda:\n",
        "        print('Running on GPU')\n",
        "    else:\n",
        "        print('Running on CPU')\n",
        "\n",
        "    if args.tensorboard:\n",
        "        from tensorboardX import SummaryWriter\n",
        "        writer = SummaryWriter()\n",
        "\n",
        "    batch_size = args.batch_size\n",
        "    cuda       = args.cuda\n",
        "    n_epochs   = args.epochs\n",
        "    \n",
        "    n_classes  = 6\n",
        "    D_m = 100\n",
        "    D_e = 100\n",
        "    D_h = 100\n",
        "\n",
        "    model = LSTMModel(D_m, D_e, D_h,\n",
        "                      n_classes=n_classes,\n",
        "                      dropout=args.dropout,\n",
        "                      attention=args.attention)\n",
        "    if cuda:\n",
        "        model.cuda()\n",
        "        \n",
        "    loss_weights = torch.FloatTensor([1.0, 0.60072, 0.38066, 0.54019, 0.67924, 0.34332])\n",
        "    \n",
        "    if args.class_weight:\n",
        "        loss_function  = MaskedNLLLoss(loss_weights.cuda() if cuda else loss_weights)\n",
        "    else:\n",
        "        loss_function = MaskedNLLLoss()\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                           lr=args.lr,\n",
        "                           weight_decay=args.l2)\n",
        "\n",
        "    train_loader, valid_loader, test_loader = get_IEMOCAP_loaders('drive/MyDrive/IEMOCAP_features_raw.pkl',\n",
        "                                                                  batch_size=batch_size,\n",
        "                                                                  valid=0.0)\n",
        "\n",
        "    best_loss, best_label, best_pred, best_mask = None, None, None, None\n",
        "\n",
        "    for e in range(n_epochs):\n",
        "        start_time = time.time()\n",
        "        train_loss, train_acc, _, _, _, train_fscore, _ = train_or_eval_model(model, loss_function,\n",
        "                                               train_loader, e, optimizer, True)\n",
        "        valid_loss, valid_acc, _, _, _, val_fscore, _ = train_or_eval_model(model, loss_function, valid_loader, e)\n",
        "        test_loss, test_acc, test_label, test_pred, test_mask, test_fscore, attentions = train_or_eval_model(model, loss_function, test_loader, e)\n",
        "\n",
        "        if best_loss == None or best_loss > test_loss:\n",
        "            best_loss, best_label, best_pred, best_mask, best_attn =\\\n",
        "                    test_loss, test_label, test_pred, test_mask, attentions\n",
        "\n",
        "        if args.tensorboard:\n",
        "            writer.add_scalar('test: accuracy/loss', test_acc/test_loss, e)\n",
        "            writer.add_scalar('train: accuracy/loss', train_acc/train_loss, e)\n",
        "        print('epoch {} train_loss {} train_acc {} train_fscore {} valid_loss {} valid_acc {} val_fscore {} test_loss {} test_acc {} test_fscore {} time {}'.\\\n",
        "                format(e+1, train_loss, train_acc, train_fscore, valid_loss, valid_acc, val_fscore,\\\n",
        "                        test_loss, test_acc, test_fscore, round(time.time()-start_time, 2)))\n",
        "    if args.tensorboard:\n",
        "        writer.close()\n",
        "\n",
        "    print('Test performance..')\n",
        "    print('Loss {} F1-score {}'.format(best_loss,\n",
        "                                     round(f1_score(best_label, best_pred, sample_weight=best_mask, average='weighted')*100, 2)))\n",
        "    print(classification_report(best_label, best_pred, sample_weight=best_mask, digits=4))\n",
        "    print(confusion_matrix(best_label, best_pred, sample_weight=best_mask))"
      ],
      "metadata": {
        "id": "kYWUbLns6hWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MELD without attention\n",
        "import numpy as np\n",
        "import argparse, time, pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "\n",
        "def get_train_valid_sampler(trainset, valid=0.1):\n",
        "    size = len(trainset)\n",
        "    idx = list(range(size))\n",
        "    split = int(valid*size)\n",
        "    return SubsetRandomSampler(idx[split:]), SubsetRandomSampler(idx[:split])\n",
        "\n",
        "def get_MELD_loaders(path, batch_size=32, valid=0.1, classify='emotion', num_workers=0, pin_memory=False):\n",
        "    trainset = MELDDataset(path, classify)\n",
        "    testset = MELDDataset(path, classify, train=False)\n",
        "    \n",
        "    train_sampler, valid_sampler = get_train_valid_sampler(trainset, valid)\n",
        "    train_loader = DataLoader(trainset,\n",
        "                              batch_size=batch_size,\n",
        "                              sampler=train_sampler,\n",
        "                              collate_fn=trainset.collate_fn,\n",
        "                              num_workers=num_workers,\n",
        "                              pin_memory=pin_memory)\n",
        "    valid_loader = DataLoader(trainset,\n",
        "                              batch_size=batch_size,\n",
        "                              sampler=valid_sampler,\n",
        "                              collate_fn=trainset.collate_fn,\n",
        "                              num_workers=num_workers,\n",
        "                              pin_memory=pin_memory)  \n",
        "    test_loader = DataLoader(testset,\n",
        "                             batch_size=batch_size,\n",
        "                             collate_fn=testset.collate_fn,\n",
        "                             num_workers=num_workers,\n",
        "                             pin_memory=pin_memory)\n",
        "\n",
        "    return train_loader, valid_loader, test_loader\n",
        "\n",
        "def train_or_eval_model(model, loss_function, dataloader, epoch, optimizer=None, train=False):\n",
        "    losses = []\n",
        "    preds = []\n",
        "    labels = []\n",
        "    masks = []\n",
        "    alphas, alphas_f, alphas_b, vids = [], [], [], []\n",
        "    assert not train or optimizer!=None\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    for data in dataloader:\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "        \n",
        "        textf, acouf, qmask, umask, label =\\\n",
        "                [d.cuda() for d in data[:-1]] if cuda else data[:-1]\n",
        "        \n",
        "        # log_prob = model(torch.cat((textf, acouf), dim=-1), qmask, umask) \n",
        "        log_prob, alpha, alpha_f, alpha_b = model(textf, qmask, umask) \n",
        "        lp_ = log_prob.transpose(0, 1).contiguous().view(-1, log_prob.size()[2])\n",
        "        labels_ = label.view(-1)\n",
        "        loss = loss_function(lp_, labels_, umask)\n",
        "\n",
        "        pred_ = torch.argmax(lp_, 1)\n",
        "        preds.append(pred_.data.cpu().numpy())\n",
        "        labels.append(labels_.data.cpu().numpy())\n",
        "        masks.append(umask.view(-1).cpu().numpy())\n",
        "\n",
        "        losses.append(loss.item()*masks[-1].sum())\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            if args.tensorboard:\n",
        "                for param in model.named_parameters():\n",
        "                    writer.add_histogram(param[0], param[1].grad, epoch)\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            alphas += alpha\n",
        "            alphas_f += alpha_f\n",
        "            alphas_b += alpha_b\n",
        "            vids += data[-1]\n",
        "\n",
        "    if preds!=[]:\n",
        "        preds  = np.concatenate(preds)\n",
        "        labels = np.concatenate(labels)\n",
        "        masks  = np.concatenate(masks)\n",
        "    else:\n",
        "        return float('nan'), float('nan'), [], [], [], float('nan'), []\n",
        "\n",
        "    avg_loss = round(np.sum(losses)/np.sum(masks), 4)\n",
        "    avg_accuracy = round(accuracy_score(labels, preds, sample_weight=masks)*100, 2)\n",
        "    avg_fscore = round(f1_score(labels, preds, sample_weight=masks, average='weighted')*100, 2)\n",
        "    return avg_loss, avg_accuracy, labels, preds, masks, avg_fscore, [alphas, alphas_f, alphas_b, vids]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--no-cuda', action='store_true', default=False, help='does not use GPU')\n",
        "    parser.add_argument('--lr', type=float, default=0.0001, metavar='LR', help='learning rate')\n",
        "    parser.add_argument('--l2', type=float, default=0.00001, metavar='L2', help='L2 regularization weight')\n",
        "    parser.add_argument('--dropout', type=float, default=0.25, metavar='dropout', help='dropout rate')\n",
        "    parser.add_argument('--batch-size', type=int, default=32, metavar='BS', help='batch size')\n",
        "    parser.add_argument('--epochs', type=int, default=60, metavar='E', help='number of epochs')\n",
        "    parser.add_argument('--attention', action='store_true', default=False, help='use attention on top of lstm')\n",
        "    parser.add_argument('--tensorboard', action='store_true', default=False, help='Enables tensorboard log')\n",
        "    parser.add_argument('--classify', default='emotion', help='classify emotion or sentiment')\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    print(args)\n",
        "    seed_everything(seed=2023)\n",
        "    args.cuda = torch.cuda.is_available() and not args.no_cuda\n",
        "    if args.cuda:\n",
        "        print('Running on GPU')\n",
        "    else:\n",
        "        print('Running on CPU')\n",
        "\n",
        "    if args.tensorboard:\n",
        "        from tensorboardX import SummaryWriter\n",
        "        writer = SummaryWriter()\n",
        "\n",
        "    batch_size = args.batch_size\n",
        "    cuda       = args.cuda\n",
        "    n_epochs   = args.epochs\n",
        "    \n",
        "    if args.classify == 'emotion':\n",
        "        n_classes  = 7\n",
        "    elif args.classify == 'sentiment':\n",
        "        n_classes  = 3\n",
        "\n",
        "    D_m = 600\n",
        "    D_e = 100\n",
        "    D_h = 100\n",
        "\n",
        "    model = LSTMModel(D_m, D_e, D_h,\n",
        "                      n_classes=n_classes,\n",
        "                      dropout=args.dropout,\n",
        "                      attention=args.attention)\n",
        "    if cuda:\n",
        "        model.cuda()\n",
        "        \n",
        "    loss_function = MaskedNLLLoss()\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                           lr=args.lr,\n",
        "                           weight_decay=args.l2)\n",
        "    \n",
        "    train_loader, valid_loader, test_loader = get_MELD_loaders('drive/MyDrive/MELD_features_raw.pkl',\n",
        "                                                                batch_size=batch_size,\n",
        "                                                                valid=0.0,\n",
        "                                                                classify=args.classify)\n",
        "\n",
        "    best_fscore, best_loss, best_label, best_pred, best_mask = None, None, None, None, None\n",
        "\n",
        "    for e in range(n_epochs):\n",
        "        start_time = time.time()\n",
        "        train_loss, train_acc, _, _, _, train_fscore, _ = train_or_eval_model(model, loss_function,\n",
        "                                               train_loader, e, optimizer, True)\n",
        "        valid_loss, valid_acc, _, _, _, val_fscore, _ = train_or_eval_model(model, loss_function, valid_loader, e)\n",
        "        test_loss, test_acc, test_label, test_pred, test_mask, test_fscore, attentions = train_or_eval_model(model, loss_function, test_loader, e)\n",
        "\n",
        "        if best_fscore == None or best_fscore < test_fscore:\n",
        "            best_fscore, best_loss, best_label, best_pred, best_mask, best_attn =\\\n",
        "                test_fscore, test_loss, test_label, test_pred, test_mask, attentions\n",
        "\n",
        "        if args.tensorboard:\n",
        "            writer.add_scalar('test: accuracy/loss', test_acc/test_loss, e)\n",
        "            writer.add_scalar('train: accuracy/loss', train_acc/train_loss, e)\n",
        "        print('epoch {} train_loss {} train_acc {} train_fscore {} valid_loss {} valid_acc {} val_fscore {} test_loss {} test_acc {} test_fscore {} time {}'.\\\n",
        "                format(e+1, train_loss, train_acc, train_fscore, valid_loss, valid_acc, val_fscore,\\\n",
        "                        test_loss, test_acc, test_fscore, round(time.time()-start_time, 2)))\n",
        "    if args.tensorboard:\n",
        "        writer.close()\n",
        "\n",
        "    print('Test performance..')\n",
        "    print('Loss {} F1-score {}'.format(best_loss,\n",
        "                                     round(f1_score(best_label, best_pred, sample_weight=best_mask, average='weighted')*100, 2)))\n",
        "    print(classification_report(best_label, best_pred, sample_weight=best_mask, digits=4))\n",
        "    print(confusion_matrix(best_label, best_pred, sample_weight=best_mask))"
      ],
      "metadata": {
        "id": "GOGQXFlF6hYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#IEMOCAP with matching attention\n",
        "import numpy as np\n",
        "import argparse, time, pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "def get_train_valid_sampler(trainset, valid=0.1):\n",
        "    size = len(trainset)\n",
        "    idx = list(range(size))\n",
        "    split = int(valid*size)\n",
        "    return SubsetRandomSampler(idx[split:]), SubsetRandomSampler(idx[:split])\n",
        "\n",
        "def get_IEMOCAP_loaders(path, batch_size=32, valid=0.1, num_workers=0, pin_memory=False):\n",
        "    trainset = IEMOCAPDataset(path=path)\n",
        "    testset = IEMOCAPDataset(path=path, train=False)\n",
        "    \n",
        "    train_sampler, valid_sampler = get_train_valid_sampler(trainset, valid)\n",
        "    train_loader = DataLoader(trainset,\n",
        "                              batch_size=batch_size,\n",
        "                              sampler=train_sampler,\n",
        "                              collate_fn=trainset.collate_fn,\n",
        "                              num_workers=num_workers,\n",
        "                              pin_memory=pin_memory)\n",
        "    valid_loader = DataLoader(trainset,\n",
        "                              batch_size=batch_size,\n",
        "                              sampler=valid_sampler,\n",
        "                              collate_fn=trainset.collate_fn,\n",
        "                              num_workers=num_workers,\n",
        "                              pin_memory=pin_memory)\n",
        "    test_loader = DataLoader(testset,\n",
        "                             batch_size=batch_size,\n",
        "                             collate_fn=testset.collate_fn,\n",
        "                             num_workers=num_workers,\n",
        "                             pin_memory=pin_memory)\n",
        "\n",
        "    return train_loader, valid_loader, test_loader\n",
        "\n",
        "def train_or_eval_model(model, loss_function, dataloader, epoch, optimizer=None, train=False):\n",
        "    losses = []\n",
        "    preds = []\n",
        "    labels = []\n",
        "    masks = []\n",
        "    alphas, alphas_f, alphas_b, vids = [], [], [], []\n",
        "    assert not train or optimizer!=None\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    for data in dataloader:\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "        \n",
        "        textf, visuf, acouf, qmask, umask, label =\\\n",
        "                [d.cuda() for d in data[:-1]] if cuda else data[:-1]\n",
        "        \n",
        "        # log_prob = model(torch.cat((textf, acouf, visuf), dim=-1), qmask, umask) \n",
        "        log_prob, alpha, alpha_f, alpha_b = model(textf, qmask, umask) \n",
        "        lp_ = log_prob.transpose(0, 1).contiguous().view(-1, log_prob.size()[2])\n",
        "        labels_ = label.view(-1) \n",
        "        loss = loss_function(lp_, labels_, umask)\n",
        "\n",
        "        pred_ = torch.argmax(lp_, 1) \n",
        "        preds.append(pred_.data.cpu().numpy())\n",
        "        labels.append(labels_.data.cpu().numpy())\n",
        "        masks.append(umask.view(-1).cpu().numpy())\n",
        "\n",
        "        losses.append(loss.item()*masks[-1].sum())\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            if args.tensorboard:\n",
        "                for param in model.named_parameters():\n",
        "                    writer.add_histogram(param[0], param[1].grad, epoch)\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            alphas += alpha\n",
        "            alphas_f += alpha_f\n",
        "            alphas_b += alpha_b\n",
        "            vids += data[-1]\n",
        "\n",
        "    if preds!=[]:\n",
        "        preds  = np.concatenate(preds)\n",
        "        labels = np.concatenate(labels)\n",
        "        masks  = np.concatenate(masks)\n",
        "    else:\n",
        "        return float('nan'), float('nan'), [], [], [], float('nan'), []\n",
        "\n",
        "    avg_loss = round(np.sum(losses)/np.sum(masks), 4)\n",
        "    avg_accuracy = round(accuracy_score(labels, preds, sample_weight=masks)*100, 2)\n",
        "    avg_fscore = round(f1_score(labels, preds, sample_weight=masks, average='weighted')*100, 2)\n",
        "    return avg_loss, avg_accuracy, labels, preds, masks, avg_fscore, [alphas, alphas_f, alphas_b, vids]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--no-cuda', action='store_true', default=False, help='does not use GPU')\n",
        "    parser.add_argument('--lr', type=float, default=0.0001, metavar='LR', help='learning rate')\n",
        "    parser.add_argument('--l2', type=float, default=0.00001, metavar='L2', help='L2 regularization weight')\n",
        "    parser.add_argument('--dropout', type=float, default=0.25, metavar='dropout', help='dropout rate')\n",
        "    parser.add_argument('--batch-size', type=int, default=32, metavar='BS', help='batch size')\n",
        "    parser.add_argument('--epochs', type=int, default=60, metavar='E', help='number of epochs')\n",
        "    parser.add_argument('--class-weight', action='store_true', default=False, help='use class weight')\n",
        "    parser.add_argument('--attention', action='store_true', default=True, help='use attention on top of lstm')\n",
        "    parser.add_argument('--tensorboard', action='store_true', default=False, help='Enables tensorboard log')\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    print(args)\n",
        "    seed_everything(seed=2023)\n",
        "    args.cuda = torch.cuda.is_available() and not args.no_cuda\n",
        "    if args.cuda:\n",
        "        print('Running on GPU')\n",
        "    else:\n",
        "        print('Running on CPU')\n",
        "\n",
        "    if args.tensorboard:\n",
        "        from tensorboardX import SummaryWriter\n",
        "        writer = SummaryWriter()\n",
        "\n",
        "    batch_size = args.batch_size\n",
        "    cuda       = args.cuda\n",
        "    n_epochs   = args.epochs\n",
        "    \n",
        "    n_classes  = 6\n",
        "    D_m = 100\n",
        "    D_e = 100\n",
        "    D_h = 100\n",
        "\n",
        "    model = LSTMModel(D_m, D_e, D_h,\n",
        "                      n_classes=n_classes,\n",
        "                      dropout=args.dropout,\n",
        "                      attention=args.attention)\n",
        "    if cuda:\n",
        "        model.cuda()\n",
        "        \n",
        "    loss_weights = torch.FloatTensor([1.0, 0.60072, 0.38066, 0.54019, 0.67924, 0.34332])\n",
        "    \n",
        "    if args.class_weight:\n",
        "        loss_function  = MaskedNLLLoss(loss_weights.cuda() if cuda else loss_weights)\n",
        "    else:\n",
        "        loss_function = MaskedNLLLoss()\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                           lr=args.lr,\n",
        "                           weight_decay=args.l2)\n",
        "\n",
        "    train_loader, valid_loader, test_loader = get_IEMOCAP_loaders('drive/MyDrive/IEMOCAP_features_raw.pkl',\n",
        "                                                                  batch_size=batch_size,\n",
        "                                                                  valid=0.0)\n",
        "\n",
        "    best_loss, best_label, best_pred, best_mask = None, None, None, None\n",
        "\n",
        "    for e in range(n_epochs):\n",
        "        start_time = time.time()\n",
        "        train_loss, train_acc, _, _, _, train_fscore, _ = train_or_eval_model(model, loss_function,\n",
        "                                               train_loader, e, optimizer, True)\n",
        "        valid_loss, valid_acc, _, _, _, val_fscore, _ = train_or_eval_model(model, loss_function, valid_loader, e)\n",
        "        test_loss, test_acc, test_label, test_pred, test_mask, test_fscore, attentions = train_or_eval_model(model, loss_function, test_loader, e)\n",
        "\n",
        "        if best_loss == None or best_loss > test_loss:\n",
        "            best_loss, best_label, best_pred, best_mask, best_attn =\\\n",
        "                    test_loss, test_label, test_pred, test_mask, attentions\n",
        "\n",
        "        if args.tensorboard:\n",
        "            writer.add_scalar('test: accuracy/loss', test_acc/test_loss, e)\n",
        "            writer.add_scalar('train: accuracy/loss', train_acc/train_loss, e)\n",
        "        print('epoch {} train_loss {} train_acc {} train_fscore {} valid_loss {} valid_acc {} val_fscore {} test_loss {} test_acc {} test_fscore {} time {}'.\\\n",
        "                format(e+1, train_loss, train_acc, train_fscore, valid_loss, valid_acc, val_fscore,\\\n",
        "                        test_loss, test_acc, test_fscore, round(time.time()-start_time, 2)))\n",
        "    if args.tensorboard:\n",
        "        writer.close()\n",
        "\n",
        "    print('Test performance..')\n",
        "    print('Loss {} F1-score {}'.format(best_loss,\n",
        "                                     round(f1_score(best_label, best_pred, sample_weight=best_mask, average='weighted')*100, 2)))\n",
        "    print(classification_report(best_label, best_pred, sample_weight=best_mask, digits=4))\n",
        "    print(confusion_matrix(best_label, best_pred, sample_weight=best_mask))"
      ],
      "metadata": {
        "id": "9rGSWh2p7n6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MELD with matching attention\n",
        "import numpy as np\n",
        "import argparse, time, pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "\n",
        "def get_train_valid_sampler(trainset, valid=0.1):\n",
        "    size = len(trainset)\n",
        "    idx = list(range(size))\n",
        "    split = int(valid*size)\n",
        "    return SubsetRandomSampler(idx[split:]), SubsetRandomSampler(idx[:split])\n",
        "\n",
        "def get_MELD_loaders(path, batch_size=32, valid=0.1, classify='emotion', num_workers=0, pin_memory=False):\n",
        "    trainset = MELDDataset(path, classify)\n",
        "    testset = MELDDataset(path, classify, train=False)\n",
        "    \n",
        "    train_sampler, valid_sampler = get_train_valid_sampler(trainset, valid)\n",
        "    train_loader = DataLoader(trainset,\n",
        "                              batch_size=batch_size,\n",
        "                              sampler=train_sampler,\n",
        "                              collate_fn=trainset.collate_fn,\n",
        "                              num_workers=num_workers,\n",
        "                              pin_memory=pin_memory)\n",
        "    valid_loader = DataLoader(trainset,\n",
        "                              batch_size=batch_size,\n",
        "                              sampler=valid_sampler,\n",
        "                              collate_fn=trainset.collate_fn,\n",
        "                              num_workers=num_workers,\n",
        "                              pin_memory=pin_memory)  \n",
        "    test_loader = DataLoader(testset,\n",
        "                             batch_size=batch_size,\n",
        "                             collate_fn=testset.collate_fn,\n",
        "                             num_workers=num_workers,\n",
        "                             pin_memory=pin_memory)\n",
        "\n",
        "    return train_loader, valid_loader, test_loader\n",
        "\n",
        "def train_or_eval_model(model, loss_function, dataloader, epoch, optimizer=None, train=False):\n",
        "    losses = []\n",
        "    preds = []\n",
        "    labels = []\n",
        "    masks = []\n",
        "    alphas, alphas_f, alphas_b, vids = [], [], [], []\n",
        "    assert not train or optimizer!=None\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    for data in dataloader:\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "        \n",
        "        textf, acouf, qmask, umask, label =\\\n",
        "                [d.cuda() for d in data[:-1]] if cuda else data[:-1]\n",
        "        \n",
        "        # log_prob = model(torch.cat((textf, acouf), dim=-1), qmask, umask) \n",
        "        log_prob, alpha, alpha_f, alpha_b = model(textf, qmask, umask) \n",
        "        lp_ = log_prob.transpose(0, 1).contiguous().view(-1, log_prob.size()[2])\n",
        "        labels_ = label.view(-1)\n",
        "        loss = loss_function(lp_, labels_, umask)\n",
        "\n",
        "        pred_ = torch.argmax(lp_, 1)\n",
        "        preds.append(pred_.data.cpu().numpy())\n",
        "        labels.append(labels_.data.cpu().numpy())\n",
        "        masks.append(umask.view(-1).cpu().numpy())\n",
        "\n",
        "        losses.append(loss.item()*masks[-1].sum())\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            if args.tensorboard:\n",
        "                for param in model.named_parameters():\n",
        "                    writer.add_histogram(param[0], param[1].grad, epoch)\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            alphas += alpha\n",
        "            alphas_f += alpha_f\n",
        "            alphas_b += alpha_b\n",
        "            vids += data[-1]\n",
        "\n",
        "    if preds!=[]:\n",
        "        preds  = np.concatenate(preds)\n",
        "        labels = np.concatenate(labels)\n",
        "        masks  = np.concatenate(masks)\n",
        "    else:\n",
        "        return float('nan'), float('nan'), [], [], [], float('nan'), []\n",
        "\n",
        "    avg_loss = round(np.sum(losses)/np.sum(masks), 4)\n",
        "    avg_accuracy = round(accuracy_score(labels, preds, sample_weight=masks)*100, 2)\n",
        "    avg_fscore = round(f1_score(labels, preds, sample_weight=masks, average='weighted')*100, 2)\n",
        "    return avg_loss, avg_accuracy, labels, preds, masks, avg_fscore, [alphas, alphas_f, alphas_b, vids]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--no-cuda', action='store_true', default=False, help='does not use GPU')\n",
        "    parser.add_argument('--lr', type=float, default=0.0001, metavar='LR', help='learning rate')\n",
        "    parser.add_argument('--l2', type=float, default=0.00001, metavar='L2', help='L2 regularization weight')\n",
        "    parser.add_argument('--dropout', type=float, default=0.25, metavar='dropout', help='dropout rate')\n",
        "    parser.add_argument('--batch-size', type=int, default=32, metavar='BS', help='batch size')\n",
        "    parser.add_argument('--epochs', type=int, default=60, metavar='E', help='number of epochs')\n",
        "    parser.add_argument('--attention', action='store_true', default=True, help='use attention on top of lstm')\n",
        "    parser.add_argument('--tensorboard', action='store_true', default=False, help='Enables tensorboard log')\n",
        "    parser.add_argument('--classify', default='emotion', help='classify emotion or sentiment')\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    print(args)\n",
        "    seed_everything(seed=2023)\n",
        "    args.cuda = torch.cuda.is_available() and not args.no_cuda\n",
        "    if args.cuda:\n",
        "        print('Running on GPU')\n",
        "    else:\n",
        "        print('Running on CPU')\n",
        "\n",
        "    if args.tensorboard:\n",
        "        from tensorboardX import SummaryWriter\n",
        "        writer = SummaryWriter()\n",
        "\n",
        "    batch_size = args.batch_size\n",
        "    cuda       = args.cuda\n",
        "    n_epochs   = args.epochs\n",
        "    \n",
        "    if args.classify == 'emotion':\n",
        "        n_classes  = 7\n",
        "    elif args.classify == 'sentiment':\n",
        "        n_classes  = 3\n",
        "\n",
        "    D_m = 600\n",
        "    D_e = 100\n",
        "    D_h = 100\n",
        "\n",
        "    model = LSTMModel(D_m, D_e, D_h,\n",
        "                      n_classes=n_classes,\n",
        "                      dropout=args.dropout,\n",
        "                      attention=args.attention)\n",
        "    if cuda:\n",
        "        model.cuda()\n",
        "        \n",
        "    loss_function = MaskedNLLLoss()\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                           lr=args.lr,\n",
        "                           weight_decay=args.l2)\n",
        "    \n",
        "    train_loader, valid_loader, test_loader = get_MELD_loaders('drive/MyDrive/MELD_features_raw.pkl',\n",
        "                                                                batch_size=batch_size,\n",
        "                                                                valid=0.0,\n",
        "                                                                classify=args.classify)\n",
        "\n",
        "    best_fscore, best_loss, best_label, best_pred, best_mask = None, None, None, None, None\n",
        "\n",
        "    for e in range(n_epochs):\n",
        "        start_time = time.time()\n",
        "        train_loss, train_acc, _, _, _, train_fscore, _ = train_or_eval_model(model, loss_function,\n",
        "                                               train_loader, e, optimizer, True)\n",
        "        valid_loss, valid_acc, _, _, _, val_fscore, _ = train_or_eval_model(model, loss_function, valid_loader, e)\n",
        "        test_loss, test_acc, test_label, test_pred, test_mask, test_fscore, attentions = train_or_eval_model(model, loss_function, test_loader, e)\n",
        "\n",
        "        if best_fscore == None or best_fscore < test_fscore:\n",
        "            best_fscore, best_loss, best_label, best_pred, best_mask, best_attn =\\\n",
        "                test_fscore, test_loss, test_label, test_pred, test_mask, attentions\n",
        "\n",
        "        if args.tensorboard:\n",
        "            writer.add_scalar('test: accuracy/loss', test_acc/test_loss, e)\n",
        "            writer.add_scalar('train: accuracy/loss', train_acc/train_loss, e)\n",
        "        print('epoch {} train_loss {} train_acc {} train_fscore {} valid_loss {} valid_acc {} val_fscore {} test_loss {} test_acc {} test_fscore {} time {}'.\\\n",
        "                format(e+1, train_loss, train_acc, train_fscore, valid_loss, valid_acc, val_fscore,\\\n",
        "                        test_loss, test_acc, test_fscore, round(time.time()-start_time, 2)))\n",
        "    if args.tensorboard:\n",
        "        writer.close()\n",
        "\n",
        "    print('Test performance..')\n",
        "    print('Loss {} F1-score {}'.format(best_loss,\n",
        "                                     round(f1_score(best_label, best_pred, sample_weight=best_mask, average='weighted')*100, 2)))\n",
        "    print(classification_report(best_label, best_pred, sample_weight=best_mask, digits=4))\n",
        "    print(confusion_matrix(best_label, best_pred, sample_weight=best_mask))"
      ],
      "metadata": {
        "id": "030b8d5T7n8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dKUIlkFQ6hav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AKztVCyy6hc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8iOTE_6-6hfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2IeXc0vZ6hhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "twnU-xCr6hkI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}