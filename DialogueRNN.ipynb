{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kashiwagi-Eri/HelloWorld/blob/master/DialogueRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The code below is modified from original repo:\n",
        "https://github.com/declare-lab/conv-emotion/tree/master/DialogueRNN\n",
        "\n",
        "###Modified by CS6493 course project Group 18\n",
        "Last modified: 2023.03.27"
      ],
      "metadata": {
        "id": "UVptRuZSSOW9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_ual1-KWKzKP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import torch\n",
        "import torch.nn\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "\n",
        "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "\n",
        "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "\n",
        "!pip install torch-geometric\n",
        "\n",
        "!pip install tensorboardX\n",
        "\n",
        "from torch_geometric.data import Data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DabzDhRJKzoP",
        "outputId": "8bca7ed4-9d7d-4c48-a0e8-81f4d8bc06ac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_scatter-2.1.1%2Bpt113cu116-cp39-cp39-linux_x86_64.whl (9.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.1+pt113cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_sparse-0.6.17%2Bpt113cu116-cp39-cp39-linux_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch-sparse) (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.9/dist-packages (from scipy->torch-sparse) (1.22.4)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.17+pt113cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_cluster-1.6.1%2Bpt113cu116-cp39-cp39-linux_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch-cluster) (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.9/dist-packages (from scipy->torch-cluster) (1.22.4)\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.1+pt113cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.13.1+cu116.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_spline_conv-1.2.2%2Bpt113cu116-cp39-cp39-linux_x86_64.whl (868 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.4/868.4 KB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.2+pt113cu116\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.3.0.tar.gz (616 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.2/616.2 KB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (4.65.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (5.9.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (1.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (2.27.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch-geometric) (2.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torch-geometric) (1.26.15)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch-geometric) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.3.0-py3-none-any.whl size=909897 sha256=6f5adad7d95eef50a474591e6c9bac10d8cf088e17a072486a362b06d974901a\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/7d/6b/17150450b80b4a3656a84330e22709ccd8dc0f8f4773ba4133\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.3.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 KB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from tensorboardX) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorboardX) (23.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.9/dist-packages (from tensorboardX) (3.19.6)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukj5kX25Kzqq",
        "outputId": "012ae187-721d-4e21-910e-fff822921c93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "class IEMOCAPDataset(Dataset):\n",
        "\n",
        "    def __init__(self, path, train=True):\n",
        "        self.videoIDs, self.videoSpeakers, self.videoLabels, self.videoText,\\\n",
        "        self.videoAudio, self.videoVisual, self.videoSentence, self.trainVid,\\\n",
        "        self.testVid = pickle.load(open(path, 'rb'), encoding='latin1')\n",
        "        '''\n",
        "        label index mapping = {'hap':0, 'sad':1, 'neu':2, 'ang':3, 'exc':4, 'fru':5}\n",
        "        '''\n",
        "        self.keys = [x for x in (self.trainVid if train else self.testVid)]\n",
        "\n",
        "        self.len = len(self.keys)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        vid = self.keys[index]\n",
        "        return torch.FloatTensor(self.videoText[vid]),\\\n",
        "               torch.FloatTensor(self.videoVisual[vid]),\\\n",
        "               torch.FloatTensor(self.videoAudio[vid]),\\\n",
        "               torch.FloatTensor([[1,0] if x=='M' else [0,1] for x in\\\n",
        "                                  self.videoSpeakers[vid]]),\\\n",
        "               torch.FloatTensor([1]*len(self.videoLabels[vid])),\\\n",
        "               torch.LongTensor(self.videoLabels[vid]),\\\n",
        "               vid\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def collate_fn(self, data):\n",
        "        dat = pd.DataFrame(data)\n",
        "        return [pad_sequence(dat[i]) if i<4 else pad_sequence(dat[i], True) if i<6 else dat[i].tolist() for i in dat]\n",
        "\n",
        "\n",
        "class MELDDataset(Dataset):\n",
        "\n",
        "    def __init__(self, path, n_classes, train=True):\n",
        "        if n_classes == 3:\n",
        "            self.videoIDs, self.videoSpeakers, _, self.videoText,\\\n",
        "            self.videoAudio, self.videoSentence, self.trainVid,\\\n",
        "            self.testVid, self.videoLabels = pickle.load(open(path, 'rb'))\n",
        "        elif n_classes == 7:\n",
        "            self.videoIDs, self.videoSpeakers, self.videoLabels, self.videoText,\\\n",
        "            self.videoAudio, self.videoSentence, self.trainVid,\\\n",
        "            self.testVid, _ = pickle.load(open(path, 'rb'))\n",
        "        '''\n",
        "        label index mapping = {'neutral': 0, 'surprise': 1, 'fear': 2, 'sadness': 3, 'joy': 4, 'disgust': 5, 'anger':6}\n",
        "        '''\n",
        "        self.keys = [x for x in (self.trainVid if train else self.testVid)]\n",
        "\n",
        "        self.len = len(self.keys)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        vid = self.keys[index]\n",
        "        return torch.FloatTensor(self.videoText[vid]),\\\n",
        "               torch.FloatTensor(self.videoAudio[vid]),\\\n",
        "               torch.FloatTensor(self.videoSpeakers[vid]),\\\n",
        "               torch.FloatTensor([1]*len(self.videoLabels[vid])),\\\n",
        "               torch.LongTensor(self.videoLabels[vid]),\\\n",
        "               vid\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def collate_fn(self, data):\n",
        "        dat = pd.DataFrame(data)\n",
        "        return [pad_sequence(dat[i]) if i<3 else pad_sequence(dat[i], True) if i<5 else dat[i].tolist() for i in dat]\n",
        "\n"
      ],
      "metadata": {
        "id": "IRMs9rmdKzs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class SimpleAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim):\n",
        "        super(SimpleAttention, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.scalar = nn.Linear(self.input_dim,1,bias=False)\n",
        "\n",
        "    def forward(self, M, x=None):\n",
        "        \"\"\"\n",
        "        M -> (seq_len, batch, vector)\n",
        "        x -> dummy argument for the compatibility with MatchingAttention\n",
        "        \"\"\"\n",
        "        scale = self.scalar(M) # seq_len, batch, 1\n",
        "        alpha = F.softmax(scale, dim=0).permute(1,2,0) # batch, 1, seq_len\n",
        "        attn_pool = torch.bmm(alpha, M.transpose(0,1))[:,0,:] # batch, vector\n",
        "\n",
        "        return attn_pool, alpha\n",
        "\n",
        "class MatchingAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, mem_dim, cand_dim, alpha_dim=None, att_type='general'):\n",
        "        super(MatchingAttention, self).__init__()\n",
        "        assert att_type!='concat' or alpha_dim!=None\n",
        "        assert att_type!='dot' or mem_dim==cand_dim\n",
        "        self.mem_dim = mem_dim\n",
        "        self.cand_dim = cand_dim\n",
        "        self.att_type = att_type\n",
        "        if att_type=='general':\n",
        "            self.transform = nn.Linear(cand_dim, mem_dim, bias=False)\n",
        "        if att_type=='general2':\n",
        "            self.transform = nn.Linear(cand_dim, mem_dim, bias=True)\n",
        "            #torch.nn.init.normal_(self.transform.weight,std=0.01)\n",
        "        elif att_type=='concat':\n",
        "            self.transform = nn.Linear(cand_dim+mem_dim, alpha_dim, bias=False)\n",
        "            self.vector_prod = nn.Linear(alpha_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, M, x, mask=None):\n",
        "        \"\"\"\n",
        "        M -> (seq_len, batch, mem_dim)\n",
        "        x -> (batch, cand_dim)\n",
        "        mask -> (batch, seq_len)\n",
        "        \"\"\"\n",
        "        if type(mask)==type(None):\n",
        "            mask = torch.ones(M.size(1), M.size(0)).type(M.type())\n",
        "\n",
        "        if self.att_type=='dot':\n",
        "            # vector = cand_dim = mem_dim\n",
        "            M_ = M.permute(1,2,0) # batch, vector, seqlen\n",
        "            x_ = x.unsqueeze(1) # batch, 1, vector\n",
        "            alpha = F.softmax(torch.bmm(x_, M_), dim=2) # batch, 1, seqlen\n",
        "        elif self.att_type=='general':\n",
        "            M_ = M.permute(1,2,0) # batch, mem_dim, seqlen\n",
        "            x_ = self.transform(x).unsqueeze(1) # batch, 1, mem_dim\n",
        "            alpha = F.softmax(torch.bmm(x_, M_), dim=2) # batch, 1, seqlen\n",
        "        elif self.att_type=='general2':\n",
        "            M_ = M.permute(1,2,0) # batch, mem_dim, seqlen\n",
        "            x_ = self.transform(x).unsqueeze(1) # batch, 1, mem_dim\n",
        "            alpha_ = F.softmax((torch.bmm(x_, M_))*mask.unsqueeze(1), dim=2) # batch, 1, seqlen\n",
        "            alpha_masked = alpha_*mask.unsqueeze(1) # batch, 1, seqlen\n",
        "            alpha_sum = torch.sum(alpha_masked, dim=2, keepdim=True) # batch, 1, 1\n",
        "            alpha = alpha_masked/alpha_sum # batch, 1, 1 ; normalized\n",
        "            #import ipdb;ipdb.set_trace()\n",
        "        else:\n",
        "            M_ = M.transpose(0,1) # batch, seqlen, mem_dim\n",
        "            x_ = x.unsqueeze(1).expand(-1,M.size()[0],-1) # batch, seqlen, cand_dim\n",
        "            M_x_ = torch.cat([M_,x_],2) # batch, seqlen, mem_dim+cand_dim\n",
        "            mx_a = F.tanh(self.transform(M_x_)) # batch, seqlen, alpha_dim\n",
        "            alpha = F.softmax(self.vector_prod(mx_a),1).transpose(1,2) # batch, 1, seqlen\n",
        "\n",
        "        attn_pool = torch.bmm(alpha, M.transpose(0,1))[:,0,:] # batch, mem_dim\n",
        "        \n",
        "        return attn_pool, alpha\n",
        "\n",
        "\n",
        "class DialogueRNNCell(nn.Module):\n",
        "\n",
        "    def __init__(self, D_m, D_g, D_p, D_e, listener_state=False,\n",
        "                            context_attention='simple', D_a=100, dropout=0.5):\n",
        "        super(DialogueRNNCell, self).__init__()\n",
        "\n",
        "        self.D_m = D_m\n",
        "        self.D_g = D_g\n",
        "        self.D_p = D_p\n",
        "        self.D_e = D_e\n",
        "\n",
        "        self.listener_state = listener_state\n",
        "        self.g_cell = nn.GRUCell(D_m+D_p,D_g)\n",
        "        self.p_cell = nn.GRUCell(D_m+D_g,D_p)\n",
        "        self.e_cell = nn.GRUCell(D_p,D_e)\n",
        "        if listener_state:\n",
        "            self.l_cell = nn.GRUCell(D_m+D_p,D_p)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        if context_attention=='simple':\n",
        "            self.attention = SimpleAttention(D_g)\n",
        "        else:\n",
        "            self.attention = MatchingAttention(D_g, D_m, D_a, context_attention)\n",
        "\n",
        "    def _select_parties(self, X, indices):\n",
        "        q0_sel = []\n",
        "        for idx, j in zip(indices, X):\n",
        "            q0_sel.append(j[idx].unsqueeze(0))\n",
        "        q0_sel = torch.cat(q0_sel,0)\n",
        "        return q0_sel\n",
        "\n",
        "    def forward(self, U, qmask, g_hist, q0, e0):\n",
        "        \"\"\"\n",
        "        U -> batch, D_m\n",
        "        qmask -> batch, party\n",
        "        g_hist -> t-1, batch, D_g\n",
        "        q0 -> batch, party, D_p\n",
        "        e0 -> batch, self.D_e\n",
        "        \"\"\"\n",
        "        qm_idx = torch.argmax(qmask, 1)\n",
        "        q0_sel = self._select_parties(q0, qm_idx)\n",
        "\n",
        "        g_ = self.g_cell(torch.cat([U,q0_sel], dim=1),\n",
        "                torch.zeros(U.size()[0],self.D_g).type(U.type()) if g_hist.size()[0]==0 else\n",
        "                g_hist[-1])\n",
        "        g_ = self.dropout(g_)\n",
        "        if g_hist.size()[0]==0:\n",
        "            c_ = torch.zeros(U.size()[0],self.D_g).type(U.type())\n",
        "            alpha = None\n",
        "        else:\n",
        "            c_, alpha = self.attention(g_hist,U)\n",
        "        # c_ = torch.zeros(U.size()[0],self.D_g).type(U.type()) if g_hist.size()[0]==0\\\n",
        "        #         else self.attention(g_hist,U)[0] # batch, D_g\n",
        "        U_c_ = torch.cat([U,c_], dim=1).unsqueeze(1).expand(-1,qmask.size()[1],-1)\n",
        "        qs_ = self.p_cell(U_c_.contiguous().view(-1,self.D_m+self.D_g),\n",
        "                q0.view(-1, self.D_p)).view(U.size()[0],-1,self.D_p)\n",
        "        qs_ = self.dropout(qs_)\n",
        "\n",
        "        if self.listener_state:\n",
        "            U_ = U.unsqueeze(1).expand(-1,qmask.size()[1],-1).contiguous().view(-1,self.D_m)\n",
        "            ss_ = self._select_parties(qs_, qm_idx).unsqueeze(1).\\\n",
        "                    expand(-1,qmask.size()[1],-1).contiguous().view(-1,self.D_p)\n",
        "            U_ss_ = torch.cat([U_,ss_],1)\n",
        "            ql_ = self.l_cell(U_ss_,q0.view(-1, self.D_p)).view(U.size()[0],-1,self.D_p)\n",
        "            ql_ = self.dropout(ql_)\n",
        "        else:\n",
        "            ql_ = q0\n",
        "        qmask_ = qmask.unsqueeze(2)\n",
        "        q_ = ql_*(1-qmask_) + qs_*qmask_\n",
        "        e0 = torch.zeros(qmask.size()[0], self.D_e).type(U.type()) if e0.size()[0]==0\\\n",
        "                else e0\n",
        "        e_ = self.e_cell(self._select_parties(q_,qm_idx), e0)\n",
        "        e_ = self.dropout(e_)\n",
        "\n",
        "        return g_,q_,e_,alpha\n",
        "\n",
        "class DialogueRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, D_m, D_g, D_p, D_e, listener_state=False,\n",
        "                            context_attention='simple', D_a=100, dropout=0.5):\n",
        "        super(DialogueRNN, self).__init__()\n",
        "\n",
        "        self.D_m = D_m\n",
        "        self.D_g = D_g\n",
        "        self.D_p = D_p\n",
        "        self.D_e = D_e\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.dialogue_cell = DialogueRNNCell(D_m, D_g, D_p, D_e,\n",
        "                            listener_state, context_attention, D_a, dropout)\n",
        "\n",
        "    def forward(self, U, qmask):\n",
        "        \"\"\"\n",
        "        U -> seq_len, batch, D_m\n",
        "        qmask -> seq_len, batch, party\n",
        "        \"\"\"\n",
        "\n",
        "        g_hist = torch.zeros(0).type(U.type()) # 0-dimensional tensor\n",
        "        q_ = torch.zeros(qmask.size()[1], qmask.size()[2],\n",
        "                                    self.D_p).type(U.type()) # batch, party, D_p\n",
        "        e_ = torch.zeros(0).type(U.type()) # batch, D_e\n",
        "        e = e_\n",
        "\n",
        "        alpha = []\n",
        "        for u_,qmask_ in zip(U, qmask):\n",
        "            g_, q_, e_, alpha_ = self.dialogue_cell(u_, qmask_, g_hist, q_, e_)\n",
        "            g_hist = torch.cat([g_hist, g_.unsqueeze(0)],0)\n",
        "            e = torch.cat([e, e_.unsqueeze(0)],0)\n",
        "            if type(alpha_)!=type(None):\n",
        "                alpha.append(alpha_[:,0,:])\n",
        "\n",
        "        return e,alpha # seq_len, batch, D_e\n",
        "class BiModel(nn.Module):\n",
        "\n",
        "    def __init__(self, D_m, D_g, D_p, D_e, D_h,\n",
        "                 n_classes=7, listener_state=False, context_attention='simple', D_a=100, dropout_rec=0.5,\n",
        "                 dropout=0.5):\n",
        "        super(BiModel, self).__init__()\n",
        "\n",
        "        self.D_m       = D_m\n",
        "        self.D_g       = D_g\n",
        "        self.D_p       = D_p\n",
        "        self.D_e       = D_e\n",
        "        self.D_h       = D_h\n",
        "        self.n_classes = n_classes\n",
        "        self.dropout   = nn.Dropout(dropout)\n",
        "        self.dropout_rec = nn.Dropout(dropout+0.15)\n",
        "        self.dialog_rnn_f = DialogueRNN(D_m, D_g, D_p, D_e,listener_state,\n",
        "                                    context_attention, D_a, dropout_rec)\n",
        "        self.dialog_rnn_r = DialogueRNN(D_m, D_g, D_p, D_e,listener_state,\n",
        "                                    context_attention, D_a, dropout_rec)\n",
        "        self.linear     = nn.Linear(2*D_e, 2*D_h)\n",
        "        self.smax_fc    = nn.Linear(2*D_h, n_classes)\n",
        "        self.matchatt = MatchingAttention(2*D_e,2*D_e,att_type='general2')\n",
        "\n",
        "    def _reverse_seq(self, X, mask):\n",
        "        \"\"\"\n",
        "        X -> seq_len, batch, dim\n",
        "        mask -> batch, seq_len\n",
        "        \"\"\"\n",
        "        X_ = X.transpose(0,1)\n",
        "        mask_sum = torch.sum(mask, 1).int()\n",
        "\n",
        "        xfs = []\n",
        "        for x, c in zip(X_, mask_sum):\n",
        "            xf = torch.flip(x[:c], [0])\n",
        "            xfs.append(xf)\n",
        "\n",
        "        return pad_sequence(xfs)\n",
        "\n",
        "\n",
        "    def forward(self, U, qmask, umask,att2=True):\n",
        "        \"\"\"\n",
        "        U -> seq_len, batch, D_m\n",
        "        qmask -> seq_len, batch, party\n",
        "        \"\"\"\n",
        "\n",
        "        emotions_f, alpha_f = self.dialog_rnn_f(U, qmask) # seq_len, batch, D_e\n",
        "        emotions_f = self.dropout_rec(emotions_f)\n",
        "        rev_U = self._reverse_seq(U, umask)\n",
        "        rev_qmask = self._reverse_seq(qmask, umask)\n",
        "        emotions_b, alpha_b = self.dialog_rnn_r(rev_U, rev_qmask)\n",
        "        emotions_b = self._reverse_seq(emotions_b, umask)\n",
        "        emotions_b = self.dropout_rec(emotions_b)\n",
        "        emotions = torch.cat([emotions_f,emotions_b],dim=-1)\n",
        "        if att2:\n",
        "            att_emotions = []\n",
        "            alpha = []\n",
        "            for t in emotions:\n",
        "                att_em, alpha_ = self.matchatt(emotions,t,mask=umask)\n",
        "                att_emotions.append(att_em.unsqueeze(0))\n",
        "                alpha.append(alpha_[:,0,:])\n",
        "            att_emotions = torch.cat(att_emotions,dim=0)\n",
        "            hidden = F.relu(self.linear(att_emotions))\n",
        "        else:\n",
        "            hidden = F.relu(self.linear(emotions))\n",
        "        #hidden = F.relu(self.linear(emotions))\n",
        "        hidden = self.dropout(hidden)\n",
        "        log_prob = F.log_softmax(self.smax_fc(hidden), 2) # seq_len, batch, n_classes\n",
        "        if att2:\n",
        "            return log_prob, alpha, alpha_f, alpha_b\n",
        "        else:\n",
        "            return log_prob, [], alpha_f, alpha_b\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self, D_m, D_g, D_p, D_e, D_h,\n",
        "                 n_classes=7, listener_state=False, context_attention='simple', D_a=100, dropout_rec=0.5,\n",
        "                 dropout=0.5):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.D_m       = D_m\n",
        "        self.D_g       = D_g\n",
        "        self.D_p       = D_p\n",
        "        self.D_e       = D_e\n",
        "        self.D_h       = D_h\n",
        "        self.n_classes = n_classes\n",
        "        self.dropout   = nn.Dropout(dropout)\n",
        "        #self.dropout_rec = nn.Dropout(0.2)\n",
        "        self.dropout_rec = nn.Dropout(dropout+0.15)\n",
        "        self.dialog_rnn = DialogueRNN(D_m, D_g, D_p, D_e,listener_state,\n",
        "                                    context_attention, D_a, dropout_rec)\n",
        "        self.linear1     = nn.Linear(D_e, D_h)\n",
        "        #self.linear2     = nn.Linear(D_h, D_h)\n",
        "        #self.linear3     = nn.Linear(D_h, D_h)\n",
        "        self.smax_fc    = nn.Linear(D_h, n_classes)\n",
        "\n",
        "        self.matchatt = MatchingAttention(D_e,D_e,att_type='general2')\n",
        "\n",
        "    def forward(self, U, qmask, umask=None, att2=False):\n",
        "        \"\"\"\n",
        "        U -> seq_len, batch, D_m\n",
        "        qmask -> seq_len, batch, party\n",
        "        \"\"\"\n",
        "\n",
        "        emotions,_ = self.dialog_rnn(U, qmask) # seq_len, batch, D_e\n",
        "        #print(emotions)\n",
        "        emotions = self.dropout_rec(emotions)\n",
        "\n",
        "        #emotions = emotions.unsqueeze(1)\n",
        "        if att2:\n",
        "            att_emotions = []\n",
        "            for t in emotions:\n",
        "                att_emotions.append(self.matchatt(emotions,t,mask=umask)[0].unsqueeze(0))\n",
        "            att_emotions = torch.cat(att_emotions,dim=0)\n",
        "            hidden = F.relu(self.linear1(att_emotions))\n",
        "        else:\n",
        "            hidden = F.relu(self.linear1(emotions))\n",
        "        #hidden = F.relu(self.linear2(hidden))\n",
        "        #hidden = F.relu(self.linear3(hidden))\n",
        "        hidden = self.dropout(hidden)\n",
        "        log_prob = F.log_softmax(self.smax_fc(hidden), 2) # seq_len, batch, n_classes\n",
        "        return log_prob\n",
        "\n",
        "\n",
        "class MaskedNLLLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, weight=None):\n",
        "        super(MaskedNLLLoss, self).__init__()\n",
        "        self.weight = weight\n",
        "        self.loss = nn.NLLLoss(weight=weight,\n",
        "                               reduction='sum')\n",
        "\n",
        "    def forward(self, pred, target, mask):\n",
        "        \"\"\"\n",
        "        pred -> batch*seq_len, n_classes\n",
        "        target -> batch*seq_len\n",
        "        mask -> batch, seq_len\n",
        "        \"\"\"\n",
        "        mask_ = mask.view(-1,1) # batch*seq_len, 1\n",
        "        if type(self.weight)==type(None):\n",
        "            loss = self.loss(pred*mask_, target)/torch.sum(mask)\n",
        "        else:\n",
        "            loss = self.loss(pred*mask_, target)\\\n",
        "                            /torch.sum(self.weight[target]*mask_.squeeze())\n",
        "        return loss\n",
        "\n"
      ],
      "metadata": {
        "id": "hKfzKnpDK_aZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed=2023):\n",
        "    print(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "GgOCQG6aG6Au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bimodel for IEMOCAP\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(1234)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.optim as optim\n",
        "\n",
        "import argparse\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score,\\\n",
        "                        classification_report, precision_recall_fscore_support\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def get_train_valid_sampler(trainset, valid=0.1):\n",
        "    size = len(trainset)\n",
        "    idx = list(range(size))\n",
        "    split = int(valid*size)\n",
        "    return SubsetRandomSampler(idx[split:]), SubsetRandomSampler(idx[:split])\n",
        "\n",
        "def get_IEMOCAP_loaders(path, batch_size=32, valid=0.1, num_workers=0, pin_memory=False):\n",
        "    trainset = IEMOCAPDataset(path=path)\n",
        "    train_sampler, valid_sampler = get_train_valid_sampler(trainset, valid)\n",
        "    train_loader = DataLoader(trainset,\n",
        "                              batch_size=batch_size,\n",
        "                              sampler=train_sampler,\n",
        "                              collate_fn=trainset.collate_fn,\n",
        "                              num_workers=num_workers,\n",
        "                              pin_memory=pin_memory)\n",
        "    valid_loader = DataLoader(trainset,\n",
        "                              batch_size=batch_size,\n",
        "                              sampler=valid_sampler,\n",
        "                              collate_fn=trainset.collate_fn,\n",
        "                              num_workers=num_workers,\n",
        "                              pin_memory=pin_memory)\n",
        "\n",
        "    testset = IEMOCAPDataset(path=path, train=False)\n",
        "    test_loader = DataLoader(testset,\n",
        "                             batch_size=batch_size,\n",
        "                             collate_fn=testset.collate_fn,\n",
        "                             num_workers=num_workers,\n",
        "                             pin_memory=pin_memory)\n",
        "\n",
        "    return train_loader, valid_loader, test_loader\n",
        "\n",
        "def train_or_eval_model(model, loss_function, dataloader, epoch, optimizer=None, train=False):\n",
        "    losses = []\n",
        "    preds = []\n",
        "    labels = []\n",
        "    masks = []\n",
        "    alphas, alphas_f, alphas_b, vids = [], [], [], []\n",
        "    assert not train or optimizer!=None\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    for data in dataloader:\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "        # import ipdb;ipdb.set_trace()\n",
        "        textf, visuf, acouf, qmask, umask, label =\\\n",
        "                [d.cuda() for d in data[:-1]] if cuda else data[:-1]\n",
        "        #log_prob = model(torch.cat((textf,acouf,visuf),dim=-1), qmask,umask,att2=True) # seq_len, batch, n_classes\n",
        "        log_prob, alpha, alpha_f, alpha_b = model(textf, qmask,umask,att2=True) # seq_len, batch, n_classes\n",
        "        lp_ = log_prob.transpose(0,1).contiguous().view(-1,log_prob.size()[2]) # batch*seq_len, n_classes\n",
        "        labels_ = label.view(-1) # batch*seq_len\n",
        "        loss = loss_function(lp_, labels_, umask)\n",
        "\n",
        "        pred_ = torch.argmax(lp_,1) # batch*seq_len\n",
        "        preds.append(pred_.data.cpu().numpy())\n",
        "        labels.append(labels_.data.cpu().numpy())\n",
        "        masks.append(umask.view(-1).cpu().numpy())\n",
        "\n",
        "        losses.append(loss.item()*masks[-1].sum())\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            if args.tensorboard:\n",
        "                for param in model.named_parameters():\n",
        "                    writer.add_histogram(param[0], param[1].grad, epoch)\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            alphas += alpha\n",
        "            alphas_f += alpha_f\n",
        "            alphas_b += alpha_b\n",
        "            vids += data[-1]\n",
        "\n",
        "    if preds!=[]:\n",
        "        preds  = np.concatenate(preds)\n",
        "        labels = np.concatenate(labels)\n",
        "        masks  = np.concatenate(masks)\n",
        "    else:\n",
        "        return float('nan'), float('nan'), [], [], [], float('nan'),[]\n",
        "\n",
        "    avg_loss = round(np.sum(losses)/np.sum(masks),4)\n",
        "    avg_accuracy = round(accuracy_score(labels,preds,sample_weight=masks)*100,2)\n",
        "    avg_fscore = round(f1_score(labels,preds,sample_weight=masks,average='weighted')*100,2)\n",
        "    return avg_loss, avg_accuracy, labels, preds, masks,avg_fscore, [alphas, alphas_f, alphas_b, vids]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "                        help='does not use GPU')\n",
        "    parser.add_argument('--lr', type=float, default=0.0001, metavar='LR',\n",
        "                        help='learning rate')\n",
        "    parser.add_argument('--l2', type=float, default=0.00001, metavar='L2',\n",
        "                        help='L2 regularization weight')\n",
        "    parser.add_argument('--rec-dropout', type=float, default=0.1,\n",
        "                        metavar='rec_dropout', help='rec_dropout rate')\n",
        "    parser.add_argument('--dropout', type=float, default=0.1, metavar='dropout',\n",
        "                        help='dropout rate')\n",
        "    parser.add_argument('--batch-size', type=int, default=30, metavar='BS',\n",
        "                        help='batch size')\n",
        "    parser.add_argument('--epochs', type=int, default=60, metavar='E',\n",
        "                        help='number of epochs')\n",
        "    parser.add_argument('--class-weight', action='store_true', default=True,\n",
        "                        help='class weight')\n",
        "    parser.add_argument('--active-listener', action='store_true', default=False,\n",
        "                        help='active listener')\n",
        "    parser.add_argument('--attention', default='general', help='Attention type')\n",
        "    parser.add_argument('--tensorboard', action='store_true', default=False,\n",
        "                        help='Enables tensorboard log')\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    print(args)\n",
        "    seed_everything(seed=2023)\n",
        "    args.cuda = torch.cuda.is_available() and not args.no_cuda\n",
        "    if args.cuda:\n",
        "        print('Running on GPU')\n",
        "    else:\n",
        "        print('Running on CPU')\n",
        "\n",
        "    if args.tensorboard:\n",
        "        from tensorboardX import SummaryWriter\n",
        "        writer = SummaryWriter()\n",
        "\n",
        "    batch_size = args.batch_size\n",
        "    n_classes  = 6\n",
        "    cuda       = args.cuda\n",
        "    n_epochs   = args.epochs\n",
        "\n",
        "    D_m = 100\n",
        "    D_g = 500\n",
        "    D_p = 500\n",
        "    D_e = 300\n",
        "    D_h = 300\n",
        "\n",
        "    D_a = 100 # concat attention\n",
        "\n",
        "    model = BiModel(D_m, D_g, D_p, D_e, D_h,\n",
        "                    n_classes=n_classes,\n",
        "                    listener_state=args.active_listener,\n",
        "                    context_attention=args.attention,\n",
        "                    dropout_rec=args.rec_dropout,\n",
        "                    dropout=args.dropout)\n",
        "    if cuda:\n",
        "        model.cuda()\n",
        "    loss_weights = torch.FloatTensor([\n",
        "                                        1/0.086747,\n",
        "                                        1/0.144406,\n",
        "                                        1/0.227883,\n",
        "                                        1/0.160585,\n",
        "                                        1/0.127711,\n",
        "                                        1/0.252668,\n",
        "                                        ])\n",
        "    if args.class_weight:\n",
        "        loss_function  = MaskedNLLLoss(loss_weights.cuda() if cuda else loss_weights)\n",
        "    else:\n",
        "        loss_function = MaskedNLLLoss()\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                           lr=args.lr,\n",
        "                           weight_decay=args.l2)\n",
        "\n",
        "    train_loader, valid_loader, test_loader =\\\n",
        "            get_IEMOCAP_loaders('drive/MyDrive/IEMOCAP_features_raw.pkl',\n",
        "                                valid=0.0,\n",
        "                                batch_size=batch_size,\n",
        "                                num_workers=2)\n",
        "\n",
        "    best_loss, best_label, best_pred, best_mask = None, None, None, None\n",
        "\n",
        "    for e in range(n_epochs):\n",
        "        start_time = time.time()\n",
        "        train_loss, train_acc, _,_,_,train_fscore,_= train_or_eval_model(model, loss_function,\n",
        "                                               train_loader, e, optimizer, True)\n",
        "        valid_loss, valid_acc, _,_,_,val_fscore,_= train_or_eval_model(model, loss_function, valid_loader, e)\n",
        "        test_loss, test_acc, test_label, test_pred, test_mask, test_fscore, attentions = train_or_eval_model(model, loss_function, test_loader, e)\n",
        "\n",
        "        if best_loss == None or best_loss > test_loss:\n",
        "            best_loss, best_label, best_pred, best_mask, best_attn =\\\n",
        "                    test_loss, test_label, test_pred, test_mask, attentions\n",
        "\n",
        "        if args.tensorboard:\n",
        "            writer.add_scalar('test: accuracy/loss',test_acc/test_loss,e)\n",
        "            writer.add_scalar('train: accuracy/loss',train_acc/train_loss,e)\n",
        "        print('epoch {} train_loss {} train_acc {} train_fscore{} valid_loss {} valid_acc {} val_fscore{} test_loss {} test_acc {} test_fscore {} time {}'.\\\n",
        "                format(e+1, train_loss, train_acc, train_fscore, valid_loss, valid_acc, val_fscore,\\\n",
        "                        test_loss, test_acc, test_fscore, round(time.time()-start_time,2)))\n",
        "    if args.tensorboard:\n",
        "        writer.close()\n",
        "\n",
        "    print('Test performance..')\n",
        "    print('Loss {} accuracy {}'.format(best_loss,\n",
        "                                     round(accuracy_score(best_label,best_pred,sample_weight=best_mask)*100,2)))\n",
        "    print(classification_report(best_label,best_pred,sample_weight=best_mask,digits=4))\n",
        "    print(confusion_matrix(best_label,best_pred,sample_weight=best_mask))\n",
        "    # with open('best_attention.p','wb') as f:\n",
        "    #     pickle.dump(best_attn+[best_label,best_pred,best_mask],f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_U3U_PDK_c5",
        "outputId": "29e19ca6-374f-4b5f-aa17-4d11a7fcc323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(no_cuda=False, lr=0.0001, l2=1e-05, rec_dropout=0.1, dropout=0.1, batch_size=30, epochs=60, class_weight=True, active_listener=False, attention='general', tensorboard=False)\n",
            "2023\n",
            "Running on GPU\n",
            "epoch 1 train_loss 1.7478 train_acc 29.83 train_fscore29.45 valid_loss nan valid_acc nan val_fscorenan test_loss 1.6893 test_acc 52.8 test_fscore 49.76 time 7.15\n",
            "epoch 2 train_loss 1.5615 train_acc 55.89 train_fscore54.0 valid_loss nan valid_acc nan val_fscorenan test_loss 1.5572 test_acc 59.15 test_fscore 55.64 time 9.12\n",
            "epoch 3 train_loss 1.3681 train_acc 61.86 train_fscore59.54 valid_loss nan valid_acc nan val_fscorenan test_loss 1.4074 test_acc 61.12 test_fscore 58.05 time 7.14\n",
            "epoch 4 train_loss 1.1795 train_acc 65.78 train_fscore63.86 valid_loss nan valid_acc nan val_fscorenan test_loss 1.2597 test_acc 62.66 test_fscore 60.26 time 8.8\n",
            "epoch 5 train_loss 1.0207 train_acc 68.23 train_fscore66.58 valid_loss nan valid_acc nan val_fscorenan test_loss 1.1336 test_acc 63.96 test_fscore 62.26 time 11.54\n",
            "epoch 6 train_loss 0.8807 train_acc 72.27 train_fscore71.54 valid_loss nan valid_acc nan val_fscorenan test_loss 1.0466 test_acc 64.02 test_fscore 63.42 time 10.4\n",
            "epoch 7 train_loss 0.7641 train_acc 74.82 train_fscore74.55 valid_loss nan valid_acc nan val_fscorenan test_loss 0.9958 test_acc 63.77 test_fscore 63.98 time 9.02\n",
            "epoch 8 train_loss 0.6737 train_acc 77.54 train_fscore77.44 valid_loss nan valid_acc nan val_fscorenan test_loss 0.9773 test_acc 62.91 test_fscore 63.17 time 8.25\n",
            "epoch 9 train_loss 0.5925 train_acc 81.27 train_fscore81.23 valid_loss nan valid_acc nan val_fscorenan test_loss 0.9638 test_acc 61.8 test_fscore 62.06 time 6.83\n",
            "epoch 10 train_loss 0.5359 train_acc 83.36 train_fscore83.34 valid_loss nan valid_acc nan val_fscorenan test_loss 0.9753 test_acc 61.86 test_fscore 62.07 time 8.08\n",
            "epoch 11 train_loss 0.4738 train_acc 85.9 train_fscore85.9 valid_loss nan valid_acc nan val_fscorenan test_loss 0.9875 test_acc 61.61 test_fscore 61.88 time 6.63\n",
            "epoch 12 train_loss 0.4415 train_acc 86.95 train_fscore86.96 valid_loss nan valid_acc nan val_fscorenan test_loss 0.9947 test_acc 62.05 test_fscore 62.45 time 8.22\n",
            "epoch 13 train_loss 0.4029 train_acc 87.95 train_fscore87.96 valid_loss nan valid_acc nan val_fscorenan test_loss 1.0306 test_acc 61.55 test_fscore 61.92 time 6.83\n",
            "epoch 14 train_loss 0.369 train_acc 89.26 train_fscore89.25 valid_loss nan valid_acc nan val_fscorenan test_loss 1.0879 test_acc 61.0 test_fscore 61.33 time 8.12\n",
            "epoch 15 train_loss 0.3373 train_acc 90.69 train_fscore90.71 valid_loss nan valid_acc nan val_fscorenan test_loss 1.1278 test_acc 59.95 test_fscore 60.23 time 6.84\n",
            "epoch 16 train_loss 0.3251 train_acc 90.93 train_fscore90.93 valid_loss nan valid_acc nan val_fscorenan test_loss 1.144 test_acc 60.44 test_fscore 60.64 time 8.0\n",
            "epoch 17 train_loss 0.3001 train_acc 91.53 train_fscore91.53 valid_loss nan valid_acc nan val_fscorenan test_loss 1.1459 test_acc 60.94 test_fscore 61.05 time 7.81\n",
            "epoch 18 train_loss 0.2819 train_acc 92.12 train_fscore92.12 valid_loss nan valid_acc nan val_fscorenan test_loss 1.1593 test_acc 60.94 test_fscore 61.05 time 7.55\n",
            "epoch 19 train_loss 0.2731 train_acc 92.22 train_fscore92.21 valid_loss nan valid_acc nan val_fscorenan test_loss 1.1796 test_acc 61.18 test_fscore 61.33 time 8.05\n",
            "epoch 20 train_loss 0.2608 train_acc 92.84 train_fscore92.84 valid_loss nan valid_acc nan val_fscorenan test_loss 1.1881 test_acc 61.31 test_fscore 61.5 time 7.36\n",
            "epoch 21 train_loss 0.2515 train_acc 92.96 train_fscore92.96 valid_loss nan valid_acc nan val_fscorenan test_loss 1.187 test_acc 60.75 test_fscore 60.95 time 7.79\n",
            "epoch 22 train_loss 0.2447 train_acc 92.98 train_fscore92.98 valid_loss nan valid_acc nan val_fscorenan test_loss 1.1992 test_acc 60.63 test_fscore 60.9 time 6.88\n",
            "epoch 23 train_loss 0.2425 train_acc 93.15 train_fscore93.16 valid_loss nan valid_acc nan val_fscorenan test_loss 1.2133 test_acc 60.75 test_fscore 61.0 time 8.02\n",
            "epoch 24 train_loss 0.2342 train_acc 93.37 train_fscore93.38 valid_loss nan valid_acc nan val_fscorenan test_loss 1.224 test_acc 60.57 test_fscore 60.7 time 7.22\n",
            "epoch 25 train_loss 0.2162 train_acc 93.75 train_fscore93.74 valid_loss nan valid_acc nan val_fscorenan test_loss 1.2351 test_acc 60.26 test_fscore 60.35 time 8.07\n",
            "epoch 26 train_loss 0.2138 train_acc 93.86 train_fscore93.85 valid_loss nan valid_acc nan val_fscorenan test_loss 1.253 test_acc 60.57 test_fscore 60.68 time 8.19\n",
            "epoch 27 train_loss 0.2058 train_acc 93.99 train_fscore93.99 valid_loss nan valid_acc nan val_fscorenan test_loss 1.2596 test_acc 59.95 test_fscore 60.14 time 7.16\n",
            "epoch 28 train_loss 0.1927 train_acc 94.53 train_fscore94.52 valid_loss nan valid_acc nan val_fscorenan test_loss 1.2493 test_acc 60.2 test_fscore 60.34 time 8.39\n",
            "epoch 29 train_loss 0.1953 train_acc 94.13 train_fscore94.13 valid_loss nan valid_acc nan val_fscorenan test_loss 1.2653 test_acc 60.69 test_fscore 60.65 time 6.71\n",
            "epoch 30 train_loss 0.1896 train_acc 94.25 train_fscore94.24 valid_loss nan valid_acc nan val_fscorenan test_loss 1.2983 test_acc 60.26 test_fscore 60.15 time 8.14\n",
            "epoch 31 train_loss 0.1928 train_acc 94.53 train_fscore94.52 valid_loss nan valid_acc nan val_fscorenan test_loss 1.3222 test_acc 59.83 test_fscore 59.84 time 6.77\n",
            "epoch 32 train_loss 0.1773 train_acc 95.01 train_fscore95.01 valid_loss nan valid_acc nan val_fscorenan test_loss 1.3345 test_acc 59.15 test_fscore 59.27 time 8.6\n",
            "epoch 33 train_loss 0.1864 train_acc 94.27 train_fscore94.27 valid_loss nan valid_acc nan val_fscorenan test_loss 1.3425 test_acc 59.46 test_fscore 59.57 time 7.16\n",
            "epoch 34 train_loss 0.1726 train_acc 94.94 train_fscore94.94 valid_loss nan valid_acc nan val_fscorenan test_loss 1.3566 test_acc 59.33 test_fscore 59.34 time 8.35\n",
            "epoch 35 train_loss 0.1684 train_acc 95.03 train_fscore95.02 valid_loss nan valid_acc nan val_fscorenan test_loss 1.3488 test_acc 59.7 test_fscore 59.66 time 7.75\n",
            "epoch 36 train_loss 0.173 train_acc 95.09 train_fscore95.09 valid_loss nan valid_acc nan val_fscorenan test_loss 1.3441 test_acc 59.4 test_fscore 59.42 time 7.59\n",
            "epoch 37 train_loss 0.1671 train_acc 94.89 train_fscore94.88 valid_loss nan valid_acc nan val_fscorenan test_loss 1.3401 test_acc 59.58 test_fscore 59.57 time 8.34\n",
            "epoch 38 train_loss 0.1595 train_acc 95.04 train_fscore95.04 valid_loss nan valid_acc nan val_fscorenan test_loss 1.3614 test_acc 59.7 test_fscore 59.62 time 7.17\n",
            "epoch 39 train_loss 0.1576 train_acc 95.27 train_fscore95.26 valid_loss nan valid_acc nan val_fscorenan test_loss 1.384 test_acc 59.89 test_fscore 59.8 time 8.07\n",
            "epoch 40 train_loss 0.1554 train_acc 95.39 train_fscore95.38 valid_loss nan valid_acc nan val_fscorenan test_loss 1.3984 test_acc 59.52 test_fscore 59.44 time 7.2\n",
            "epoch 41 train_loss 0.1535 train_acc 95.2 train_fscore95.19 valid_loss nan valid_acc nan val_fscorenan test_loss 1.4196 test_acc 58.84 test_fscore 58.83 time 8.13\n",
            "epoch 42 train_loss 0.1482 train_acc 95.4 train_fscore95.4 valid_loss nan valid_acc nan val_fscorenan test_loss 1.4186 test_acc 58.96 test_fscore 59.04 time 6.82\n",
            "epoch 43 train_loss 0.1513 train_acc 95.3 train_fscore95.3 valid_loss nan valid_acc nan val_fscorenan test_loss 1.4116 test_acc 59.46 test_fscore 59.53 time 8.0\n",
            "epoch 44 train_loss 0.1394 train_acc 95.9 train_fscore95.9 valid_loss nan valid_acc nan val_fscorenan test_loss 1.3974 test_acc 59.58 test_fscore 59.67 time 7.39\n",
            "epoch 45 train_loss 0.137 train_acc 95.49 train_fscore95.49 valid_loss nan valid_acc nan val_fscorenan test_loss 1.3912 test_acc 59.21 test_fscore 59.19 time 8.1\n",
            "epoch 46 train_loss 0.1386 train_acc 95.58 train_fscore95.57 valid_loss nan valid_acc nan val_fscorenan test_loss 1.4207 test_acc 59.4 test_fscore 59.31 time 8.42\n",
            "epoch 47 train_loss 0.1393 train_acc 95.89 train_fscore95.88 valid_loss nan valid_acc nan val_fscorenan test_loss 1.4493 test_acc 58.96 test_fscore 58.95 time 7.08\n",
            "epoch 48 train_loss 0.1302 train_acc 95.94 train_fscore95.93 valid_loss nan valid_acc nan val_fscorenan test_loss 1.4821 test_acc 59.03 test_fscore 59.05 time 8.68\n",
            "epoch 49 train_loss 0.1284 train_acc 96.18 train_fscore96.18 valid_loss nan valid_acc nan val_fscorenan test_loss 1.4825 test_acc 58.9 test_fscore 58.91 time 8.57\n",
            "epoch 50 train_loss 0.123 train_acc 96.23 train_fscore96.23 valid_loss nan valid_acc nan val_fscorenan test_loss 1.4799 test_acc 58.6 test_fscore 58.64 time 8.29\n",
            "epoch 51 train_loss 0.1218 train_acc 96.33 train_fscore96.33 valid_loss nan valid_acc nan val_fscorenan test_loss 1.4853 test_acc 58.53 test_fscore 58.57 time 7.93\n",
            "epoch 52 train_loss 0.1192 train_acc 96.37 train_fscore96.36 valid_loss nan valid_acc nan val_fscorenan test_loss 1.4894 test_acc 58.35 test_fscore 58.35 time 7.58\n",
            "epoch 53 train_loss 0.1183 train_acc 96.4 train_fscore96.4 valid_loss nan valid_acc nan val_fscorenan test_loss 1.5027 test_acc 58.47 test_fscore 58.41 time 8.2\n",
            "epoch 54 train_loss 0.1201 train_acc 96.44 train_fscore96.43 valid_loss nan valid_acc nan val_fscorenan test_loss 1.5116 test_acc 58.23 test_fscore 58.14 time 7.06\n",
            "epoch 55 train_loss 0.1159 train_acc 96.7 train_fscore96.69 valid_loss nan valid_acc nan val_fscorenan test_loss 1.5175 test_acc 58.23 test_fscore 58.16 time 8.06\n",
            "epoch 56 train_loss 0.1087 train_acc 96.63 train_fscore96.62 valid_loss nan valid_acc nan val_fscorenan test_loss 1.5226 test_acc 58.53 test_fscore 58.51 time 7.18\n",
            "epoch 57 train_loss 0.1086 train_acc 96.57 train_fscore96.57 valid_loss nan valid_acc nan val_fscorenan test_loss 1.529 test_acc 58.9 test_fscore 58.93 time 8.32\n",
            "epoch 58 train_loss 0.1089 train_acc 96.83 train_fscore96.83 valid_loss nan valid_acc nan val_fscorenan test_loss 1.5428 test_acc 58.66 test_fscore 58.63 time 7.2\n",
            "epoch 59 train_loss 0.1038 train_acc 96.85 train_fscore96.85 valid_loss nan valid_acc nan val_fscorenan test_loss 1.5783 test_acc 58.47 test_fscore 58.36 time 8.06\n",
            "epoch 60 train_loss 0.1131 train_acc 96.63 train_fscore96.62 valid_loss nan valid_acc nan val_fscorenan test_loss 1.573 test_acc 58.41 test_fscore 58.33 time 8.41\n",
            "Test performance..\n",
            "Loss 0.9638 accuracy 61.8\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4661    0.3819    0.4198     144.0\n",
            "           1     0.9077    0.7224    0.8045     245.0\n",
            "           2     0.5341    0.6120    0.5704     384.0\n",
            "           3     0.6319    0.6059    0.6186     170.0\n",
            "           4     0.6817    0.6589    0.6701     299.0\n",
            "           5     0.5646    0.6194    0.5907     381.0\n",
            "\n",
            "    accuracy                         0.6180    1623.0\n",
            "   macro avg     0.6310    0.6001    0.6124    1623.0\n",
            "weighted avg     0.6290    0.6180    0.6206    1623.0\n",
            "\n",
            "[[ 55.   1.  30.   0.  58.   0.]\n",
            " [  3. 177.  21.   3.   1.  40.]\n",
            " [ 10.  12. 235.  17.  28.  82.]\n",
            " [  0.   1.   7. 103.   0.  59.]\n",
            " [ 50.   1.  50.   0. 197.   1.]\n",
            " [  0.   3.  97.  40.   5. 236.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BiModel for MELD \n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.optim as optim\n",
        "\n",
        "import argparse\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score,\\\n",
        "                        classification_report, precision_recall_fscore_support\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def seed_everything(seed=2021):\n",
        "    print(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def get_train_valid_sampler(trainset, valid=0.1):\n",
        "    size = len(trainset)\n",
        "    idx = list(range(size))\n",
        "    split = int(valid*size)\n",
        "    return SubsetRandomSampler(idx[split:]), SubsetRandomSampler(idx[:split])\n",
        "\n",
        "def get_MELD_loaders(path, n_classes, batch_size=32, valid=0.1, num_workers=0, pin_memory=False):\n",
        "    trainset = MELDDataset(path=path, n_classes=n_classes)\n",
        "    train_sampler, valid_sampler = get_train_valid_sampler(trainset, valid)\n",
        "    train_loader = DataLoader(trainset,\n",
        "                              batch_size=batch_size,\n",
        "                              sampler=train_sampler,\n",
        "                              collate_fn=trainset.collate_fn,\n",
        "                              num_workers=num_workers,\n",
        "                              pin_memory=pin_memory)\n",
        "    valid_loader = DataLoader(trainset,\n",
        "                              batch_size=batch_size,\n",
        "                              sampler=valid_sampler,\n",
        "                              collate_fn=trainset.collate_fn,\n",
        "                              num_workers=num_workers,\n",
        "                              pin_memory=pin_memory)\n",
        "\n",
        "    testset = MELDDataset(path=path, n_classes=n_classes, train=False)\n",
        "    test_loader = DataLoader(testset,\n",
        "                             batch_size=batch_size,\n",
        "                             collate_fn=testset.collate_fn,\n",
        "                             num_workers=num_workers,\n",
        "                             pin_memory=pin_memory)\n",
        "\n",
        "    return train_loader, valid_loader, test_loader\n",
        "\n",
        "def train_or_eval_model(model, loss_function, dataloader, epoch, optimizer=None, train=False):\n",
        "    losses = []\n",
        "    preds = []\n",
        "    labels = []\n",
        "    masks = []\n",
        "    alphas, alphas_f, alphas_b, vids = [], [], [], []\n",
        "    assert not train or optimizer!=None\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    for data in dataloader:\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "        # import ipdb;ipdb.set_trace()\n",
        "        textf, acouf, qmask, umask, label =\\\n",
        "                [d.cuda() for d in data[:-1]] if cuda else data[:-1]\n",
        "        if feature_type == \"audio\":\n",
        "            log_prob, alpha, alpha_f, alpha_b = model(acouf, qmask,umask) # seq_len, batch, n_classes\n",
        "        elif feature_type == \"text\":\n",
        "            log_prob, alpha, alpha_f, alpha_b = model(textf, qmask,umask) # seq_len, batch, n_classes\n",
        "        else:\n",
        "            log_prob, alpha, alpha_f, alpha_b = model(torch.cat((textf,acouf),dim=-1), qmask,umask) # seq_len, batch, n_classes\n",
        "        lp_ = log_prob.transpose(0,1).contiguous().view(-1,log_prob.size()[2]) # batch*seq_len, n_classes\n",
        "        labels_ = label.view(-1) # batch*seq_len\n",
        "        loss = loss_function(lp_, labels_, umask)\n",
        "\n",
        "        pred_ = torch.argmax(lp_,1) # batch*seq_len\n",
        "        preds.append(pred_.data.cpu().numpy())\n",
        "        labels.append(labels_.data.cpu().numpy())\n",
        "        masks.append(umask.view(-1).cpu().numpy())\n",
        "\n",
        "        losses.append(loss.item()*masks[-1].sum())\n",
        "        if train:\n",
        "            loss.backward()\n",
        "#             if args.tensorboard:\n",
        "#                 for param in model.named_parameters():\n",
        "#                     writer.add_histogram(param[0], param[1].grad, epoch)\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            alphas += alpha\n",
        "            alphas_f += alpha_f\n",
        "            alphas_b += alpha_b\n",
        "            vids += data[-1]\n",
        "\n",
        "    if preds!=[]:\n",
        "        preds  = np.concatenate(preds)\n",
        "        labels = np.concatenate(labels)\n",
        "        masks  = np.concatenate(masks)\n",
        "    else:\n",
        "        return float('nan'), float('nan'), [], [], [], float('nan'),[]\n",
        "\n",
        "    avg_loss = round(np.sum(losses)/np.sum(masks),4)\n",
        "    avg_accuracy = round(accuracy_score(labels,preds,sample_weight=masks)*100,2)\n",
        "    avg_fscore = round(f1_score(labels,preds,sample_weight=masks,average='weighted')*100,2)\n",
        "    class_report = classification_report(labels,preds,sample_weight=masks,digits=4)\n",
        "    return avg_loss, avg_accuracy, labels, preds, masks,avg_fscore, [alphas, alphas_f, alphas_b, vids], class_report\n",
        "\n",
        "\n",
        "seed_everything(seed=2023)\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "if cuda:\n",
        "    print('Running on GPU')\n",
        "else:\n",
        "    print('Running on CPU')\n",
        "    \n",
        "tensorboard = True    \n",
        "if tensorboard:\n",
        "    from tensorboardX import SummaryWriter\n",
        "writer = SummaryWriter()\n",
        "\n",
        "# choose between 'sentiment' or 'emotion'\n",
        "classification_type = 'emotion'\n",
        "feature_type = 'multimodal'\n",
        "\n",
        "data_path = 'drive/MyDrive/'\n",
        "batch_size = 30\n",
        "n_classes = 3\n",
        "n_epochs = 100\n",
        "active_listener = False\n",
        "attention = 'general'\n",
        "class_weight = False\n",
        "dropout = 0.1\n",
        "rec_dropout = 0.1\n",
        "l2 = 0.00001\n",
        "lr = 0.0005\n",
        "\n",
        "if feature_type == 'text':\n",
        "    print(\"Running on the text features........\")\n",
        "    D_m = 600\n",
        "elif feature_type == 'audio':\n",
        "    print(\"Running on the audio features........\")\n",
        "    D_m = 300\n",
        "else:\n",
        "    print(\"Running on the multimodal features........\")\n",
        "    D_m = 900\n",
        "D_g = 150\n",
        "D_p = 150\n",
        "D_e = 100\n",
        "D_h = 100\n",
        "\n",
        "D_a = 100 # concat attention\n",
        "\n",
        "loss_weights = torch.FloatTensor([1.0,1.0,1.0])\n",
        "\n",
        "if classification_type.strip().lower() == 'emotion':\n",
        "    n_classes = 7\n",
        "    loss_weights = torch.FloatTensor([1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
        "\n",
        "model = BiModel(D_m, D_g, D_p, D_e, D_h,\n",
        "                n_classes=n_classes,\n",
        "                listener_state=active_listener,\n",
        "                context_attention=attention,\n",
        "                dropout_rec=rec_dropout,\n",
        "                dropout=dropout)\n",
        "\n",
        "if cuda:\n",
        "    model.cuda()\n",
        "if class_weight:\n",
        "    loss_function  = MaskedNLLLoss(loss_weights.cuda() if cuda else loss_weights)\n",
        "else:\n",
        "    loss_function = MaskedNLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=lr,\n",
        "                       weight_decay=l2)\n",
        "\n",
        "train_loader, valid_loader, test_loader =\\\n",
        "        get_MELD_loaders(data_path + 'MELD_features_raw.pkl', n_classes,\n",
        "                            valid=0.0,\n",
        "                            batch_size=batch_size,\n",
        "                            num_workers=0)\n",
        "\n",
        "best_fscore, best_loss, best_label, best_pred, best_mask = None, None, None, None, None\n",
        "\n",
        "\n",
        "for e in range(n_epochs):\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc, _,_,_,train_fscore,_,_= train_or_eval_model(model, loss_function,\n",
        "                                           train_loader, e, optimizer, True)\n",
        "    valid_loss, valid_acc, _,_,_,val_fscore,_= train_or_eval_model(model, loss_function, valid_loader, e)\n",
        "    test_loss, test_acc, test_label, test_pred, test_mask, test_fscore, attentions, test_class_report = train_or_eval_model(model, loss_function, test_loader, e)\n",
        "\n",
        "    if best_fscore == None or best_fscore < test_fscore:\n",
        "        best_fscore, best_loss, best_label, best_pred, best_mask, best_attn =\\\n",
        "                test_fscore, test_loss, test_label, test_pred, test_mask, attentions\n",
        "\n",
        "#     if args.tensorboard:\n",
        "#         writer.add_scalar('test: accuracy/loss',test_acc/test_loss,e)\n",
        "#         writer.add_scalar('train: accuracy/loss',train_acc/train_loss,e)\n",
        "    print('epoch {} train_loss {} train_acc {} train_fscore {} valid_loss {} valid_acc {} val_fscore {} test_loss {} test_acc {} test_fscore {} time {}'.\\\n",
        "            format(e+1, train_loss, train_acc, train_fscore, valid_loss, valid_acc, val_fscore,\\\n",
        "                    test_loss, test_acc, test_fscore, round(time.time()-start_time,2)))\n",
        "    print (test_class_report)\n",
        "if tensorboard:\n",
        "    writer.close()\n",
        "\n",
        "print('Test performance..')\n",
        "print('Fscore {} accuracy {}'.format(best_fscore,\n",
        "                                 round(accuracy_score(best_label,best_pred,sample_weight=best_mask)*100,2)))\n",
        "print(classification_report(best_label,best_pred,sample_weight=best_mask,digits=4))\n",
        "print(confusion_matrix(best_label,best_pred,sample_weight=best_mask))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4cJp2kqK_e-",
        "outputId": "4fca9ec8-35e1-46f4-b407-3e3b056d22ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023\n",
            "Running on GPU\n",
            "Running on the multimodal features........\n",
            "epoch 1 train_loss 1.5797 train_acc 46.4 train_fscore 29.7 valid_loss nan valid_acc nan val_fscore nan test_loss 1.5061 test_acc 48.12 test_fscore 31.27 time 13.92\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4812    1.0000    0.6498    1256.0\n",
            "           1     0.0000    0.0000    0.0000     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.0000    0.0000    0.0000     208.0\n",
            "           4     0.0000    0.0000    0.0000     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.0000    0.0000    0.0000     345.0\n",
            "\n",
            "    accuracy                         0.4812    2610.0\n",
            "   macro avg     0.0687    0.1429    0.0928    2610.0\n",
            "weighted avg     0.2316    0.4812    0.3127    2610.0\n",
            "\n",
            "epoch 2 train_loss 1.4226 train_acc 49.2 train_fscore 37.99 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4228 test_acc 48.89 test_fscore 43.98 time 14.22\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7099    0.7404    0.7249    1256.0\n",
            "           1     0.0000    0.0000    0.0000     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.0000    0.0000    0.0000     208.0\n",
            "           4     0.3407    0.2289    0.2738     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.2466    0.7362    0.3695     345.0\n",
            "\n",
            "    accuracy                         0.4889    2610.0\n",
            "   macro avg     0.1853    0.2436    0.1954    2610.0\n",
            "weighted avg     0.4267    0.4889    0.4398    2610.0\n",
            "\n",
            "epoch 3 train_loss 1.2282 train_acc 55.87 train_fscore 50.2 valid_loss nan valid_acc nan val_fscore nan test_loss 1.415 test_acc 48.47 test_fscore 42.18 time 13.99\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7274    0.7564    0.7416    1256.0\n",
            "           1     0.1711    0.0463    0.0728     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.0000    0.0000    0.0000     208.0\n",
            "           4     0.2459    0.7512    0.3706     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.0000    0.0000    0.0000     345.0\n",
            "\n",
            "    accuracy                         0.4847    2610.0\n",
            "   macro avg     0.1635    0.2220    0.1693    2610.0\n",
            "weighted avg     0.4063    0.4847    0.4218    2610.0\n",
            "\n",
            "epoch 4 train_loss 1.1746 train_acc 58.12 train_fscore 53.46 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3352 test_acc 54.94 test_fscore 48.65 time 13.36\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7071    0.8209    0.7598    1256.0\n",
            "           1     0.3333    0.0036    0.0070     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.0000    0.0000    0.0000     208.0\n",
            "           4     0.3223    0.6766    0.4366     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4262    0.3768    0.4000     345.0\n",
            "\n",
            "    accuracy                         0.5494    2610.0\n",
            "   macro avg     0.2556    0.2683    0.2291    2610.0\n",
            "weighted avg     0.4822    0.5494    0.4865    2610.0\n",
            "\n",
            "epoch 5 train_loss 1.1582 train_acc 59.86 train_fscore 55.44 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3058 test_acc 56.25 test_fscore 52.43 time 14.07\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7102    0.8272    0.7643    1256.0\n",
            "           1     0.3108    0.2456    0.2744     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3333    0.0288    0.0531     208.0\n",
            "           4     0.4620    0.4080    0.4333     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3442    0.5507    0.4236     345.0\n",
            "\n",
            "    accuracy                         0.5625    2610.0\n",
            "   macro avg     0.3086    0.2943    0.2784    2610.0\n",
            "weighted avg     0.5184    0.5625    0.5243    2610.0\n",
            "\n",
            "epoch 6 train_loss 1.1131 train_acc 61.9 train_fscore 58.3 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2676 test_acc 56.9 test_fscore 53.79 time 13.35\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7195    0.8025    0.7588    1256.0\n",
            "           1     0.4390    0.2562    0.3236     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2679    0.0721    0.1136     208.0\n",
            "           4     0.4247    0.5124    0.4645     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3651    0.5333    0.4335     345.0\n",
            "\n",
            "    accuracy                         0.5690    2610.0\n",
            "   macro avg     0.3166    0.3110    0.2991    2610.0\n",
            "weighted avg     0.5285    0.5690    0.5379    2610.0\n",
            "\n",
            "epoch 7 train_loss 1.0574 train_acc 65.57 train_fscore 61.88 valid_loss nan valid_acc nan val_fscore nan test_loss 1.253 test_acc 56.74 test_fscore 55.6 time 13.74\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7353    0.7476    0.7414    1256.0\n",
            "           1     0.4084    0.4520    0.4291     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.1958    0.1779    0.1864     208.0\n",
            "           4     0.5487    0.4900    0.5177     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3819    0.5246    0.4420     345.0\n",
            "\n",
            "    accuracy                         0.5674    2610.0\n",
            "   macro avg     0.3243    0.3417    0.3309    2610.0\n",
            "weighted avg     0.5484    0.5674    0.5560    2610.0\n",
            "\n",
            "epoch 8 train_loss 1.011 train_acc 67.4 train_fscore 64.39 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2492 test_acc 58.16 test_fscore 55.65 time 13.53\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7286    0.7675    0.7476    1256.0\n",
            "           1     0.4565    0.5409    0.4951     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3556    0.0769    0.1265     208.0\n",
            "           4     0.4597    0.5672    0.5078     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3826    0.4580    0.4169     345.0\n",
            "\n",
            "    accuracy                         0.5816    2610.0\n",
            "   macro avg     0.3404    0.3444    0.3277    2610.0\n",
            "weighted avg     0.5495    0.5816    0.5565    2610.0\n",
            "\n",
            "epoch 9 train_loss 0.9928 train_acc 67.95 train_fscore 64.99 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2519 test_acc 57.93 test_fscore 55.44 time 14.14\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7269    0.7755    0.7504    1256.0\n",
            "           1     0.3952    0.6441    0.4899     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2597    0.0962    0.1404     208.0\n",
            "           4     0.5012    0.5299    0.5151     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4000    0.3594    0.3786     345.0\n",
            "\n",
            "    accuracy                         0.5793    2610.0\n",
            "   macro avg     0.3261    0.3436    0.3249    2610.0\n",
            "weighted avg     0.5431    0.5793    0.5544    2610.0\n",
            "\n",
            "epoch 10 train_loss 1.0016 train_acc 67.41 train_fscore 64.37 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2435 test_acc 57.36 test_fscore 55.85 time 13.15\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7401    0.7412    0.7407    1256.0\n",
            "           1     0.4769    0.4769    0.4769     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2148    0.1394    0.1691     208.0\n",
            "           4     0.4703    0.5721    0.5163     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3870    0.5014    0.4369     345.0\n",
            "\n",
            "    accuracy                         0.5736    2610.0\n",
            "   macro avg     0.3270    0.3473    0.3343    2610.0\n",
            "weighted avg     0.5482    0.5736    0.5585    2610.0\n",
            "\n",
            "epoch 11 train_loss 0.9676 train_acc 68.69 train_fscore 66.2 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2423 test_acc 58.66 test_fscore 56.4 time 13.79\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7212    0.7866    0.7525    1256.0\n",
            "           1     0.4479    0.5196    0.4811     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2621    0.1298    0.1736     208.0\n",
            "           4     0.5048    0.5199    0.5123     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4055    0.4667    0.4340     345.0\n",
            "\n",
            "    accuracy                         0.5866    2610.0\n",
            "   macro avg     0.3345    0.3461    0.3362    2610.0\n",
            "weighted avg     0.5475    0.5866    0.5640    2610.0\n",
            "\n",
            "epoch 12 train_loss 0.9679 train_acc 68.85 train_fscore 66.13 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2241 test_acc 58.93 test_fscore 56.74 time 13.27\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7199    0.7962    0.7561    1256.0\n",
            "           1     0.4218    0.5658    0.4833     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2880    0.1731    0.2162     208.0\n",
            "           4     0.5341    0.4876    0.5098     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4176    0.4261    0.4218     345.0\n",
            "\n",
            "    accuracy                         0.5893    2610.0\n",
            "   macro avg     0.3402    0.3498    0.3410    2610.0\n",
            "weighted avg     0.5523    0.5893    0.5674    2610.0\n",
            "\n",
            "epoch 13 train_loss 0.9542 train_acc 69.1 train_fscore 66.66 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2127 test_acc 59.54 test_fscore 56.77 time 13.85\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7114    0.8185    0.7612    1256.0\n",
            "           1     0.4468    0.5231    0.4820     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2759    0.1154    0.1627     208.0\n",
            "           4     0.5444    0.4726    0.5060     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4125    0.4783    0.4430     345.0\n",
            "\n",
            "    accuracy                         0.5954    2610.0\n",
            "   macro avg     0.3416    0.3440    0.3364    2610.0\n",
            "weighted avg     0.5508    0.5954    0.5677    2610.0\n",
            "\n",
            "epoch 14 train_loss 0.9527 train_acc 68.59 train_fscore 65.99 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2097 test_acc 58.47 test_fscore 56.62 time 13.35\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7241    0.7731    0.7478    1256.0\n",
            "           1     0.4790    0.4875    0.4832     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2745    0.2019    0.2327     208.0\n",
            "           4     0.4618    0.5721    0.5111     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4398    0.4232    0.4313     345.0\n",
            "\n",
            "    accuracy                         0.5847    2610.0\n",
            "   macro avg     0.3399    0.3511    0.3437    2610.0\n",
            "weighted avg     0.5512    0.5847    0.5662    2610.0\n",
            "\n",
            "epoch 15 train_loss 0.9473 train_acc 69.33 train_fscore 66.92 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2188 test_acc 59.2 test_fscore 56.57 time 13.19\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7161    0.7994    0.7555    1256.0\n",
            "           1     0.4596    0.5267    0.4909     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3188    0.1058    0.1588     208.0\n",
            "           4     0.5313    0.4851    0.5072     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3911    0.5101    0.4428     345.0\n",
            "\n",
            "    accuracy                         0.5920    2610.0\n",
            "   macro avg     0.3453    0.3467    0.3364    2610.0\n",
            "weighted avg     0.5530    0.5920    0.5657    2610.0\n",
            "\n",
            "epoch 16 train_loss 0.9333 train_acc 69.41 train_fscore 67.03 valid_loss nan valid_acc nan val_fscore nan test_loss 1.214 test_acc 59.54 test_fscore 57.05 time 13.78\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7167    0.7978    0.7551    1256.0\n",
            "           1     0.4723    0.5160    0.4932     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3333    0.1394    0.1966     208.0\n",
            "           4     0.5035    0.5398    0.5210     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4160    0.4667    0.4399     345.0\n",
            "\n",
            "    accuracy                         0.5954    2610.0\n",
            "   macro avg     0.3488    0.3514    0.3437    2610.0\n",
            "weighted avg     0.5549    0.5954    0.5705    2610.0\n",
            "\n",
            "epoch 17 train_loss 0.9318 train_acc 69.81 train_fscore 67.45 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2418 test_acc 58.12 test_fscore 56.64 time 13.96\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7358    0.7540    0.7448    1256.0\n",
            "           1     0.4794    0.4555    0.4672     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3333    0.2019    0.2515     208.0\n",
            "           4     0.5096    0.5274    0.5183     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3658    0.5449    0.4377     345.0\n",
            "\n",
            "    accuracy                         0.5812    2610.0\n",
            "   macro avg     0.3463    0.3548    0.3456    2610.0\n",
            "weighted avg     0.5591    0.5812    0.5664    2610.0\n",
            "\n",
            "epoch 18 train_loss 0.9327 train_acc 69.33 train_fscore 66.93 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2271 test_acc 57.39 test_fscore 56.5 time 13.11\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7508    0.7221    0.7362    1256.0\n",
            "           1     0.4599    0.5302    0.4926     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2569    0.3125    0.2820     208.0\n",
            "           4     0.4534    0.6169    0.5227     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4640    0.3739    0.4141     345.0\n",
            "\n",
            "    accuracy                         0.5739    2610.0\n",
            "   macro avg     0.3407    0.3651    0.3496    2610.0\n",
            "weighted avg     0.5625    0.5739    0.5650    2610.0\n",
            "\n",
            "epoch 19 train_loss 0.9227 train_acc 69.63 train_fscore 67.22 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2118 test_acc 59.73 test_fscore 57.12 time 13.85\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7145    0.8129    0.7605    1256.0\n",
            "           1     0.4408    0.5302    0.4814     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3696    0.1635    0.2267     208.0\n",
            "           4     0.5000    0.5249    0.5121     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4377    0.4174    0.4273     345.0\n",
            "\n",
            "    accuracy                         0.5973    2610.0\n",
            "   macro avg     0.3518    0.3498    0.3440    2610.0\n",
            "weighted avg     0.5556    0.5973    0.5712    2610.0\n",
            "\n",
            "epoch 20 train_loss 0.917 train_acc 69.65 train_fscore 67.29 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2322 test_acc 59.04 test_fscore 57.16 time 13.98\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7214    0.7938    0.7559    1256.0\n",
            "           1     0.4671    0.4804    0.4737     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2747    0.2404    0.2564     208.0\n",
            "           4     0.5035    0.5299    0.5164     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4371    0.4232    0.4300     345.0\n",
            "\n",
            "    accuracy                         0.5904    2610.0\n",
            "   macro avg     0.3434    0.3525    0.3475    2610.0\n",
            "weighted avg     0.5547    0.5904    0.5716    2610.0\n",
            "\n",
            "epoch 21 train_loss 0.9154 train_acc 70.11 train_fscore 67.83 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2064 test_acc 59.2 test_fscore 56.2 time 14.21\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7162    0.8097    0.7601    1256.0\n",
            "           1     0.4803    0.4769    0.4786     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.4054    0.0721    0.1224     208.0\n",
            "           4     0.5382    0.4552    0.4933     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3670    0.5681    0.4460     345.0\n",
            "\n",
            "    accuracy                         0.5920    2610.0\n",
            "   macro avg     0.3582    0.3403    0.3286    2610.0\n",
            "weighted avg     0.5601    0.5920    0.5620    2610.0\n",
            "\n",
            "epoch 22 train_loss 0.9103 train_acc 69.85 train_fscore 67.42 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2169 test_acc 60.34 test_fscore 57.21 time 13.16\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7106    0.8328    0.7669    1256.0\n",
            "           1     0.4294    0.5409    0.4787     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3770    0.1106    0.1710     208.0\n",
            "           4     0.5234    0.5000    0.5115     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4513    0.4435    0.4474     345.0\n",
            "\n",
            "    accuracy                         0.6034    2610.0\n",
            "   macro avg     0.3560    0.3468    0.3393    2610.0\n",
            "weighted avg     0.5585    0.6034    0.5721    2610.0\n",
            "\n",
            "epoch 23 train_loss 0.9119 train_acc 70.02 train_fscore 67.61 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2175 test_acc 59.31 test_fscore 56.91 time 13.88\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7169    0.7986    0.7556    1256.0\n",
            "           1     0.4554    0.4911    0.4726     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3690    0.1490    0.2123     208.0\n",
            "           4     0.5273    0.4801    0.5026     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3996    0.5304    0.4558     345.0\n",
            "\n",
            "    accuracy                         0.5931    2610.0\n",
            "   macro avg     0.3526    0.3499    0.3427    2610.0\n",
            "weighted avg     0.5575    0.5931    0.5691    2610.0\n",
            "\n",
            "epoch 24 train_loss 0.9035 train_acc 70.28 train_fscore 67.95 valid_loss nan valid_acc nan val_fscore nan test_loss 1.237 test_acc 59.23 test_fscore 56.59 time 12.97\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7215    0.7962    0.7570    1256.0\n",
            "           1     0.4444    0.5552    0.4937     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.4200    0.1010    0.1628     208.0\n",
            "           4     0.5263    0.4975    0.5115     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3815    0.4899    0.4289     345.0\n",
            "\n",
            "    accuracy                         0.5923    2610.0\n",
            "   macro avg     0.3563    0.3485    0.3363    2610.0\n",
            "weighted avg     0.5600    0.5923    0.5659    2610.0\n",
            "\n",
            "epoch 25 train_loss 0.895 train_acc 70.69 train_fscore 68.42 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2319 test_acc 59.85 test_fscore 57.06 time 14.05\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7163    0.8121    0.7612    1256.0\n",
            "           1     0.4922    0.4484    0.4693     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.4237    0.1202    0.1873     208.0\n",
            "           4     0.5037    0.5025    0.5031     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4021    0.5478    0.4638     345.0\n",
            "\n",
            "    accuracy                         0.5985    2610.0\n",
            "   macro avg     0.3626    0.3473    0.3407    2610.0\n",
            "weighted avg     0.5622    0.5985    0.5706    2610.0\n",
            "\n",
            "epoch 26 train_loss 0.8931 train_acc 70.45 train_fscore 68.11 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2232 test_acc 59.81 test_fscore 57.18 time 13.34\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7194    0.8145    0.7640    1256.0\n",
            "           1     0.4903    0.4520    0.4704     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3889    0.1346    0.2000     208.0\n",
            "           4     0.5389    0.4652    0.4993     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3843    0.5681    0.4585     345.0\n",
            "\n",
            "    accuracy                         0.5981    2610.0\n",
            "   macro avg     0.3603    0.3478    0.3417    2610.0\n",
            "weighted avg     0.5638    0.5981    0.5718    2610.0\n",
            "\n",
            "epoch 27 train_loss 0.8816 train_acc 70.85 train_fscore 68.53 valid_loss nan valid_acc nan val_fscore nan test_loss 1.24 test_acc 58.58 test_fscore 56.4 time 14.21\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7244    0.7954    0.7583    1256.0\n",
            "           1     0.4745    0.4626    0.4685     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3434    0.1635    0.2215     208.0\n",
            "           4     0.5614    0.3980    0.4658     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3595    0.5971    0.4488     345.0\n",
            "\n",
            "    accuracy                         0.5858    2610.0\n",
            "   macro avg     0.3519    0.3452    0.3375    2610.0\n",
            "weighted avg     0.5611    0.5858    0.5640    2610.0\n",
            "\n",
            "epoch 28 train_loss 0.8853 train_acc 70.7 train_fscore 68.42 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2339 test_acc 57.47 test_fscore 56.37 time 13.56\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7374    0.7444    0.7409    1256.0\n",
            "           1     0.4716    0.4733    0.4725     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2886    0.2788    0.2836     208.0\n",
            "           4     0.5493    0.4154    0.4731     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3730    0.6000    0.4600     345.0\n",
            "\n",
            "    accuracy                         0.5747    2610.0\n",
            "   macro avg     0.3457    0.3589    0.3472    2610.0\n",
            "weighted avg     0.5625    0.5747    0.5637    2610.0\n",
            "\n",
            "epoch 29 train_loss 0.8767 train_acc 71.01 train_fscore 68.72 valid_loss nan valid_acc nan val_fscore nan test_loss 1.229 test_acc 59.12 test_fscore 56.91 time 14.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7272    0.7811    0.7532    1256.0\n",
            "           1     0.4563    0.5018    0.4780     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3737    0.1779    0.2410     208.0\n",
            "           4     0.4487    0.6095    0.5169     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4528    0.4029    0.4264     345.0\n",
            "\n",
            "    accuracy                         0.5912    2610.0\n",
            "   macro avg     0.3512    0.3533    0.3451    2610.0\n",
            "weighted avg     0.5578    0.5912    0.5691    2610.0\n",
            "\n",
            "epoch 30 train_loss 0.8732 train_acc 71.03 train_fscore 68.79 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2046 test_acc 60.31 test_fscore 57.77 time 13.43\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7151    0.8193    0.7636    1256.0\n",
            "           1     0.4980    0.4448    0.4699     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3700    0.1779    0.2403     208.0\n",
            "           4     0.5289    0.5000    0.5141     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4136    0.5275    0.4637     345.0\n",
            "\n",
            "    accuracy                         0.6031    2610.0\n",
            "   macro avg     0.3608    0.3528    0.3502    2610.0\n",
            "weighted avg     0.5634    0.6031    0.5777    2610.0\n",
            "\n",
            "epoch 31 train_loss 0.8728 train_acc 71.01 train_fscore 68.82 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2443 test_acc 59.39 test_fscore 56.84 time 14.27\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7186    0.8113    0.7622    1256.0\n",
            "           1     0.4649    0.4947    0.4793     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3452    0.1394    0.1986     208.0\n",
            "           4     0.5574    0.4229    0.4809     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3829    0.5594    0.4547     345.0\n",
            "\n",
            "    accuracy                         0.5939    2610.0\n",
            "   macro avg     0.3527    0.3468    0.3394    2610.0\n",
            "weighted avg     0.5598    0.5939    0.5684    2610.0\n",
            "\n",
            "epoch 32 train_loss 0.8672 train_acc 71.17 train_fscore 68.76 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2316 test_acc 58.43 test_fscore 56.15 time 13.03\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7196    0.7866    0.7516    1256.0\n",
            "           1     0.4591    0.5196    0.4875     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3182    0.1346    0.1892     208.0\n",
            "           4     0.5207    0.4378    0.4757     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3793    0.5420    0.4463     345.0\n",
            "\n",
            "    accuracy                         0.5843    2610.0\n",
            "   macro avg     0.3424    0.3458    0.3358    2610.0\n",
            "weighted avg     0.5514    0.5843    0.5615    2610.0\n",
            "\n",
            "epoch 33 train_loss 0.86 train_acc 71.38 train_fscore 69.25 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2552 test_acc 58.39 test_fscore 56.1 time 13.91\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7266    0.7723    0.7487    1256.0\n",
            "           1     0.4982    0.4840    0.4910     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.4444    0.0962    0.1581     208.0\n",
            "           4     0.5163    0.4726    0.4935     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3562    0.6029    0.4478     345.0\n",
            "\n",
            "    accuracy                         0.5839    2610.0\n",
            "   macro avg     0.3631    0.3469    0.3342    2610.0\n",
            "weighted avg     0.5653    0.5839    0.5610    2610.0\n",
            "\n",
            "epoch 34 train_loss 0.8601 train_acc 71.61 train_fscore 69.26 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2443 test_acc 57.78 test_fscore 56.48 time 13.15\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7305    0.7596    0.7447    1256.0\n",
            "           1     0.4629    0.4662    0.4645     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2624    0.2548    0.2585     208.0\n",
            "           4     0.5287    0.4577    0.4907     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3949    0.5391    0.4559     345.0\n",
            "\n",
            "    accuracy                         0.5778    2610.0\n",
            "   macro avg     0.3399    0.3539    0.3449    2610.0\n",
            "weighted avg     0.5559    0.5778    0.5648    2610.0\n",
            "\n",
            "epoch 35 train_loss 0.8443 train_acc 72.11 train_fscore 69.82 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2229 test_acc 57.74 test_fscore 55.49 time 13.78\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7227    0.7739    0.7474    1256.0\n",
            "           1     0.4560    0.5160    0.4841     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3433    0.1106    0.1673     208.0\n",
            "           4     0.5167    0.4229    0.4651     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3575    0.5710    0.4397     345.0\n",
            "\n",
            "    accuracy                         0.5774    2610.0\n",
            "   macro avg     0.3423    0.3421    0.3291    2610.0\n",
            "weighted avg     0.5511    0.5774    0.5549    2610.0\n",
            "\n",
            "epoch 36 train_loss 0.8483 train_acc 71.77 train_fscore 69.63 valid_loss nan valid_acc nan val_fscore nan test_loss 1.228 test_acc 58.47 test_fscore 56.09 time 13.4\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7188    0.7834    0.7497    1256.0\n",
            "           1     0.4704    0.4804    0.4754     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3529    0.1154    0.1739     208.0\n",
            "           4     0.4893    0.5124    0.5006     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3831    0.5130    0.4387     345.0\n",
            "\n",
            "    accuracy                         0.5847    2610.0\n",
            "   macro avg     0.3449    0.3435    0.3340    2610.0\n",
            "weighted avg     0.5507    0.5847    0.5609    2610.0\n",
            "\n",
            "epoch 37 train_loss 0.8342 train_acc 72.13 train_fscore 69.88 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2426 test_acc 58.89 test_fscore 56.53 time 13.91\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7229    0.7914    0.7556    1256.0\n",
            "           1     0.4471    0.5409    0.4895     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3766    0.1394    0.2035     208.0\n",
            "           4     0.5296    0.4229    0.4703     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3863    0.5565    0.4561     345.0\n",
            "\n",
            "    accuracy                         0.5889    2610.0\n",
            "   macro avg     0.3518    0.3502    0.3393    2610.0\n",
            "weighted avg     0.5587    0.5889    0.5653    2610.0\n",
            "\n",
            "epoch 38 train_loss 0.837 train_acc 71.84 train_fscore 69.76 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2567 test_acc 57.05 test_fscore 55.78 time 13.48\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7434    0.7221    0.7326    1256.0\n",
            "           1     0.4557    0.5302    0.4901     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3864    0.1635    0.2297     208.0\n",
            "           4     0.5026    0.4826    0.4924     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3510    0.5942    0.4413     345.0\n",
            "\n",
            "    accuracy                         0.5705    2610.0\n",
            "   macro avg     0.3484    0.3561    0.3409    2610.0\n",
            "weighted avg     0.5614    0.5705    0.5578    2610.0\n",
            "\n",
            "epoch 39 train_loss 0.8262 train_acc 72.15 train_fscore 70.05 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2623 test_acc 58.2 test_fscore 55.56 time 13.84\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7164    0.7882    0.7506    1256.0\n",
            "           1     0.4844    0.4413    0.4618     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.4419    0.0913    0.1514     208.0\n",
            "           4     0.5361    0.4254    0.4743     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3530    0.6232    0.4507     345.0\n",
            "\n",
            "    accuracy                         0.5820    2610.0\n",
            "   macro avg     0.3617    0.3385    0.3270    2610.0\n",
            "weighted avg     0.5613    0.5820    0.5556    2610.0\n",
            "\n",
            "epoch 40 train_loss 0.8268 train_acc 72.28 train_fscore 70.24 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2649 test_acc 59.0 test_fscore 56.46 time 13.46\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7144    0.8105    0.7594    1256.0\n",
            "           1     0.4741    0.4235    0.4474     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3556    0.1538    0.2148     208.0\n",
            "           4     0.5333    0.4179    0.4686     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3867    0.5884    0.4667     345.0\n",
            "\n",
            "    accuracy                         0.5900    2610.0\n",
            "   macro avg     0.3520    0.3420    0.3367    2610.0\n",
            "weighted avg     0.5564    0.5900    0.5646    2610.0\n",
            "\n",
            "epoch 41 train_loss 0.8229 train_acc 71.94 train_fscore 70.01 valid_loss nan valid_acc nan val_fscore nan test_loss 1.264 test_acc 59.12 test_fscore 56.4 time 14.12\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7136    0.8073    0.7576    1256.0\n",
            "           1     0.4808    0.4448    0.4621     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3731    0.1202    0.1818     208.0\n",
            "           4     0.5027    0.4677    0.4845     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3914    0.5536    0.4586     345.0\n",
            "\n",
            "    accuracy                         0.5912    2610.0\n",
            "   macro avg     0.3517    0.3419    0.3349    2610.0\n",
            "weighted avg     0.5541    0.5912    0.5640    2610.0\n",
            "\n",
            "epoch 42 train_loss 0.8178 train_acc 72.73 train_fscore 70.65 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2916 test_acc 57.7 test_fscore 55.81 time 13.78\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7300    0.7643    0.7468    1256.0\n",
            "           1     0.4589    0.5160    0.4858     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3908    0.1635    0.2305     208.0\n",
            "           4     0.5754    0.3607    0.4434     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3485    0.6435    0.4521     345.0\n",
            "\n",
            "    accuracy                         0.5770    2610.0\n",
            "   macro avg     0.3577    0.3497    0.3369    2610.0\n",
            "weighted avg     0.5666    0.5770    0.5581    2610.0\n",
            "\n",
            "epoch 43 train_loss 0.8021 train_acc 73.07 train_fscore 71.04 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2665 test_acc 58.58 test_fscore 56.95 time 14.09\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7285    0.7691    0.7483    1256.0\n",
            "           1     0.4774    0.4520    0.4644     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3594    0.2212    0.2738     208.0\n",
            "           4     0.5185    0.4876    0.5026     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3826    0.5623    0.4554     345.0\n",
            "\n",
            "    accuracy                         0.5858    2610.0\n",
            "   macro avg     0.3524    0.3560    0.3492    2610.0\n",
            "weighted avg     0.5611    0.5858    0.5695    2610.0\n",
            "\n",
            "epoch 44 train_loss 0.7982 train_acc 73.31 train_fscore 71.47 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3115 test_acc 56.59 test_fscore 55.84 time 13.77\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7422    0.7197    0.7308    1256.0\n",
            "           1     0.4743    0.4591    0.4665     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2850    0.2837    0.2843     208.0\n",
            "           4     0.5313    0.4428    0.4830     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3594    0.6000    0.4495     345.0\n",
            "\n",
            "    accuracy                         0.5659    2610.0\n",
            "   macro avg     0.3417    0.3579    0.3449    2610.0\n",
            "weighted avg     0.5603    0.5659    0.5584    2610.0\n",
            "\n",
            "epoch 45 train_loss 0.7975 train_acc 73.14 train_fscore 71.34 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2718 test_acc 58.62 test_fscore 56.68 time 14.15\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7153    0.7842    0.7482    1256.0\n",
            "           1     0.4878    0.4270    0.4554     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3739    0.2067    0.2663     208.0\n",
            "           4     0.5446    0.4403    0.4869     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3825    0.5942    0.4654     345.0\n",
            "\n",
            "    accuracy                         0.5862    2610.0\n",
            "   macro avg     0.3577    0.3504    0.3460    2610.0\n",
            "weighted avg     0.5610    0.5862    0.5668    2610.0\n",
            "\n",
            "epoch 46 train_loss 0.7835 train_acc 73.46 train_fscore 71.63 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2708 test_acc 59.16 test_fscore 56.83 time 14.02\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7139    0.8185    0.7626    1256.0\n",
            "           1     0.4597    0.4875    0.4732     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3448    0.1923    0.2469     208.0\n",
            "           4     0.4919    0.4552    0.4729     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4239    0.4522    0.4376     345.0\n",
            "\n",
            "    accuracy                         0.5916    2610.0\n",
            "   macro avg     0.3478    0.3437    0.3419    2610.0\n",
            "weighted avg     0.5523    0.5916    0.5683    2610.0\n",
            "\n",
            "epoch 47 train_loss 0.781 train_acc 73.65 train_fscore 71.99 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2962 test_acc 57.13 test_fscore 55.81 time 14.43\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7329    0.7580    0.7452    1256.0\n",
            "           1     0.4310    0.5445    0.4811     281.0\n",
            "           2     0.0208    0.0200    0.0204      50.0\n",
            "           3     0.3836    0.1346    0.1993     208.0\n",
            "           4     0.4959    0.4527    0.4733     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3924    0.5072    0.4425     345.0\n",
            "\n",
            "    accuracy                         0.5713    2610.0\n",
            "   macro avg     0.3509    0.3453    0.3374    2610.0\n",
            "weighted avg     0.5583    0.5713    0.5581    2610.0\n",
            "\n",
            "epoch 48 train_loss 0.7784 train_acc 73.42 train_fscore 71.76 valid_loss nan valid_acc nan val_fscore nan test_loss 1.289 test_acc 57.16 test_fscore 55.97 time 14.15\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7353    0.7389    0.7371    1256.0\n",
            "           1     0.4794    0.4555    0.4672     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2950    0.1971    0.2363     208.0\n",
            "           4     0.4952    0.5174    0.5061     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3674    0.5420    0.4379     345.0\n",
            "\n",
            "    accuracy                         0.5716    2610.0\n",
            "   macro avg     0.3389    0.3501    0.3407    2610.0\n",
            "weighted avg     0.5538    0.5716    0.5597    2610.0\n",
            "\n",
            "epoch 49 train_loss 0.7716 train_acc 73.96 train_fscore 72.32 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3297 test_acc 57.16 test_fscore 55.24 time 14.01\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7267    0.7516    0.7389    1256.0\n",
            "           1     0.4823    0.3879    0.4300     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3553    0.1298    0.1901     208.0\n",
            "           4     0.5039    0.4801    0.4917     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3498    0.6348    0.4511     345.0\n",
            "\n",
            "    accuracy                         0.5716    2610.0\n",
            "   macro avg     0.3454    0.3406    0.3288    2610.0\n",
            "weighted avg     0.5538    0.5716    0.5524    2610.0\n",
            "\n",
            "epoch 50 train_loss 0.7652 train_acc 74.36 train_fscore 72.71 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3176 test_acc 57.78 test_fscore 56.09 time 13.86\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7250    0.7747    0.7490    1256.0\n",
            "           1     0.4594    0.4626    0.4610     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2797    0.1923    0.2279     208.0\n",
            "           4     0.5586    0.4030    0.4682     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3745    0.5884    0.4577     345.0\n",
            "\n",
            "    accuracy                         0.5778    2610.0\n",
            "   macro avg     0.3425    0.3459    0.3377    2610.0\n",
            "weighted avg     0.5562    0.5778    0.5609    2610.0\n",
            "\n",
            "epoch 51 train_loss 0.7546 train_acc 74.81 train_fscore 73.17 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3202 test_acc 56.44 test_fscore 55.59 time 14.65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7335    0.7341    0.7338    1256.0\n",
            "           1     0.4265    0.5267    0.4713     281.0\n",
            "           2     0.0526    0.0200    0.0290      50.0\n",
            "           3     0.2842    0.2500    0.2660     208.0\n",
            "           4     0.4951    0.5075    0.5012     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3822    0.4232    0.4017     345.0\n",
            "\n",
            "    accuracy                         0.5644    2610.0\n",
            "   macro avg     0.3392    0.3516    0.3433    2610.0\n",
            "weighted avg     0.5493    0.5644    0.5559    2610.0\n",
            "\n",
            "epoch 52 train_loss 0.7454 train_acc 74.64 train_fscore 73.01 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3278 test_acc 56.78 test_fscore 55.29 time 13.73\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7236    0.7588    0.7408    1256.0\n",
            "           1     0.4010    0.5836    0.4754     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3053    0.1923    0.2360     208.0\n",
            "           4     0.5162    0.4353    0.4723     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3807    0.4348    0.4060     345.0\n",
            "\n",
            "    accuracy                         0.5678    2610.0\n",
            "   macro avg     0.3324    0.3435    0.3329    2610.0\n",
            "weighted avg     0.5456    0.5678    0.5529    2610.0\n",
            "\n",
            "epoch 53 train_loss 0.7423 train_acc 74.53 train_fscore 73.13 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3751 test_acc 59.12 test_fscore 56.1 time 14.13\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7160    0.8049    0.7579    1256.0\n",
            "           1     0.4900    0.4377    0.4624     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.5161    0.0769    0.1339     208.0\n",
            "           4     0.4765    0.5050    0.4903     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3893    0.5507    0.4562     345.0\n",
            "\n",
            "    accuracy                         0.5912    2610.0\n",
            "   macro avg     0.3697    0.3393    0.3287    2610.0\n",
            "weighted avg     0.5633    0.5912    0.5610    2610.0\n",
            "\n",
            "epoch 54 train_loss 0.7361 train_acc 75.02 train_fscore 73.55 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3137 test_acc 57.74 test_fscore 56.18 time 13.84\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7281    0.7675    0.7473    1256.0\n",
            "           1     0.4889    0.3915    0.4348     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2826    0.1875    0.2254     208.0\n",
            "           4     0.4800    0.5373    0.5070     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3964    0.5159    0.4484     345.0\n",
            "\n",
            "    accuracy                         0.5774    2610.0\n",
            "   macro avg     0.3394    0.3428    0.3376    2610.0\n",
            "weighted avg     0.5519    0.5774    0.5618    2610.0\n",
            "\n",
            "epoch 55 train_loss 0.7275 train_acc 74.9 train_fscore 73.62 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3623 test_acc 57.01 test_fscore 55.94 time 14.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7391    0.7309    0.7350    1256.0\n",
            "           1     0.4790    0.4057    0.4393     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3038    0.2308    0.2623     208.0\n",
            "           4     0.4942    0.5274    0.5102     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3670    0.5681    0.4460     345.0\n",
            "\n",
            "    accuracy                         0.5701    2610.0\n",
            "   macro avg     0.3404    0.3518    0.3418    2610.0\n",
            "weighted avg     0.5561    0.5701    0.5594    2610.0\n",
            "\n",
            "epoch 56 train_loss 0.7259 train_acc 75.18 train_fscore 73.73 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3263 test_acc 58.39 test_fscore 56.2 time 13.88\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7203    0.7874    0.7524    1256.0\n",
            "           1     0.4618    0.4093    0.4340     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3924    0.1490    0.2160     208.0\n",
            "           4     0.5156    0.4527    0.4821     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3826    0.6000    0.4673     345.0\n",
            "\n",
            "    accuracy                         0.5839    2610.0\n",
            "   macro avg     0.3533    0.3426    0.3360    2610.0\n",
            "weighted avg     0.5576    0.5839    0.5620    2610.0\n",
            "\n",
            "epoch 57 train_loss 0.7118 train_acc 75.76 train_fscore 74.53 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3679 test_acc 57.89 test_fscore 56.23 time 13.83\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7266    0.7723    0.7487    1256.0\n",
            "           1     0.4338    0.5480    0.4843     281.0\n",
            "           2     0.0435    0.0200    0.0274      50.0\n",
            "           3     0.3750    0.1731    0.2368     208.0\n",
            "           4     0.4676    0.5199    0.4923     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4184    0.4087    0.4135     345.0\n",
            "\n",
            "    accuracy                         0.5789    2610.0\n",
            "   macro avg     0.3521    0.3489    0.3433    2610.0\n",
            "weighted avg     0.5544    0.5789    0.5623    2610.0\n",
            "\n",
            "epoch 58 train_loss 0.7192 train_acc 75.47 train_fscore 74.25 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3778 test_acc 57.2 test_fscore 56.07 time 13.94\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7366    0.7548    0.7456    1256.0\n",
            "           1     0.4718    0.4164    0.4423     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3590    0.2019    0.2585     208.0\n",
            "           4     0.4836    0.4776    0.4806     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3767    0.5623    0.4512     345.0\n",
            "\n",
            "    accuracy                         0.5720    2610.0\n",
            "   macro avg     0.3468    0.3447    0.3397    2610.0\n",
            "weighted avg     0.5582    0.5720    0.5607    2610.0\n",
            "\n",
            "epoch 59 train_loss 0.7005 train_acc 75.95 train_fscore 74.72 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3616 test_acc 56.7 test_fscore 55.56 time 13.7\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7219    0.7564    0.7387    1256.0\n",
            "           1     0.4045    0.5801    0.4766     281.0\n",
            "           2     0.0909    0.0200    0.0328      50.0\n",
            "           3     0.3108    0.2212    0.2584     208.0\n",
            "           4     0.5102    0.4353    0.4698     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4155    0.4203    0.4179     345.0\n",
            "\n",
            "    accuracy                         0.5670    2610.0\n",
            "   macro avg     0.3505    0.3476    0.3420    2610.0\n",
            "weighted avg     0.5509    0.5670    0.5556    2610.0\n",
            "\n",
            "epoch 60 train_loss 0.6954 train_acc 76.02 train_fscore 74.84 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3998 test_acc 55.59 test_fscore 55.34 time 13.99\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7468    0.7022    0.7238    1256.0\n",
            "           1     0.4481    0.4911    0.4686     281.0\n",
            "           2     0.0476    0.0400    0.0435      50.0\n",
            "           3     0.2949    0.2212    0.2527     208.0\n",
            "           4     0.5053    0.4726    0.4884     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3655    0.5594    0.4422     345.0\n",
            "\n",
            "    accuracy                         0.5559    2610.0\n",
            "   macro avg     0.3440    0.3552    0.3456    2610.0\n",
            "weighted avg     0.5582    0.5559    0.5534    2610.0\n",
            "\n",
            "epoch 61 train_loss 0.6784 train_acc 76.66 train_fscore 75.51 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3888 test_acc 57.09 test_fscore 55.76 time 13.83\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7252    0.7564    0.7405    1256.0\n",
            "           1     0.4737    0.4484    0.4607     281.0\n",
            "           2     0.1250    0.0200    0.0345      50.0\n",
            "           3     0.2930    0.2212    0.2521     208.0\n",
            "           4     0.5015    0.4254    0.4603     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3784    0.5681    0.4542     345.0\n",
            "\n",
            "    accuracy                         0.5709    2610.0\n",
            "   macro avg     0.3567    0.3485    0.3432    2610.0\n",
            "weighted avg     0.5530    0.5709    0.5576    2610.0\n",
            "\n",
            "epoch 62 train_loss 0.679 train_acc 76.44 train_fscore 75.33 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4594 test_acc 56.44 test_fscore 55.37 time 13.93\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7274    0.7436    0.7354    1256.0\n",
            "           1     0.4600    0.4911    0.4750     281.0\n",
            "           2     0.0476    0.0200    0.0282      50.0\n",
            "           3     0.2805    0.2212    0.2473     208.0\n",
            "           4     0.5507    0.3781    0.4484     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3640    0.5855    0.4489     345.0\n",
            "\n",
            "    accuracy                         0.5644    2610.0\n",
            "   macro avg     0.3472    0.3485    0.3405    2610.0\n",
            "weighted avg     0.5558    0.5644    0.5537    2610.0\n",
            "\n",
            "epoch 63 train_loss 0.6691 train_acc 77.12 train_fscore 76.14 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4214 test_acc 54.1 test_fscore 54.2 time 13.84\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7262    0.7412    0.7336    1256.0\n",
            "           1     0.4463    0.4875    0.4660     281.0\n",
            "           2     0.0139    0.0200    0.0164      50.0\n",
            "           3     0.2251    0.2933    0.2547     208.0\n",
            "           4     0.5586    0.3557    0.4347     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3737    0.4029    0.3877     345.0\n",
            "\n",
            "    accuracy                         0.5410    2610.0\n",
            "   macro avg     0.3348    0.3287    0.3276    2610.0\n",
            "weighted avg     0.5511    0.5410    0.5420    2610.0\n",
            "\n",
            "epoch 64 train_loss 0.6578 train_acc 77.22 train_fscore 76.28 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4235 test_acc 55.71 test_fscore 54.57 time 13.75\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7334    0.7404    0.7369    1256.0\n",
            "           1     0.4225    0.4947    0.4557     281.0\n",
            "           2     0.0385    0.0200    0.0263      50.0\n",
            "           3     0.3494    0.1394    0.1993     208.0\n",
            "           4     0.4704    0.4751    0.4728     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3445    0.4754    0.3995     345.0\n",
            "\n",
            "    accuracy                         0.5571    2610.0\n",
            "   macro avg     0.3370    0.3350    0.3272    2610.0\n",
            "weighted avg     0.5450    0.5571    0.5457    2610.0\n",
            "\n",
            "epoch 65 train_loss 0.6459 train_acc 77.56 train_fscore 76.62 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4196 test_acc 57.2 test_fscore 55.87 time 14.21\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7277    0.7596    0.7433    1256.0\n",
            "           1     0.4227    0.5445    0.4759     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3525    0.2067    0.2606     208.0\n",
            "           4     0.4726    0.4726    0.4726     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4080    0.4435    0.4250     345.0\n",
            "\n",
            "    accuracy                         0.5720    2610.0\n",
            "   macro avg     0.3405    0.3467    0.3396    2610.0\n",
            "weighted avg     0.5505    0.5720    0.5587    2610.0\n",
            "\n",
            "epoch 66 train_loss 0.6305 train_acc 78.55 train_fscore 77.62 valid_loss nan valid_acc nan val_fscore nan test_loss 1.47 test_acc 54.98 test_fscore 54.32 time 14.06\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7242    0.7277    0.7260    1256.0\n",
            "           1     0.4420    0.4342    0.4381     281.0\n",
            "           2     0.0526    0.0200    0.0290      50.0\n",
            "           3     0.2314    0.2548    0.2426     208.0\n",
            "           4     0.5543    0.3557    0.4333     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3713    0.5855    0.4544     345.0\n",
            "\n",
            "    accuracy                         0.5498    2610.0\n",
            "   macro avg     0.3394    0.3397    0.3319    2610.0\n",
            "weighted avg     0.5500    0.5498    0.5432    2610.0\n",
            "\n",
            "epoch 67 train_loss 0.6313 train_acc 78.35 train_fscore 77.49 valid_loss nan valid_acc nan val_fscore nan test_loss 1.478 test_acc 56.7 test_fscore 55.49 time 14.02\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7268    0.7604    0.7432    1256.0\n",
            "           1     0.4264    0.4947    0.4580     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2778    0.2163    0.2432     208.0\n",
            "           4     0.4863    0.4428    0.4635     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3985    0.4725    0.4324     345.0\n",
            "\n",
            "    accuracy                         0.5670    2610.0\n",
            "   macro avg     0.3308    0.3409    0.3343    2610.0\n",
            "weighted avg     0.5454    0.5670    0.5549    2610.0\n",
            "\n",
            "epoch 68 train_loss 0.6241 train_acc 78.62 train_fscore 77.83 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4595 test_acc 56.97 test_fscore 55.53 time 13.94\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7274    0.7627    0.7447    1256.0\n",
            "           1     0.4625    0.4164    0.4382     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.4000    0.1538    0.2222     208.0\n",
            "           4     0.5014    0.4552    0.4772     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3621    0.5710    0.4432     345.0\n",
            "\n",
            "    accuracy                         0.5697    2610.0\n",
            "   macro avg     0.3505    0.3370    0.3322    2610.0\n",
            "weighted avg     0.5568    0.5697    0.5553    2610.0\n",
            "\n",
            "epoch 69 train_loss 0.6145 train_acc 78.83 train_fscore 78.0 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4937 test_acc 55.25 test_fscore 54.77 time 13.92\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7397    0.7150    0.7271    1256.0\n",
            "           1     0.4087    0.5338    0.4630     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3118    0.2548    0.2804     208.0\n",
            "           4     0.4778    0.4552    0.4662     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3666    0.4580    0.4072     345.0\n",
            "\n",
            "    accuracy                         0.5525    2610.0\n",
            "   macro avg     0.3292    0.3453    0.3349    2610.0\n",
            "weighted avg     0.5469    0.5525    0.5477    2610.0\n",
            "\n",
            "epoch 70 train_loss 0.6046 train_acc 79.25 train_fscore 78.49 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4565 test_acc 56.17 test_fscore 55.21 time 13.86\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7321    0.7508    0.7414    1256.0\n",
            "           1     0.4290    0.4840    0.4548     281.0\n",
            "           2     0.0250    0.0200    0.0222      50.0\n",
            "           3     0.3243    0.1731    0.2257     208.0\n",
            "           4     0.4726    0.4726    0.4726     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3791    0.4638    0.4172     345.0\n",
            "\n",
            "    accuracy                         0.5617    2610.0\n",
            "   macro avg     0.3375    0.3378    0.3334    2610.0\n",
            "weighted avg     0.5478    0.5617    0.5521    2610.0\n",
            "\n",
            "epoch 71 train_loss 0.5928 train_acc 79.56 train_fscore 78.86 valid_loss nan valid_acc nan val_fscore nan test_loss 1.502 test_acc 55.59 test_fscore 54.69 time 14.13\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7242    0.7484    0.7361    1256.0\n",
            "           1     0.4083    0.4911    0.4459     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2600    0.1875    0.2179     208.0\n",
            "           4     0.5074    0.4254    0.4628     402.0\n",
            "           5     0.0333    0.0147    0.0204      68.0\n",
            "           6     0.3794    0.4696    0.4197     345.0\n",
            "\n",
            "    accuracy                         0.5559    2610.0\n",
            "   macro avg     0.3304    0.3338    0.3290    2610.0\n",
            "weighted avg     0.5423    0.5559    0.5469    2610.0\n",
            "\n",
            "epoch 72 train_loss 0.5841 train_acc 79.92 train_fscore 79.22 valid_loss nan valid_acc nan val_fscore nan test_loss 1.5179 test_acc 55.1 test_fscore 54.62 time 13.93\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7407    0.7142    0.7272    1256.0\n",
            "           1     0.4528    0.4093    0.4299     281.0\n",
            "           2     0.0286    0.0200    0.0235      50.0\n",
            "           3     0.2857    0.1827    0.2229     208.0\n",
            "           4     0.4886    0.4801    0.4843     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3514    0.5623    0.4326     345.0\n",
            "\n",
            "    accuracy                         0.5510    2610.0\n",
            "   macro avg     0.3354    0.3384    0.3315    2610.0\n",
            "weighted avg     0.5502    0.5510    0.5462    2610.0\n",
            "\n",
            "epoch 73 train_loss 0.5776 train_acc 80.2 train_fscore 79.62 valid_loss nan valid_acc nan val_fscore nan test_loss 1.5621 test_acc 55.71 test_fscore 54.45 time 13.99\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7241    0.7500    0.7368    1256.0\n",
            "           1     0.4391    0.4235    0.4312     281.0\n",
            "           2     0.0435    0.0200    0.0274      50.0\n",
            "           3     0.3304    0.1779    0.2313     208.0\n",
            "           4     0.5099    0.3831    0.4375     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3436    0.5826    0.4323     345.0\n",
            "\n",
            "    accuracy                         0.5571    2610.0\n",
            "   macro avg     0.3415    0.3339    0.3281    2610.0\n",
            "weighted avg     0.5468    0.5571    0.5445    2610.0\n",
            "\n",
            "epoch 74 train_loss 0.5649 train_acc 80.29 train_fscore 79.69 valid_loss nan valid_acc nan val_fscore nan test_loss 1.5439 test_acc 56.32 test_fscore 54.87 time 13.83\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7133    0.7667    0.7391    1256.0\n",
            "           1     0.4680    0.4164    0.4407     281.0\n",
            "           2     0.0500    0.0200    0.0286      50.0\n",
            "           3     0.2632    0.1923    0.2222     208.0\n",
            "           4     0.4631    0.4677    0.4653     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3842    0.4667    0.4215     345.0\n",
            "\n",
            "    accuracy                         0.5632    2610.0\n",
            "   macro avg     0.3345    0.3328    0.3310    2610.0\n",
            "weighted avg     0.5377    0.5632    0.5487    2610.0\n",
            "\n",
            "epoch 75 train_loss 0.5638 train_acc 80.67 train_fscore 80.02 valid_loss nan valid_acc nan val_fscore nan test_loss 1.574 test_acc 57.2 test_fscore 55.54 time 14.13\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7210    0.7818    0.7502    1256.0\n",
            "           1     0.4478    0.4733    0.4602     281.0\n",
            "           2     0.0500    0.0200    0.0286      50.0\n",
            "           3     0.2966    0.1683    0.2147     208.0\n",
            "           4     0.4807    0.4652    0.4728     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3790    0.4493    0.4111     345.0\n",
            "\n",
            "    accuracy                         0.5720    2610.0\n",
            "   macro avg     0.3393    0.3368    0.3340    2610.0\n",
            "weighted avg     0.5439    0.5720    0.5554    2610.0\n",
            "\n",
            "epoch 76 train_loss 0.5428 train_acc 80.87 train_fscore 80.27 valid_loss nan valid_acc nan val_fscore nan test_loss 1.5656 test_acc 55.4 test_fscore 54.94 time 13.84\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7392    0.7245    0.7318    1256.0\n",
            "           1     0.4040    0.5018    0.4476     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3116    0.2067    0.2486     208.0\n",
            "           4     0.4921    0.4677    0.4796     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3744    0.4754    0.4189     345.0\n",
            "\n",
            "    accuracy                         0.5540    2610.0\n",
            "   macro avg     0.3316    0.3394    0.3324    2610.0\n",
            "weighted avg     0.5494    0.5540    0.5494    2610.0\n",
            "\n",
            "epoch 77 train_loss 0.5346 train_acc 81.37 train_fscore 80.85 valid_loss nan valid_acc nan val_fscore nan test_loss 1.587 test_acc 54.48 test_fscore 54.36 time 14.15\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7396    0.7189    0.7291    1256.0\n",
            "           1     0.4017    0.5089    0.4490     281.0\n",
            "           2     0.0172    0.0200    0.0185      50.0\n",
            "           3     0.2795    0.2163    0.2439     208.0\n",
            "           4     0.4985    0.4254    0.4591     402.0\n",
            "           5     0.0294    0.0147    0.0196      68.0\n",
            "           6     0.3616    0.4580    0.4041     345.0\n",
            "\n",
            "    accuracy                         0.5448    2610.0\n",
            "   macro avg     0.3325    0.3375    0.3319    2610.0\n",
            "weighted avg     0.5471    0.5448    0.5436    2610.0\n",
            "\n",
            "epoch 78 train_loss 0.5365 train_acc 81.28 train_fscore 80.78 valid_loss nan valid_acc nan val_fscore nan test_loss 1.627 test_acc 56.13 test_fscore 55.08 time 13.83\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7331    0.7349    0.7340    1256.0\n",
            "           1     0.4436    0.4342    0.4388     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3271    0.1683    0.2222     208.0\n",
            "           4     0.4899    0.4826    0.4862     402.0\n",
            "           5     0.0294    0.0147    0.0196      68.0\n",
            "           6     0.3571    0.5507    0.4333     345.0\n",
            "\n",
            "    accuracy                         0.5613    2610.0\n",
            "   macro avg     0.3400    0.3408    0.3335    2610.0\n",
            "weighted avg     0.5501    0.5613    0.5508    2610.0\n",
            "\n",
            "epoch 79 train_loss 0.5158 train_acc 81.91 train_fscore 81.43 valid_loss nan valid_acc nan val_fscore nan test_loss 1.6818 test_acc 56.25 test_fscore 54.69 time 14.26\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7237    0.7675    0.7450    1256.0\n",
            "           1     0.4399    0.4555    0.4476     281.0\n",
            "           2     0.0263    0.0200    0.0227      50.0\n",
            "           3     0.3433    0.1106    0.1673     208.0\n",
            "           4     0.5016    0.3806    0.4328     402.0\n",
            "           5     0.0256    0.0147    0.0187      68.0\n",
            "           6     0.3680    0.5739    0.4485     345.0\n",
            "\n",
            "    accuracy                         0.5625    2610.0\n",
            "   macro avg     0.3469    0.3318    0.3261    2610.0\n",
            "weighted avg     0.5501    0.5625    0.5469    2610.0\n",
            "\n",
            "epoch 80 train_loss 0.5224 train_acc 81.94 train_fscore 81.42 valid_loss nan valid_acc nan val_fscore nan test_loss 1.6002 test_acc 56.02 test_fscore 55.1 time 14.19\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7229    0.7580    0.7400    1256.0\n",
            "           1     0.4305    0.4520    0.4410     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2721    0.1923    0.2254     208.0\n",
            "           4     0.4843    0.4602    0.4719     402.0\n",
            "           5     0.0263    0.0147    0.0189      68.0\n",
            "           6     0.3995    0.4551    0.4255     345.0\n",
            "\n",
            "    accuracy                         0.5602    2610.0\n",
            "   macro avg     0.3337    0.3332    0.3318    2610.0\n",
            "weighted avg     0.5440    0.5602    0.5510    2610.0\n",
            "\n",
            "epoch 81 train_loss 0.5094 train_acc 82.27 train_fscore 81.84 valid_loss nan valid_acc nan val_fscore nan test_loss 1.6006 test_acc 56.48 test_fscore 55.19 time 14.11\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7136    0.7834    0.7469    1256.0\n",
            "           1     0.4131    0.5160    0.4589     281.0\n",
            "           2     0.0238    0.0200    0.0217      50.0\n",
            "           3     0.3000    0.1731    0.2195     208.0\n",
            "           4     0.4884    0.4204    0.4519     402.0\n",
            "           5     0.0741    0.0588    0.0656      68.0\n",
            "           6     0.4245    0.3913    0.4072     345.0\n",
            "\n",
            "    accuracy                         0.5648    2610.0\n",
            "   macro avg     0.3482    0.3376    0.3388    2610.0\n",
            "weighted avg     0.5455    0.5648    0.5519    2610.0\n",
            "\n",
            "epoch 82 train_loss 0.4854 train_acc 82.83 train_fscore 82.43 valid_loss nan valid_acc nan val_fscore nan test_loss 1.6664 test_acc 56.7 test_fscore 55.79 time 14.1\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7312    0.7604    0.7455    1256.0\n",
            "           1     0.4488    0.4520    0.4504     281.0\n",
            "           2     0.0345    0.0200    0.0253      50.0\n",
            "           3     0.3091    0.2452    0.2735     208.0\n",
            "           4     0.5089    0.4254    0.4634     402.0\n",
            "           5     0.0476    0.0147    0.0225      68.0\n",
            "           6     0.3702    0.5043    0.4270     345.0\n",
            "\n",
            "    accuracy                         0.5670    2610.0\n",
            "   macro avg     0.3500    0.3460    0.3439    2610.0\n",
            "weighted avg     0.5541    0.5670    0.5579    2610.0\n",
            "\n",
            "epoch 83 train_loss 0.4715 train_acc 83.49 train_fscore 83.1 valid_loss nan valid_acc nan val_fscore nan test_loss 1.6737 test_acc 55.82 test_fscore 54.79 time 13.89\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7214    0.7588    0.7396    1256.0\n",
            "           1     0.4461    0.4270    0.4364     281.0\n",
            "           2     0.0556    0.0200    0.0294      50.0\n",
            "           3     0.2987    0.2212    0.2541     208.0\n",
            "           4     0.4753    0.4303    0.4517     402.0\n",
            "           5     0.0303    0.0147    0.0198      68.0\n",
            "           6     0.3614    0.4725    0.4095     345.0\n",
            "\n",
            "    accuracy                         0.5582    2610.0\n",
            "   macro avg     0.3413    0.3349    0.3344    2610.0\n",
            "weighted avg     0.5418    0.5582    0.5479    2610.0\n",
            "\n",
            "epoch 84 train_loss 0.463 train_acc 83.93 train_fscore 83.59 valid_loss nan valid_acc nan val_fscore nan test_loss 1.7128 test_acc 54.98 test_fscore 54.5 time 13.85\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7304    0.7420    0.7362    1256.0\n",
            "           1     0.4258    0.4698    0.4467     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2746    0.2548    0.2643     208.0\n",
            "           4     0.4812    0.4129    0.4444     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3698    0.4406    0.4021     345.0\n",
            "\n",
            "    accuracy                         0.5498    2610.0\n",
            "   macro avg     0.3260    0.3314    0.3277    2610.0\n",
            "weighted avg     0.5422    0.5498    0.5450    2610.0\n",
            "\n",
            "epoch 85 train_loss 0.4579 train_acc 84.22 train_fscore 83.9 valid_loss nan valid_acc nan val_fscore nan test_loss 1.748 test_acc 56.28 test_fscore 54.68 time 14.07\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7224    0.7667    0.7439    1256.0\n",
            "           1     0.4172    0.4484    0.4322     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3267    0.1587    0.2136     208.0\n",
            "           4     0.4953    0.3955    0.4398     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3622    0.5449    0.4352     345.0\n",
            "\n",
            "    accuracy                         0.5628    2610.0\n",
            "   macro avg     0.3320    0.3306    0.3235    2610.0\n",
            "weighted avg     0.5428    0.5628    0.5468    2610.0\n",
            "\n",
            "epoch 86 train_loss 0.4421 train_acc 84.84 train_fscore 84.48 valid_loss nan valid_acc nan val_fscore nan test_loss 1.8012 test_acc 55.44 test_fscore 54.79 time 13.97\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7363    0.7293    0.7328    1256.0\n",
            "           1     0.4478    0.4270    0.4372     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3194    0.2212    0.2614     208.0\n",
            "           4     0.4554    0.4826    0.4686     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3608    0.4957    0.4176     345.0\n",
            "\n",
            "    accuracy                         0.5544    2610.0\n",
            "   macro avg     0.3314    0.3365    0.3311    2610.0\n",
            "weighted avg     0.5458    0.5544    0.5479    2610.0\n",
            "\n",
            "epoch 87 train_loss 0.4305 train_acc 85.38 train_fscore 85.06 valid_loss nan valid_acc nan val_fscore nan test_loss 1.7354 test_acc 54.52 test_fscore 54.2 time 13.91\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7391    0.7014    0.7198    1256.0\n",
            "           1     0.4043    0.4733    0.4361     281.0\n",
            "           2     0.0323    0.0200    0.0247      50.0\n",
            "           3     0.2960    0.1779    0.2222     208.0\n",
            "           4     0.4808    0.4975    0.4890     402.0\n",
            "           5     0.0250    0.0147    0.0185      68.0\n",
            "           6     0.3564    0.4928    0.4136     345.0\n",
            "\n",
            "    accuracy                         0.5452    2610.0\n",
            "   macro avg     0.3334    0.3397    0.3320    2610.0\n",
            "weighted avg     0.5452    0.5452    0.5420    2610.0\n",
            "\n",
            "epoch 88 train_loss 0.4287 train_acc 84.96 train_fscore 84.72 valid_loss nan valid_acc nan val_fscore nan test_loss 1.8017 test_acc 55.02 test_fscore 54.16 time 13.8\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7216    0.7389    0.7301    1256.0\n",
            "           1     0.4178    0.4342    0.4258     281.0\n",
            "           2     0.0345    0.0200    0.0253      50.0\n",
            "           3     0.3206    0.2019    0.2478     208.0\n",
            "           4     0.4938    0.3980    0.4408     402.0\n",
            "           5     0.0345    0.0147    0.0206      68.0\n",
            "           6     0.3507    0.5275    0.4213     345.0\n",
            "\n",
            "    accuracy                         0.5502    2610.0\n",
            "   macro avg     0.3391    0.3336    0.3303    2610.0\n",
            "weighted avg     0.5418    0.5502    0.5416    2610.0\n",
            "\n",
            "epoch 89 train_loss 0.428 train_acc 85.01 train_fscore 84.76 valid_loss nan valid_acc nan val_fscore nan test_loss 1.7691 test_acc 54.56 test_fscore 54.35 time 14.23\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7361    0.7150    0.7254    1256.0\n",
            "           1     0.4295    0.4662    0.4471     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2754    0.2212    0.2453     208.0\n",
            "           4     0.4638    0.4627    0.4633     402.0\n",
            "           5     0.0270    0.0147    0.0190      68.0\n",
            "           6     0.3724    0.4696    0.4154     345.0\n",
            "\n",
            "    accuracy                         0.5456    2610.0\n",
            "   macro avg     0.3292    0.3356    0.3308    2610.0\n",
            "weighted avg     0.5438    0.5456    0.5435    2610.0\n",
            "\n",
            "epoch 90 train_loss 0.4263 train_acc 84.86 train_fscore 84.55 valid_loss nan valid_acc nan val_fscore nan test_loss 1.8693 test_acc 55.82 test_fscore 54.72 time 13.77\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7316    0.7532    0.7423    1256.0\n",
            "           1     0.3652    0.5445    0.4371     281.0\n",
            "           2     0.0556    0.0200    0.0294      50.0\n",
            "           3     0.2763    0.2019    0.2333     208.0\n",
            "           4     0.4635    0.4577    0.4606     402.0\n",
            "           5     0.0556    0.0147    0.0233      68.0\n",
            "           6     0.4153    0.3768    0.3951     345.0\n",
            "\n",
            "    accuracy                         0.5582    2610.0\n",
            "   macro avg     0.3376    0.3384    0.3316    2610.0\n",
            "weighted avg     0.5422    0.5582    0.5472    2610.0\n",
            "\n",
            "epoch 91 train_loss 0.4118 train_acc 85.38 train_fscore 85.14 valid_loss nan valid_acc nan val_fscore nan test_loss 1.8197 test_acc 54.56 test_fscore 54.19 time 14.38\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7261    0.7365    0.7312    1256.0\n",
            "           1     0.4569    0.4342    0.4453     281.0\n",
            "           2     0.0492    0.0600    0.0541      50.0\n",
            "           3     0.2817    0.1923    0.2286     208.0\n",
            "           4     0.4643    0.4204    0.4413     402.0\n",
            "           5     0.0196    0.0147    0.0168      68.0\n",
            "           6     0.3636    0.4754    0.4121     345.0\n",
            "\n",
            "    accuracy                         0.5456    2610.0\n",
            "   macro avg     0.3373    0.3333    0.3327    2610.0\n",
            "weighted avg     0.5421    0.5456    0.5419    2610.0\n",
            "\n",
            "epoch 92 train_loss 0.3983 train_acc 86.02 train_fscore 85.76 valid_loss nan valid_acc nan val_fscore nan test_loss 1.8754 test_acc 55.52 test_fscore 54.91 time 13.98\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7290    0.7476    0.7382    1256.0\n",
            "           1     0.4566    0.4306    0.4432     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2452    0.2452    0.2452     208.0\n",
            "           4     0.4793    0.4328    0.4549     402.0\n",
            "           5     0.0500    0.0147    0.0227      68.0\n",
            "           6     0.3826    0.4725    0.4228     345.0\n",
            "\n",
            "    accuracy                         0.5552    2610.0\n",
            "   macro avg     0.3347    0.3348    0.3324    2610.0\n",
            "weighted avg     0.5452    0.5552    0.5491    2610.0\n",
            "\n",
            "epoch 93 train_loss 0.3928 train_acc 86.78 train_fscore 86.55 valid_loss nan valid_acc nan val_fscore nan test_loss 1.9264 test_acc 54.87 test_fscore 54.36 time 14.15\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7352    0.7293    0.7322    1256.0\n",
            "           1     0.4123    0.5018    0.4526     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2840    0.2212    0.2486     208.0\n",
            "           4     0.4534    0.4353    0.4442     402.0\n",
            "           5     0.0526    0.0147    0.0230      68.0\n",
            "           6     0.3750    0.4435    0.4064     345.0\n",
            "\n",
            "    accuracy                         0.5487    2610.0\n",
            "   macro avg     0.3303    0.3351    0.3296    2610.0\n",
            "weighted avg     0.5416    0.5487    0.5436    2610.0\n",
            "\n",
            "epoch 94 train_loss 0.3895 train_acc 86.21 train_fscore 85.98 valid_loss nan valid_acc nan val_fscore nan test_loss 1.9729 test_acc 55.06 test_fscore 53.99 time 13.99\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7305    0.7381    0.7343    1256.0\n",
            "           1     0.4653    0.4057    0.4335     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2527    0.1106    0.1538     208.0\n",
            "           4     0.4747    0.4677    0.4712     402.0\n",
            "           5     0.0270    0.0147    0.0190      68.0\n",
            "           6     0.3370    0.5333    0.4130     345.0\n",
            "\n",
            "    accuracy                         0.5506    2610.0\n",
            "   macro avg     0.3268    0.3243    0.3178    2610.0\n",
            "weighted avg     0.5401    0.5506    0.5399    2610.0\n",
            "\n",
            "epoch 95 train_loss 0.3983 train_acc 85.92 train_fscore 85.72 valid_loss nan valid_acc nan val_fscore nan test_loss 1.9666 test_acc 54.71 test_fscore 54.14 time 14.6\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7396    0.7102    0.7246    1256.0\n",
            "           1     0.4612    0.3594    0.4040     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3121    0.2115    0.2521     208.0\n",
            "           4     0.4575    0.4826    0.4697     402.0\n",
            "           5     0.0435    0.0147    0.0220      68.0\n",
            "           6     0.3397    0.5681    0.4252     345.0\n",
            "\n",
            "    accuracy                         0.5471    2610.0\n",
            "   macro avg     0.3362    0.3352    0.3282    2610.0\n",
            "weighted avg     0.5470    0.5471    0.5414    2610.0\n",
            "\n",
            "epoch 96 train_loss 0.3702 train_acc 87.1 train_fscore 86.91 valid_loss nan valid_acc nan val_fscore nan test_loss 1.8995 test_acc 52.72 test_fscore 52.46 time 14.36\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7221    0.7118    0.7169    1256.0\n",
            "           1     0.4120    0.4413    0.4261     281.0\n",
            "           2     0.0253    0.0400    0.0310      50.0\n",
            "           3     0.2881    0.1635    0.2086     208.0\n",
            "           4     0.4912    0.3458    0.4058     402.0\n",
            "           5     0.0270    0.0147    0.0190      68.0\n",
            "           6     0.3285    0.5275    0.4049     345.0\n",
            "\n",
            "    accuracy                         0.5272    2610.0\n",
            "   macro avg     0.3278    0.3206    0.3161    2610.0\n",
            "weighted avg     0.5351    0.5272    0.5246    2610.0\n",
            "\n",
            "epoch 97 train_loss 0.3759 train_acc 86.93 train_fscore 86.72 valid_loss nan valid_acc nan val_fscore nan test_loss 1.9743 test_acc 55.36 test_fscore 54.1 time 14.19\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7084    0.7659    0.7360    1256.0\n",
            "           1     0.4494    0.3950    0.4205     281.0\n",
            "           2     0.0204    0.0200    0.0202      50.0\n",
            "           3     0.2613    0.1394    0.1818     208.0\n",
            "           4     0.5100    0.3806    0.4359     402.0\n",
            "           5     0.0426    0.0294    0.0348      68.0\n",
            "           6     0.3755    0.5420    0.4437     345.0\n",
            "\n",
            "    accuracy                         0.5536    2610.0\n",
            "   macro avg     0.3382    0.3246    0.3247    2610.0\n",
            "weighted avg     0.5398    0.5536    0.5410    2610.0\n",
            "\n",
            "epoch 98 train_loss 0.3641 train_acc 87.06 train_fscore 86.91 valid_loss nan valid_acc nan val_fscore nan test_loss 2.1028 test_acc 54.64 test_fscore 53.65 time 14.17\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7378    0.7102    0.7237    1256.0\n",
            "           1     0.4017    0.5018    0.4462     281.0\n",
            "           2     0.0385    0.0200    0.0263      50.0\n",
            "           3     0.3016    0.0913    0.1402     208.0\n",
            "           4     0.4606    0.4652    0.4629     402.0\n",
            "           5     0.0263    0.0147    0.0189      68.0\n",
            "           6     0.3578    0.5362    0.4292     345.0\n",
            "\n",
            "    accuracy                         0.5464    2610.0\n",
            "   macro avg     0.3320    0.3342    0.3211    2610.0\n",
            "weighted avg     0.5420    0.5464    0.5365    2610.0\n",
            "\n",
            "epoch 99 train_loss 0.345 train_acc 87.79 train_fscore 87.66 valid_loss nan valid_acc nan val_fscore nan test_loss 2.0599 test_acc 54.83 test_fscore 54.23 time 14.09\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7352    0.7317    0.7334    1256.0\n",
            "           1     0.3840    0.4947    0.4323     281.0\n",
            "           2     0.0182    0.0200    0.0190      50.0\n",
            "           3     0.2960    0.1779    0.2222     208.0\n",
            "           4     0.4537    0.4627    0.4581     402.0\n",
            "           5     0.1053    0.0294    0.0460      68.0\n",
            "           6     0.3779    0.4261    0.4005     345.0\n",
            "\n",
            "    accuracy                         0.5483    2610.0\n",
            "   macro avg     0.3386    0.3346    0.3302    2610.0\n",
            "weighted avg     0.5416    0.5483    0.5423    2610.0\n",
            "\n",
            "epoch 100 train_loss 0.3508 train_acc 87.75 train_fscore 87.6 valid_loss nan valid_acc nan val_fscore nan test_loss 2.0889 test_acc 55.21 test_fscore 54.34 time 14.27\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7324    0.7365    0.7344    1256.0\n",
            "           1     0.4481    0.4306    0.4392     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2745    0.1346    0.1806     208.0\n",
            "           4     0.4349    0.4900    0.4608     402.0\n",
            "           5     0.0208    0.0147    0.0172      68.0\n",
            "           6     0.3832    0.4899    0.4300     345.0\n",
            "\n",
            "    accuracy                         0.5521    2610.0\n",
            "   macro avg     0.3277    0.3280    0.3232    2610.0\n",
            "weighted avg     0.5407    0.5521    0.5434    2610.0\n",
            "\n",
            "Test performance..\n",
            "Fscore 57.77 accuracy 60.31\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7151    0.8193    0.7636    1256.0\n",
            "           1     0.4980    0.4448    0.4699     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3700    0.1779    0.2403     208.0\n",
            "           4     0.5289    0.5000    0.5141     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4136    0.5275    0.4637     345.0\n",
            "\n",
            "    accuracy                         0.6031    2610.0\n",
            "   macro avg     0.3608    0.3528    0.3502    2610.0\n",
            "weighted avg     0.5634    0.6031    0.5777    2610.0\n",
            "\n",
            "[[1.029e+03 3.900e+01 0.000e+00 3.700e+01 8.100e+01 0.000e+00 7.000e+01]\n",
            " [7.500e+01 1.250e+02 0.000e+00 1.000e+00 3.700e+01 0.000e+00 4.300e+01]\n",
            " [2.600e+01 4.000e+00 0.000e+00 2.000e+00 6.000e+00 0.000e+00 1.200e+01]\n",
            " [1.040e+02 1.300e+01 0.000e+00 3.700e+01 9.000e+00 0.000e+00 4.500e+01]\n",
            " [9.600e+01 3.200e+01 0.000e+00 1.100e+01 2.010e+02 0.000e+00 6.200e+01]\n",
            " [2.600e+01 8.000e+00 0.000e+00 5.000e+00 3.000e+00 0.000e+00 2.600e+01]\n",
            " [8.300e+01 3.000e+01 0.000e+00 7.000e+00 4.300e+01 0.000e+00 1.820e+02]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DialogueRNN for IEMOCAP\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(1234)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.optim as optim\n",
        "\n",
        "import argparse\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score,\\\n",
        "                        classification_report, precision_recall_fscore_support\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def get_train_valid_sampler(trainset, valid=0.1):\n",
        "    size = len(trainset)\n",
        "    idx = list(range(size))\n",
        "    split = int(valid*size)\n",
        "    return SubsetRandomSampler(idx[split:]), SubsetRandomSampler(idx[:split])\n",
        "\n",
        "def get_IEMOCAP_loaders(path, batch_size=32, valid=0.1, num_workers=0, pin_memory=False):\n",
        "    trainset = IEMOCAPDataset(path=path)\n",
        "    train_sampler, valid_sampler = get_train_valid_sampler(trainset, valid)\n",
        "    train_loader = DataLoader(trainset,\n",
        "                              batch_size=batch_size,\n",
        "                              sampler=train_sampler,\n",
        "                              collate_fn=trainset.collate_fn,\n",
        "                              num_workers=num_workers,\n",
        "                              pin_memory=pin_memory)\n",
        "    valid_loader = DataLoader(trainset,\n",
        "                              batch_size=batch_size,\n",
        "                              sampler=valid_sampler,\n",
        "                              collate_fn=trainset.collate_fn,\n",
        "                              num_workers=num_workers,\n",
        "                              pin_memory=pin_memory)\n",
        "\n",
        "    testset = IEMOCAPDataset(path=path, train=False)\n",
        "    test_loader = DataLoader(testset,\n",
        "                             batch_size=batch_size,\n",
        "                             collate_fn=testset.collate_fn,\n",
        "                             num_workers=num_workers,\n",
        "                             pin_memory=pin_memory)\n",
        "\n",
        "    return train_loader, valid_loader, test_loader\n",
        "\n",
        "def train_or_eval_model(model, loss_function, dataloader, epoch, optimizer=None, train=False):\n",
        "    losses = []\n",
        "    preds = []\n",
        "    labels = []\n",
        "    masks = []\n",
        "    alphas, alphas_f, alphas_b, vids = [], [], [], []\n",
        "    assert not train or optimizer!=None\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    for data in dataloader:\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "        # import ipdb;ipdb.set_trace()\n",
        "        textf, visuf, acouf, qmask, umask, label =\\\n",
        "                [d.cuda() for d in data[:-1]] if cuda else data[:-1]\n",
        "        #log_prob = model(torch.cat((textf,acouf,visuf),dim=-1), qmask,umask,att2=True) # seq_len, batch, n_classes\n",
        "        log_prob = model(textf, qmask,umask,att2=True) # seq_len, batch, n_classes\n",
        "        lp_ = log_prob.transpose(0,1).contiguous().view(-1,log_prob.size()[2]) # batch*seq_len, n_classes\n",
        "        labels_ = label.view(-1) # batch*seq_len\n",
        "        loss = loss_function(lp_, labels_, umask)\n",
        "\n",
        "        pred_ = torch.argmax(lp_,1) # batch*seq_len\n",
        "        preds.append(pred_.data.cpu().numpy())\n",
        "        labels.append(labels_.data.cpu().numpy())\n",
        "        masks.append(umask.view(-1).cpu().numpy())\n",
        "\n",
        "        losses.append(loss.item()*masks[-1].sum())\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            if args.tensorboard:\n",
        "                for param in model.named_parameters():\n",
        "                    writer.add_histogram(param[0], param[1].grad, epoch)\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            #alphas += alpha\n",
        "            #alphas_f += alpha_f\n",
        "            #alphas_b += alpha_b\n",
        "            vids += data[-1]\n",
        "\n",
        "    if preds!=[]:\n",
        "        preds  = np.concatenate(preds)\n",
        "        labels = np.concatenate(labels)\n",
        "        masks  = np.concatenate(masks)\n",
        "    else:\n",
        "        return float('nan'), float('nan'), [], [], [], float('nan'),[]\n",
        "\n",
        "    avg_loss = round(np.sum(losses)/np.sum(masks),4)\n",
        "    avg_accuracy = round(accuracy_score(labels,preds,sample_weight=masks)*100,2)\n",
        "    avg_fscore = round(f1_score(labels,preds,sample_weight=masks,average='weighted')*100,2)\n",
        "    return avg_loss, avg_accuracy, labels, preds, masks,avg_fscore, [alphas, alphas_f, alphas_b, vids]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "                        help='does not use GPU')\n",
        "    parser.add_argument('--lr', type=float, default=0.0001, metavar='LR',\n",
        "                        help='learning rate')\n",
        "    parser.add_argument('--l2', type=float, default=0.00001, metavar='L2',\n",
        "                        help='L2 regularization weight')\n",
        "    parser.add_argument('--rec-dropout', type=float, default=0.1,\n",
        "                        metavar='rec_dropout', help='rec_dropout rate')\n",
        "    parser.add_argument('--dropout', type=float, default=0.1, metavar='dropout',\n",
        "                        help='dropout rate')\n",
        "    parser.add_argument('--batch-size', type=int, default=30, metavar='BS',\n",
        "                        help='batch size')\n",
        "    parser.add_argument('--epochs', type=int, default=60, metavar='E',\n",
        "                        help='number of epochs')\n",
        "    parser.add_argument('--class-weight', action='store_true', default=True,\n",
        "                        help='class weight')\n",
        "    parser.add_argument('--active-listener', action='store_true', default=False,\n",
        "                        help='active listener')\n",
        "    parser.add_argument('--attention', default='general', help='Attention type')\n",
        "    parser.add_argument('--tensorboard', action='store_true', default=False,\n",
        "                        help='Enables tensorboard log')\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    print(args)\n",
        "    seed_everything(seed=2023)\n",
        "    args.cuda = torch.cuda.is_available() and not args.no_cuda\n",
        "    if args.cuda:\n",
        "        print('Running on GPU')\n",
        "    else:\n",
        "        print('Running on CPU')\n",
        "\n",
        "    if args.tensorboard:\n",
        "        from tensorboardX import SummaryWriter\n",
        "        writer = SummaryWriter()\n",
        "\n",
        "    batch_size = args.batch_size\n",
        "    n_classes  = 6\n",
        "    cuda       = args.cuda\n",
        "    n_epochs   = args.epochs\n",
        "\n",
        "    D_m = 100\n",
        "    D_g = 500\n",
        "    D_p = 500\n",
        "    D_e = 300\n",
        "    D_h = 300\n",
        "\n",
        "    D_a = 100 # concat attention\n",
        "\n",
        "    model = Model(D_m, D_g, D_p, D_e, D_h,\n",
        "                    n_classes=n_classes,\n",
        "                    listener_state=args.active_listener,\n",
        "                    context_attention=args.attention,\n",
        "                    dropout_rec=args.rec_dropout,\n",
        "                    dropout=args.dropout)\n",
        "    if cuda:\n",
        "        model.cuda()\n",
        "    loss_weights = torch.FloatTensor([\n",
        "                                        1/0.086747,\n",
        "                                        1/0.144406,\n",
        "                                        1/0.227883,\n",
        "                                        1/0.160585,\n",
        "                                        1/0.127711,\n",
        "                                        1/0.252668,\n",
        "                                        ])\n",
        "    if args.class_weight:\n",
        "        loss_function  = MaskedNLLLoss(loss_weights.cuda() if cuda else loss_weights)\n",
        "    else:\n",
        "        loss_function = MaskedNLLLoss()\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                           lr=args.lr,\n",
        "                           weight_decay=args.l2)\n",
        "\n",
        "    train_loader, valid_loader, test_loader =\\\n",
        "            get_IEMOCAP_loaders('drive/MyDrive/IEMOCAP_features_raw.pkl',\n",
        "                                valid=0.0,\n",
        "                                batch_size=batch_size,\n",
        "                                num_workers=2)\n",
        "\n",
        "    best_loss, best_label, best_pred, best_mask = None, None, None, None\n",
        "\n",
        "    for e in range(n_epochs):\n",
        "        start_time = time.time()\n",
        "        train_loss, train_acc, _,_,_,train_fscore,_= train_or_eval_model(model, loss_function,\n",
        "                                               train_loader, e, optimizer, True)\n",
        "        valid_loss, valid_acc, _,_,_,val_fscore,_= train_or_eval_model(model, loss_function, valid_loader, e)\n",
        "        test_loss, test_acc, test_label, test_pred, test_mask, test_fscore, attentions = train_or_eval_model(model, loss_function, test_loader, e)\n",
        "\n",
        "        if best_loss == None or best_loss > test_loss:\n",
        "            best_loss, best_label, best_pred, best_mask, best_attn =\\\n",
        "                    test_loss, test_label, test_pred, test_mask, attentions\n",
        "\n",
        "        if args.tensorboard:\n",
        "            writer.add_scalar('test: accuracy/loss',test_acc/test_loss,e)\n",
        "            writer.add_scalar('train: accuracy/loss',train_acc/train_loss,e)\n",
        "        print('epoch {} train_loss {} train_acc {} train_fscore{} valid_loss {} valid_acc {} val_fscore{} test_loss {} test_acc {} test_fscore {} time {}'.\\\n",
        "                format(e+1, train_loss, train_acc, train_fscore, valid_loss, valid_acc, val_fscore,\\\n",
        "                        test_loss, test_acc, test_fscore, round(time.time()-start_time,2)))\n",
        "    if args.tensorboard:\n",
        "        writer.close()\n",
        "\n",
        "    print('Test performance..')\n",
        "    print('Loss {} accuracy {}'.format(best_loss,\n",
        "                                     round(accuracy_score(best_label,best_pred,sample_weight=best_mask)*100,2)))\n",
        "    print(classification_report(best_label,best_pred,sample_weight=best_mask,digits=4))\n",
        "    print(confusion_matrix(best_label,best_pred,sample_weight=best_mask))\n",
        "    # with open('best_attention.p','wb') as f:\n",
        "    #     pickle.dump(best_attn+[best_label,best_pred,best_mask],f)"
      ],
      "metadata": {
        "id": "DUS4dakTKzvH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7649a760-5915-458e-853a-230b4bb2740e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(no_cuda=False, lr=0.0001, l2=1e-05, rec_dropout=0.1, dropout=0.1, batch_size=30, epochs=60, class_weight=True, active_listener=False, attention='general', tensorboard=False)\n",
            "2023\n",
            "Running on GPU\n",
            "epoch 1 train_loss 1.7668 train_acc 26.85 train_fscore18.86 valid_loss nan valid_acc nan val_fscorenan test_loss 1.7414 test_acc 30.01 test_fscore 19.65 time 6.87\n",
            "epoch 2 train_loss 1.6497 train_acc 47.64 train_fscore42.03 valid_loss nan valid_acc nan val_fscorenan test_loss 1.669 test_acc 47.32 test_fscore 41.32 time 5.12\n",
            "epoch 3 train_loss 1.5349 train_acc 55.96 train_fscore52.9 valid_loss nan valid_acc nan val_fscorenan test_loss 1.582 test_acc 58.96 test_fscore 55.48 time 5.57\n",
            "epoch 4 train_loss 1.4104 train_acc 60.4 train_fscore58.19 valid_loss nan valid_acc nan val_fscorenan test_loss 1.4825 test_acc 60.38 test_fscore 58.1 time 6.25\n",
            "epoch 5 train_loss 1.2868 train_acc 62.77 train_fscore60.49 valid_loss nan valid_acc nan val_fscorenan test_loss 1.3819 test_acc 60.75 test_fscore 57.95 time 4.8\n",
            "epoch 6 train_loss 1.173 train_acc 64.68 train_fscore62.12 valid_loss nan valid_acc nan val_fscorenan test_loss 1.2903 test_acc 59.46 test_fscore 55.6 time 5.24\n",
            "epoch 7 train_loss 1.0689 train_acc 67.25 train_fscore64.81 valid_loss nan valid_acc nan val_fscorenan test_loss 1.2121 test_acc 59.64 test_fscore 55.92 time 4.79\n",
            "epoch 8 train_loss 0.9805 train_acc 69.0 train_fscore67.06 valid_loss nan valid_acc nan val_fscorenan test_loss 1.15 test_acc 59.52 test_fscore 57.0 time 4.57\n",
            "epoch 9 train_loss 0.902 train_acc 71.08 train_fscore69.53 valid_loss nan valid_acc nan val_fscorenan test_loss 1.1022 test_acc 59.89 test_fscore 58.52 time 5.72\n",
            "epoch 10 train_loss 0.8195 train_acc 73.65 train_fscore72.79 valid_loss nan valid_acc nan val_fscorenan test_loss 1.0693 test_acc 60.38 test_fscore 60.24 time 4.35\n",
            "epoch 11 train_loss 0.7401 train_acc 76.95 train_fscore76.6 valid_loss nan valid_acc nan val_fscorenan test_loss 1.0576 test_acc 60.63 test_fscore 60.76 time 4.44\n",
            "epoch 12 train_loss 0.6808 train_acc 79.36 train_fscore79.2 valid_loss nan valid_acc nan val_fscorenan test_loss 1.0491 test_acc 60.26 test_fscore 60.7 time 5.58\n",
            "epoch 13 train_loss 0.6153 train_acc 82.19 train_fscore82.1 valid_loss nan valid_acc nan val_fscorenan test_loss 1.0383 test_acc 60.07 test_fscore 60.59 time 4.25\n",
            "epoch 14 train_loss 0.5585 train_acc 83.79 train_fscore83.79 valid_loss nan valid_acc nan val_fscorenan test_loss 1.0308 test_acc 59.83 test_fscore 60.31 time 4.29\n",
            "epoch 15 train_loss 0.5042 train_acc 85.63 train_fscore85.63 valid_loss nan valid_acc nan val_fscorenan test_loss 1.0365 test_acc 60.14 test_fscore 60.46 time 5.46\n",
            "epoch 16 train_loss 0.4596 train_acc 87.09 train_fscore87.09 valid_loss nan valid_acc nan val_fscorenan test_loss 1.044 test_acc 60.63 test_fscore 60.83 time 4.4\n",
            "epoch 17 train_loss 0.4259 train_acc 88.21 train_fscore88.22 valid_loss nan valid_acc nan val_fscorenan test_loss 1.0585 test_acc 60.69 test_fscore 60.84 time 4.39\n",
            "epoch 18 train_loss 0.3958 train_acc 89.09 train_fscore89.1 valid_loss nan valid_acc nan val_fscorenan test_loss 1.0857 test_acc 60.14 test_fscore 60.26 time 5.68\n",
            "epoch 19 train_loss 0.3599 train_acc 90.4 train_fscore90.42 valid_loss nan valid_acc nan val_fscorenan test_loss 1.1199 test_acc 60.2 test_fscore 60.22 time 4.48\n",
            "epoch 20 train_loss 0.343 train_acc 90.53 train_fscore90.53 valid_loss nan valid_acc nan val_fscorenan test_loss 1.1546 test_acc 59.89 test_fscore 59.9 time 4.45\n",
            "epoch 21 train_loss 0.3243 train_acc 91.1 train_fscore91.11 valid_loss nan valid_acc nan val_fscorenan test_loss 1.1739 test_acc 59.58 test_fscore 59.59 time 5.77\n",
            "epoch 22 train_loss 0.3108 train_acc 91.45 train_fscore91.45 valid_loss nan valid_acc nan val_fscorenan test_loss 1.1884 test_acc 59.83 test_fscore 59.79 time 4.65\n",
            "epoch 23 train_loss 0.2903 train_acc 92.03 train_fscore92.03 valid_loss nan valid_acc nan val_fscorenan test_loss 1.1972 test_acc 59.89 test_fscore 59.85 time 4.45\n",
            "epoch 24 train_loss 0.2908 train_acc 92.0 train_fscore92.0 valid_loss nan valid_acc nan val_fscorenan test_loss 1.2112 test_acc 60.07 test_fscore 59.91 time 5.55\n",
            "epoch 25 train_loss 0.2644 train_acc 92.91 train_fscore92.9 valid_loss nan valid_acc nan val_fscorenan test_loss 1.2486 test_acc 60.07 test_fscore 59.84 time 4.31\n",
            "epoch 26 train_loss 0.2621 train_acc 93.01 train_fscore93.0 valid_loss nan valid_acc nan val_fscorenan test_loss 1.2635 test_acc 59.7 test_fscore 59.47 time 4.67\n",
            "epoch 27 train_loss 0.2554 train_acc 92.81 train_fscore92.79 valid_loss nan valid_acc nan val_fscorenan test_loss 1.2585 test_acc 60.26 test_fscore 60.04 time 5.8\n",
            "epoch 28 train_loss 0.2496 train_acc 92.96 train_fscore92.95 valid_loss nan valid_acc nan val_fscorenan test_loss 1.2624 test_acc 59.95 test_fscore 59.65 time 4.51\n",
            "epoch 29 train_loss 0.2405 train_acc 93.2 train_fscore93.19 valid_loss nan valid_acc nan val_fscorenan test_loss 1.2732 test_acc 59.77 test_fscore 59.5 time 4.96\n",
            "epoch 30 train_loss 0.2285 train_acc 93.61 train_fscore93.6 valid_loss nan valid_acc nan val_fscorenan test_loss 1.2897 test_acc 59.52 test_fscore 59.32 time 5.03\n",
            "epoch 31 train_loss 0.2271 train_acc 93.56 train_fscore93.55 valid_loss nan valid_acc nan val_fscorenan test_loss 1.3067 test_acc 59.27 test_fscore 59.06 time 6.36\n",
            "epoch 32 train_loss 0.2214 train_acc 93.75 train_fscore93.74 valid_loss nan valid_acc nan val_fscorenan test_loss 1.3211 test_acc 59.7 test_fscore 59.43 time 5.75\n",
            "epoch 33 train_loss 0.2147 train_acc 93.96 train_fscore93.95 valid_loss nan valid_acc nan val_fscorenan test_loss 1.3404 test_acc 59.33 test_fscore 59.03 time 4.51\n",
            "epoch 34 train_loss 0.2108 train_acc 94.01 train_fscore94.0 valid_loss nan valid_acc nan val_fscorenan test_loss 1.3411 test_acc 59.09 test_fscore 58.87 time 4.44\n",
            "epoch 35 train_loss 0.2138 train_acc 93.91 train_fscore93.9 valid_loss nan valid_acc nan val_fscorenan test_loss 1.3472 test_acc 58.84 test_fscore 58.69 time 5.46\n",
            "epoch 36 train_loss 0.2135 train_acc 93.7 train_fscore93.69 valid_loss nan valid_acc nan val_fscorenan test_loss 1.3606 test_acc 58.35 test_fscore 58.2 time 4.59\n",
            "epoch 37 train_loss 0.2076 train_acc 93.99 train_fscore93.99 valid_loss nan valid_acc nan val_fscorenan test_loss 1.3747 test_acc 58.23 test_fscore 58.04 time 6.58\n",
            "epoch 38 train_loss 0.1945 train_acc 94.44 train_fscore94.43 valid_loss nan valid_acc nan val_fscorenan test_loss 1.397 test_acc 58.23 test_fscore 57.99 time 4.74\n",
            "epoch 39 train_loss 0.1935 train_acc 94.51 train_fscore94.5 valid_loss nan valid_acc nan val_fscorenan test_loss 1.4022 test_acc 58.47 test_fscore 58.22 time 4.37\n",
            "epoch 40 train_loss 0.187 train_acc 94.58 train_fscore94.57 valid_loss nan valid_acc nan val_fscorenan test_loss 1.4064 test_acc 58.1 test_fscore 57.8 time 5.54\n",
            "epoch 41 train_loss 0.183 train_acc 94.8 train_fscore94.79 valid_loss nan valid_acc nan val_fscorenan test_loss 1.4051 test_acc 58.1 test_fscore 57.77 time 4.42\n",
            "epoch 42 train_loss 0.1804 train_acc 94.8 train_fscore94.79 valid_loss nan valid_acc nan val_fscorenan test_loss 1.4005 test_acc 58.41 test_fscore 58.12 time 4.51\n",
            "epoch 43 train_loss 0.1757 train_acc 94.91 train_fscore94.89 valid_loss nan valid_acc nan val_fscorenan test_loss 1.4049 test_acc 58.72 test_fscore 58.51 time 5.8\n",
            "epoch 44 train_loss 0.1788 train_acc 94.85 train_fscore94.84 valid_loss nan valid_acc nan val_fscorenan test_loss 1.4106 test_acc 59.03 test_fscore 58.84 time 6.76\n",
            "epoch 45 train_loss 0.1788 train_acc 95.11 train_fscore95.11 valid_loss nan valid_acc nan val_fscorenan test_loss 1.4219 test_acc 58.66 test_fscore 58.42 time 6.75\n",
            "epoch 46 train_loss 0.1778 train_acc 94.89 train_fscore94.88 valid_loss nan valid_acc nan val_fscorenan test_loss 1.4267 test_acc 58.41 test_fscore 58.16 time 4.43\n",
            "epoch 47 train_loss 0.1719 train_acc 95.13 train_fscore95.12 valid_loss nan valid_acc nan val_fscorenan test_loss 1.4283 test_acc 57.67 test_fscore 57.44 time 5.27\n",
            "epoch 48 train_loss 0.1738 train_acc 94.73 train_fscore94.72 valid_loss nan valid_acc nan val_fscorenan test_loss 1.443 test_acc 57.86 test_fscore 57.61 time 6.97\n",
            "epoch 49 train_loss 0.1618 train_acc 95.54 train_fscore95.53 valid_loss nan valid_acc nan val_fscorenan test_loss 1.4566 test_acc 58.16 test_fscore 57.87 time 4.34\n",
            "epoch 50 train_loss 0.1677 train_acc 95.27 train_fscore95.26 valid_loss nan valid_acc nan val_fscorenan test_loss 1.4648 test_acc 58.04 test_fscore 57.77 time 5.13\n",
            "epoch 51 train_loss 0.1646 train_acc 95.39 train_fscore95.38 valid_loss nan valid_acc nan val_fscorenan test_loss 1.4702 test_acc 58.35 test_fscore 58.1 time 5.24\n",
            "epoch 52 train_loss 0.1605 train_acc 95.49 train_fscore95.48 valid_loss nan valid_acc nan val_fscorenan test_loss 1.4686 test_acc 58.1 test_fscore 57.87 time 4.57\n",
            "epoch 53 train_loss 0.1606 train_acc 95.32 train_fscore95.31 valid_loss nan valid_acc nan val_fscorenan test_loss 1.4652 test_acc 58.47 test_fscore 58.28 time 5.84\n",
            "epoch 54 train_loss 0.1517 train_acc 95.63 train_fscore95.62 valid_loss nan valid_acc nan val_fscorenan test_loss 1.4695 test_acc 58.53 test_fscore 58.37 time 4.58\n",
            "epoch 55 train_loss 0.1459 train_acc 95.75 train_fscore95.74 valid_loss nan valid_acc nan val_fscorenan test_loss 1.4861 test_acc 58.9 test_fscore 58.72 time 4.3\n",
            "epoch 56 train_loss 0.1514 train_acc 95.58 train_fscore95.57 valid_loss nan valid_acc nan val_fscorenan test_loss 1.4944 test_acc 59.15 test_fscore 58.91 time 8.23\n",
            "epoch 57 train_loss 0.1519 train_acc 95.51 train_fscore95.5 valid_loss nan valid_acc nan val_fscorenan test_loss 1.4919 test_acc 58.9 test_fscore 58.67 time 6.5\n",
            "epoch 58 train_loss 0.1489 train_acc 95.63 train_fscore95.62 valid_loss nan valid_acc nan val_fscorenan test_loss 1.4922 test_acc 58.35 test_fscore 58.11 time 5.69\n",
            "epoch 59 train_loss 0.1521 train_acc 95.68 train_fscore95.67 valid_loss nan valid_acc nan val_fscorenan test_loss 1.4943 test_acc 58.6 test_fscore 58.37 time 5.5\n",
            "epoch 60 train_loss 0.1476 train_acc 95.77 train_fscore95.76 valid_loss nan valid_acc nan val_fscorenan test_loss 1.5045 test_acc 58.72 test_fscore 58.49 time 6.59\n",
            "Test performance..\n",
            "Loss 1.0308 accuracy 59.83\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3500    0.3889    0.3684     144.0\n",
            "           1     0.9200    0.6571    0.7667     245.0\n",
            "           2     0.5385    0.5469    0.5426     384.0\n",
            "           3     0.6286    0.6471    0.6377     170.0\n",
            "           4     0.6642    0.6020    0.6316     299.0\n",
            "           5     0.5619    0.6667    0.6098     381.0\n",
            "\n",
            "    accuracy                         0.5983    1623.0\n",
            "   macro avg     0.6105    0.5848    0.5928    1623.0\n",
            "weighted avg     0.6175    0.5983    0.6031    1623.0\n",
            "\n",
            "[[ 56.   1.  25.   0.  61.   1.]\n",
            " [  4. 161.  35.   4.   1.  40.]\n",
            " [ 25.   5. 210.  20.  20. 104.]\n",
            " [  0.   2.   8. 110.   0.  50.]\n",
            " [ 75.   1.  40.   0. 180.   3.]\n",
            " [  0.   5.  72.  41.   9. 254.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DialogueRNN for MELD\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.optim as optim\n",
        "\n",
        "import argparse\n",
        "import time\n",
        "import pickle\n",
        "\n",
        "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score,\\\n",
        "                        classification_report, precision_recall_fscore_support\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "np.random.seed(1234)\n",
        "\n",
        "def get_train_valid_sampler(trainset, valid=0.1):\n",
        "    size = len(trainset)\n",
        "    idx = list(range(size))\n",
        "    split = int(valid*size)\n",
        "    return SubsetRandomSampler(idx[split:]), SubsetRandomSampler(idx[:split])\n",
        "\n",
        "def get_MELD_loaders(path, n_classes, batch_size=32, valid=0.1, num_workers=0, pin_memory=False):\n",
        "    trainset = MELDDataset(path=path, n_classes=n_classes)\n",
        "    train_sampler, valid_sampler = get_train_valid_sampler(trainset, valid)\n",
        "    train_loader = DataLoader(trainset,\n",
        "                              batch_size=batch_size,\n",
        "                              sampler=train_sampler,\n",
        "                              collate_fn=trainset.collate_fn,\n",
        "                              num_workers=num_workers,\n",
        "                              pin_memory=pin_memory)\n",
        "    valid_loader = DataLoader(trainset,\n",
        "                              batch_size=batch_size,\n",
        "                              sampler=valid_sampler,\n",
        "                              collate_fn=trainset.collate_fn,\n",
        "                              num_workers=num_workers,\n",
        "                              pin_memory=pin_memory)\n",
        "\n",
        "    testset = MELDDataset(path=path, n_classes=n_classes, train=False)\n",
        "    test_loader = DataLoader(testset,\n",
        "                             batch_size=batch_size,\n",
        "                             collate_fn=testset.collate_fn,\n",
        "                             num_workers=num_workers,\n",
        "                             pin_memory=pin_memory)\n",
        "\n",
        "    return train_loader, valid_loader, test_loader\n",
        "\n",
        "def train_or_eval_model(model, loss_function, dataloader, epoch, optimizer=None, train=False):\n",
        "    losses = []\n",
        "    preds = []\n",
        "    labels = []\n",
        "    masks = []\n",
        "    alphas, alphas_f, alphas_b, vids = [], [], [], []\n",
        "    assert not train or optimizer!=None\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    for data in dataloader:\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "        # import ipdb;ipdb.set_trace()\n",
        "        textf, acouf, qmask, umask, label =\\\n",
        "                [d.cuda() for d in data[:-1]] if cuda else data[:-1]\n",
        "        if feature_type == \"audio\":\n",
        "            log_prob, alpha, alpha_f, alpha_b = model(acouf, qmask,umask) # seq_len, batch, n_classes\n",
        "        elif feature_type == \"text\":\n",
        "            log_prob, alpha, alpha_f, alpha_b = model(textf, qmask,umask) # seq_len, batch, n_classes\n",
        "        else:\n",
        "            log_prob = model(torch.cat((textf,acouf),dim=-1), qmask,umask) # seq_len, batch, n_classes\n",
        "        lp_ = log_prob.transpose(0,1).contiguous().view(-1,log_prob.size()[2]) # batch*seq_len, n_classes\n",
        "        labels_ = label.view(-1) # batch*seq_len\n",
        "        loss = loss_function(lp_, labels_, umask)\n",
        "\n",
        "        pred_ = torch.argmax(lp_,1) # batch*seq_len\n",
        "        preds.append(pred_.data.cpu().numpy())\n",
        "        labels.append(labels_.data.cpu().numpy())\n",
        "        masks.append(umask.view(-1).cpu().numpy())\n",
        "\n",
        "        losses.append(loss.item()*masks[-1].sum())\n",
        "        if train:\n",
        "            loss.backward()\n",
        "#             if args.tensorboard:\n",
        "#                 for param in model.named_parameters():\n",
        "#                     writer.add_histogram(param[0], param[1].grad, epoch)\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            #alphas += alpha\n",
        "            #alphas_f += alpha_f\n",
        "            #alphas_b += alpha_b\n",
        "            vids += data[-1]\n",
        "\n",
        "    if preds!=[]:\n",
        "        preds  = np.concatenate(preds)\n",
        "        labels = np.concatenate(labels)\n",
        "        masks  = np.concatenate(masks)\n",
        "    else:\n",
        "        return float('nan'), float('nan'), [], [], [], float('nan'),[]\n",
        "\n",
        "    avg_loss = round(np.sum(losses)/np.sum(masks),4)\n",
        "    avg_accuracy = round(accuracy_score(labels,preds,sample_weight=masks)*100,2)\n",
        "    avg_fscore = round(f1_score(labels,preds,sample_weight=masks,average='weighted')*100,2)\n",
        "    class_report = classification_report(labels,preds,sample_weight=masks,digits=4)\n",
        "    return avg_loss, avg_accuracy, labels, preds, masks,avg_fscore, [alphas, alphas_f, alphas_b, vids], class_report\n",
        "\n",
        "seed_everything(seed=2023)\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "if cuda:\n",
        "    print('Running on GPU')\n",
        "else:\n",
        "    print('Running on CPU')\n",
        "    \n",
        "tensorboard = True    \n",
        "if tensorboard:\n",
        "    from tensorboardX import SummaryWriter\n",
        "writer = SummaryWriter()\n",
        "\n",
        "# choose between 'sentiment' or 'emotion'\n",
        "classification_type = 'emotion'\n",
        "feature_type = 'multimodal'\n",
        "\n",
        "data_path = 'drive/MyDrive/'\n",
        "batch_size = 30\n",
        "n_classes = 3\n",
        "n_epochs = 100\n",
        "active_listener = False\n",
        "attention = 'general'\n",
        "class_weight = False\n",
        "dropout = 0.1\n",
        "rec_dropout = 0.1\n",
        "l2 = 0.00001\n",
        "lr = 0.0005\n",
        "\n",
        "if feature_type == 'text':\n",
        "    print(\"Running on the text features........\")\n",
        "    D_m = 600\n",
        "elif feature_type == 'audio':\n",
        "    print(\"Running on the audio features........\")\n",
        "    D_m = 300\n",
        "else:\n",
        "    print(\"Running on the multimodal features........\")\n",
        "    D_m = 900\n",
        "D_g = 150\n",
        "D_p = 150\n",
        "D_e = 100\n",
        "D_h = 100\n",
        "\n",
        "D_a = 100 # concat attention\n",
        "\n",
        "loss_weights = torch.FloatTensor([1.0,1.0,1.0])\n",
        "\n",
        "if classification_type.strip().lower() == 'emotion':\n",
        "    n_classes = 7\n",
        "    loss_weights = torch.FloatTensor([1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
        "\n",
        "model = Model(D_m, D_g, D_p, D_e, D_h,\n",
        "                n_classes=n_classes,\n",
        "                listener_state=active_listener,\n",
        "                context_attention=attention,\n",
        "                dropout_rec=rec_dropout,\n",
        "                dropout=dropout)\n",
        "\n",
        "if cuda:\n",
        "    model.cuda()\n",
        "if class_weight:\n",
        "    loss_function  = MaskedNLLLoss(loss_weights.cuda() if cuda else loss_weights)\n",
        "else:\n",
        "    loss_function = MaskedNLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=lr,\n",
        "                       weight_decay=l2)\n",
        "\n",
        "train_loader, valid_loader, test_loader =\\\n",
        "        get_MELD_loaders(data_path + 'MELD_features_raw.pkl', n_classes,\n",
        "                            valid=0.0,\n",
        "                            batch_size=batch_size,\n",
        "                            num_workers=0)\n",
        "\n",
        "best_fscore, best_loss, best_label, best_pred, best_mask = None, None, None, None, None\n",
        "\n",
        "\n",
        "for e in range(n_epochs):\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc, _,_,_,train_fscore,_,_= train_or_eval_model(model, loss_function,\n",
        "                                           train_loader, e, optimizer, True)\n",
        "    valid_loss, valid_acc, _,_,_,val_fscore,_= train_or_eval_model(model, loss_function, valid_loader, e)\n",
        "    test_loss, test_acc, test_label, test_pred, test_mask, test_fscore, attentions, test_class_report = train_or_eval_model(model, loss_function, test_loader, e)\n",
        "\n",
        "    if best_fscore == None or best_fscore < test_fscore:\n",
        "        best_fscore, best_loss, best_label, best_pred, best_mask, best_attn =\\\n",
        "                test_fscore, test_loss, test_label, test_pred, test_mask, attentions\n",
        "\n",
        "#     if args.tensorboard:\n",
        "#         writer.add_scalar('test: accuracy/loss',test_acc/test_loss,e)\n",
        "#         writer.add_scalar('train: accuracy/loss',train_acc/train_loss,e)\n",
        "    print('epoch {} train_loss {} train_acc {} train_fscore {} valid_loss {} valid_acc {} val_fscore {} test_loss {} test_acc {} test_fscore {} time {}'.\\\n",
        "            format(e+1, train_loss, train_acc, train_fscore, valid_loss, valid_acc, val_fscore,\\\n",
        "                    test_loss, test_acc, test_fscore, round(time.time()-start_time,2)))\n",
        "    print (test_class_report)\n",
        "if tensorboard:\n",
        "    writer.close()\n",
        "\n",
        "print('Test performance..')\n",
        "print('Fscore {} accuracy {}'.format(best_fscore,\n",
        "                                 round(accuracy_score(best_label,best_pred,sample_weight=best_mask)*100,2)))\n",
        "print(classification_report(best_label,best_pred,sample_weight=best_mask,digits=4))\n",
        "print(confusion_matrix(best_label,best_pred,sample_weight=best_mask))"
      ],
      "metadata": {
        "id": "rhSL989pKzxO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4509b4a8-2852-4e45-f23f-ab0c19a3201c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023\n",
            "Running on GPU\n",
            "Running on the multimodal features........\n",
            "epoch 1 train_loss 1.578 train_acc 45.28 train_fscore 30.23 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3755 test_acc 51.46 test_fscore 40.27 time 6.58\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5609    0.9650    0.7094    1256.0\n",
            "           1     0.0000    0.0000    0.0000     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.0000    0.0000    0.0000     208.0\n",
            "           4     0.2440    0.2264    0.2348     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.5263    0.1159    0.1900     345.0\n",
            "\n",
            "    accuracy                         0.5146    2610.0\n",
            "   macro avg     0.1902    0.1868    0.1620    2610.0\n",
            "weighted avg     0.3770    0.5146    0.4027    2610.0\n",
            "\n",
            "epoch 2 train_loss 1.2323 train_acc 55.93 train_fscore 48.95 valid_loss nan valid_acc nan val_fscore nan test_loss 1.336 test_acc 54.44 test_fscore 47.11 time 6.38\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6946    0.8654    0.7706    1256.0\n",
            "           1     0.0000    0.0000    0.0000     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.0000    0.0000    0.0000     208.0\n",
            "           4     0.2937    0.6567    0.4058     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4795    0.2029    0.2851     345.0\n",
            "\n",
            "    accuracy                         0.5444    2610.0\n",
            "   macro avg     0.2097    0.2464    0.2088    2610.0\n",
            "weighted avg     0.4429    0.5444    0.4711    2610.0\n",
            "\n",
            "epoch 3 train_loss 1.1075 train_acc 61.86 train_fscore 56.73 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2566 test_acc 58.28 test_fscore 54.87 time 6.71\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7205    0.8105    0.7628    1256.0\n",
            "           1     0.4215    0.5160    0.4640     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3125    0.0240    0.0446     208.0\n",
            "           4     0.4798    0.5025    0.4909     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3630    0.4377    0.3968     345.0\n",
            "\n",
            "    accuracy                         0.5828    2610.0\n",
            "   macro avg     0.3282    0.3272    0.3085    2610.0\n",
            "weighted avg     0.5389    0.5828    0.5487    2610.0\n",
            "\n",
            "epoch 4 train_loss 1.019 train_acc 66.71 train_fscore 62.53 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2266 test_acc 60.0 test_fscore 56.36 time 5.92\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7007    0.8479    0.7673    1256.0\n",
            "           1     0.5058    0.4626    0.4833     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2727    0.0721    0.1141     208.0\n",
            "           4     0.4897    0.5348    0.5113     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4159    0.4087    0.4123     345.0\n",
            "\n",
            "    accuracy                         0.6000    2610.0\n",
            "   macro avg     0.3407    0.3323    0.3269    2610.0\n",
            "weighted avg     0.5438    0.6000    0.5636    2610.0\n",
            "\n",
            "epoch 5 train_loss 0.9855 train_acc 67.99 train_fscore 64.69 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2337 test_acc 59.43 test_fscore 56.21 time 7.04\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7180    0.8129    0.7625    1256.0\n",
            "           1     0.4451    0.5480    0.4912     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3871    0.0577    0.1004     208.0\n",
            "           4     0.5123    0.5174    0.5149     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3852    0.4522    0.4160     345.0\n",
            "\n",
            "    accuracy                         0.5943    2610.0\n",
            "   macro avg     0.3497    0.3412    0.3264    2610.0\n",
            "weighted avg     0.5541    0.5943    0.5621    2610.0\n",
            "\n",
            "epoch 6 train_loss 0.9829 train_acc 68.09 train_fscore 64.58 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2185 test_acc 59.46 test_fscore 56.63 time 6.05\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7112    0.8272    0.7648    1256.0\n",
            "           1     0.4532    0.5338    0.4902     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2268    0.1058    0.1443     208.0\n",
            "           4     0.5683    0.4453    0.4993     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3990    0.4696    0.4314     345.0\n",
            "\n",
            "    accuracy                         0.5946    2610.0\n",
            "   macro avg     0.3369    0.3402    0.3329    2610.0\n",
            "weighted avg     0.5494    0.5946    0.5663    2610.0\n",
            "\n",
            "epoch 7 train_loss 0.9694 train_acc 68.51 train_fscore 65.3 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2325 test_acc 59.23 test_fscore 56.74 time 7.07\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7226    0.8089    0.7633    1256.0\n",
            "           1     0.4320    0.5765    0.4939     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2680    0.1250    0.1705     208.0\n",
            "           4     0.5623    0.4378    0.4923     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3962    0.4812    0.4346     345.0\n",
            "\n",
            "    accuracy                         0.5923    2610.0\n",
            "   macro avg     0.3402    0.3471    0.3364    2610.0\n",
            "weighted avg     0.5546    0.5923    0.5674    2610.0\n",
            "\n",
            "epoch 8 train_loss 0.9644 train_acc 68.89 train_fscore 66.08 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2252 test_acc 59.04 test_fscore 56.54 time 5.9\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7300    0.7858    0.7569    1256.0\n",
            "           1     0.4065    0.6263    0.4930     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3818    0.1010    0.1597     208.0\n",
            "           4     0.5307    0.4950    0.5122     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4000    0.4580    0.4270     345.0\n",
            "\n",
            "    accuracy                         0.5904    2610.0\n",
            "   macro avg     0.3499    0.3523    0.3355    2610.0\n",
            "weighted avg     0.5601    0.5904    0.5654    2610.0\n",
            "\n",
            "epoch 9 train_loss 0.9582 train_acc 68.71 train_fscore 65.76 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2149 test_acc 58.7 test_fscore 57.03 time 7.04\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7332    0.7635    0.7480    1256.0\n",
            "           1     0.4851    0.5231    0.5034     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3226    0.1923    0.2410     208.0\n",
            "           4     0.5036    0.5249    0.5140     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3838    0.5072    0.4370     345.0\n",
            "\n",
            "    accuracy                         0.5870    2610.0\n",
            "   macro avg     0.3469    0.3587    0.3491    2610.0\n",
            "weighted avg     0.5591    0.5870    0.5703    2610.0\n",
            "\n",
            "epoch 10 train_loss 0.9495 train_acc 68.99 train_fscore 66.43 valid_loss nan valid_acc nan val_fscore nan test_loss 1.23 test_acc 59.39 test_fscore 56.47 time 5.91\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7244    0.7930    0.7571    1256.0\n",
            "           1     0.4868    0.5267    0.5060     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3784    0.0673    0.1143     208.0\n",
            "           4     0.4769    0.5647    0.5171     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3947    0.4783    0.4325     345.0\n",
            "\n",
            "    accuracy                         0.5939    2610.0\n",
            "   macro avg     0.3516    0.3471    0.3324    2610.0\n",
            "weighted avg     0.5568    0.5939    0.5647    2610.0\n",
            "\n",
            "epoch 11 train_loss 0.9458 train_acc 69.21 train_fscore 66.71 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2063 test_acc 60.0 test_fscore 57.08 time 7.12\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7070    0.8336    0.7651    1256.0\n",
            "           1     0.4854    0.4733    0.4793     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2577    0.1202    0.1639     208.0\n",
            "           4     0.5513    0.4677    0.5061     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4149    0.5014    0.4541     345.0\n",
            "\n",
            "    accuracy                         0.6000    2610.0\n",
            "   macro avg     0.3452    0.3423    0.3383    2610.0\n",
            "weighted avg     0.5528    0.6000    0.5708    2610.0\n",
            "\n",
            "epoch 12 train_loss 0.9402 train_acc 69.35 train_fscore 66.92 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2205 test_acc 60.08 test_fscore 57.17 time 6.01\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7050    0.8392    0.7663    1256.0\n",
            "           1     0.5085    0.4270    0.4642     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2661    0.1394    0.1830     208.0\n",
            "           4     0.5497    0.4677    0.5054     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4136    0.5130    0.4580     345.0\n",
            "\n",
            "    accuracy                         0.6008    2610.0\n",
            "   macro avg     0.3490    0.3409    0.3395    2610.0\n",
            "weighted avg     0.5546    0.6008    0.5717    2610.0\n",
            "\n",
            "epoch 13 train_loss 0.9427 train_acc 69.23 train_fscore 66.84 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2155 test_acc 58.54 test_fscore 56.9 time 6.91\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7301    0.7818    0.7551    1256.0\n",
            "           1     0.4514    0.5125    0.4800     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2848    0.2260    0.2520     208.0\n",
            "           4     0.5644    0.4254    0.4851     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3849    0.5333    0.4471     345.0\n",
            "\n",
            "    accuracy                         0.5854    2610.0\n",
            "   macro avg     0.3451    0.3541    0.3456    2610.0\n",
            "weighted avg     0.5605    0.5854    0.5690    2610.0\n",
            "\n",
            "epoch 14 train_loss 0.9345 train_acc 69.49 train_fscore 67.25 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2333 test_acc 60.34 test_fscore 57.03 time 6.78\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7022    0.8392    0.7646    1256.0\n",
            "           1     0.5018    0.4840    0.4928     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2683    0.1058    0.1517     208.0\n",
            "           4     0.4839    0.5622    0.5201     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4740    0.3971    0.4322     345.0\n",
            "\n",
            "    accuracy                         0.6034    2610.0\n",
            "   macro avg     0.3472    0.3412    0.3373    2610.0\n",
            "weighted avg     0.5505    0.6034    0.5703    2610.0\n",
            "\n",
            "epoch 15 train_loss 0.9285 train_acc 69.64 train_fscore 67.19 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2206 test_acc 58.12 test_fscore 56.9 time 7.16\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7402    0.7532    0.7466    1256.0\n",
            "           1     0.4493    0.5516    0.4952     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2698    0.2452    0.2569     208.0\n",
            "           4     0.5213    0.4876    0.5039     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4005    0.4899    0.4407     345.0\n",
            "\n",
            "    accuracy                         0.5812    2610.0\n",
            "   macro avg     0.3402    0.3611    0.3490    2610.0\n",
            "weighted avg     0.5593    0.5812    0.5690    2610.0\n",
            "\n",
            "epoch 16 train_loss 0.9158 train_acc 70.21 train_fscore 67.97 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2121 test_acc 58.05 test_fscore 56.73 time 5.97\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7304    0.7635    0.7466    1256.0\n",
            "           1     0.5132    0.4164    0.4597     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2642    0.2692    0.2667     208.0\n",
            "           4     0.5072    0.5224    0.5147     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3905    0.5014    0.4391     345.0\n",
            "\n",
            "    accuracy                         0.5805    2610.0\n",
            "   macro avg     0.3436    0.3533    0.3467    2610.0\n",
            "weighted avg     0.5575    0.5805    0.5673    2610.0\n",
            "\n",
            "epoch 17 train_loss 0.9116 train_acc 70.35 train_fscore 68.05 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2323 test_acc 59.23 test_fscore 56.61 time 7.46\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7146    0.8153    0.7616    1256.0\n",
            "           1     0.4459    0.4840    0.4642     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3659    0.1442    0.2069     208.0\n",
            "           4     0.5545    0.4179    0.4766     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3860    0.5449    0.4519     345.0\n",
            "\n",
            "    accuracy                         0.5923    2610.0\n",
            "   macro avg     0.3524    0.3438    0.3373    2610.0\n",
            "weighted avg     0.5575    0.5923    0.5661    2610.0\n",
            "\n",
            "epoch 18 train_loss 0.9074 train_acc 70.33 train_fscore 68.0 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2085 test_acc 59.69 test_fscore 57.42 time 5.92\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7286    0.7994    0.7623    1256.0\n",
            "           1     0.4601    0.5338    0.4942     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3608    0.1683    0.2295     208.0\n",
            "           4     0.4895    0.5199    0.5042     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4188    0.4638    0.4402     345.0\n",
            "\n",
            "    accuracy                         0.5969    2610.0\n",
            "   macro avg     0.3511    0.3550    0.3472    2610.0\n",
            "weighted avg     0.5597    0.5969    0.5742    2610.0\n",
            "\n",
            "epoch 19 train_loss 0.9105 train_acc 70.11 train_fscore 67.84 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2472 test_acc 58.74 test_fscore 56.18 time 6.65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7250    0.7914    0.7568    1256.0\n",
            "           1     0.4360    0.5694    0.4938     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.4375    0.1010    0.1641     208.0\n",
            "           4     0.5153    0.4602    0.4862     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3720    0.5014    0.4272     345.0\n",
            "\n",
            "    accuracy                         0.5874    2610.0\n",
            "   macro avg     0.3551    0.3462    0.3326    2610.0\n",
            "weighted avg     0.5593    0.5874    0.5618    2610.0\n",
            "\n",
            "epoch 20 train_loss 0.9042 train_acc 70.37 train_fscore 68.06 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2527 test_acc 59.46 test_fscore 55.93 time 6.55\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7035    0.8424    0.7667    1256.0\n",
            "           1     0.4771    0.4448    0.4604     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.4000    0.0769    0.1290     208.0\n",
            "           4     0.5976    0.3731    0.4594     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3671    0.5884    0.4521     345.0\n",
            "\n",
            "    accuracy                         0.5946    2610.0\n",
            "   macro avg     0.3636    0.3322    0.3239    2610.0\n",
            "weighted avg     0.5623    0.5946    0.5593    2610.0\n",
            "\n",
            "epoch 21 train_loss 0.9059 train_acc 70.23 train_fscore 67.99 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2253 test_acc 58.28 test_fscore 56.39 time 6.52\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7320    0.7763    0.7535    1256.0\n",
            "           1     0.4585    0.4520    0.4552     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3673    0.1731    0.2353     208.0\n",
            "           4     0.5487    0.4204    0.4761     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3597    0.6203    0.4553     345.0\n",
            "\n",
            "    accuracy                         0.5828    2610.0\n",
            "   macro avg     0.3523    0.3489    0.3393    2610.0\n",
            "weighted avg     0.5629    0.5828    0.5639    2610.0\n",
            "\n",
            "epoch 22 train_loss 0.9025 train_acc 70.28 train_fscore 67.99 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2174 test_acc 58.7 test_fscore 56.74 time 6.35\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7324    0.7803    0.7556    1256.0\n",
            "           1     0.4298    0.5445    0.4804     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3789    0.1731    0.2376     208.0\n",
            "           4     0.5317    0.4378    0.4802     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3816    0.5420    0.4479     345.0\n",
            "\n",
            "    accuracy                         0.5870    2610.0\n",
            "   macro avg     0.3506    0.3540    0.3431    2610.0\n",
            "weighted avg     0.5613    0.5870    0.5674    2610.0\n",
            "\n",
            "epoch 23 train_loss 0.9001 train_acc 70.43 train_fscore 68.16 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2323 test_acc 59.66 test_fscore 57.21 time 5.95\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7186    0.8010    0.7575    1256.0\n",
            "           1     0.5160    0.4591    0.4859     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3721    0.1538    0.2177     208.0\n",
            "           4     0.4845    0.5448    0.5129     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4052    0.4957    0.4459     345.0\n",
            "\n",
            "    accuracy                         0.5966    2610.0\n",
            "   macro avg     0.3566    0.3506    0.3457    2610.0\n",
            "weighted avg     0.5592    0.5966    0.5721    2610.0\n",
            "\n",
            "epoch 24 train_loss 0.8952 train_acc 70.62 train_fscore 68.31 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2129 test_acc 58.62 test_fscore 57.12 time 6.93\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7403    0.7580    0.7490    1256.0\n",
            "           1     0.4762    0.4982    0.4870     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3333    0.2115    0.2588     208.0\n",
            "           4     0.4976    0.5199    0.5085     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3870    0.5362    0.4496     345.0\n",
            "\n",
            "    accuracy                         0.5862    2610.0\n",
            "   macro avg     0.3478    0.3606    0.3504    2610.0\n",
            "weighted avg     0.5619    0.5862    0.5712    2610.0\n",
            "\n",
            "epoch 25 train_loss 0.885 train_acc 70.65 train_fscore 68.37 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2547 test_acc 58.08 test_fscore 55.93 time 6.01\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7304    0.7723    0.7508    1256.0\n",
            "           1     0.5216    0.4306    0.4717     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.4074    0.1058    0.1679     208.0\n",
            "           4     0.5389    0.4478    0.4891     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3369    0.6464    0.4429     345.0\n",
            "\n",
            "    accuracy                         0.5808    2610.0\n",
            "   macro avg     0.3622    0.3433    0.3318    2610.0\n",
            "weighted avg     0.5677    0.5808    0.5593    2610.0\n",
            "\n",
            "epoch 26 train_loss 0.8847 train_acc 70.79 train_fscore 68.53 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2513 test_acc 59.39 test_fscore 57.05 time 7.01\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7240    0.7938    0.7573    1256.0\n",
            "           1     0.4515    0.5302    0.4877     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3704    0.1442    0.2076     208.0\n",
            "           4     0.5205    0.4726    0.4954     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4026    0.5333    0.4589     345.0\n",
            "\n",
            "    accuracy                         0.5939    2610.0\n",
            "   macro avg     0.3527    0.3535    0.3438    2610.0\n",
            "weighted avg     0.5600    0.5939    0.5705    2610.0\n",
            "\n",
            "epoch 27 train_loss 0.8771 train_acc 70.82 train_fscore 68.57 valid_loss nan valid_acc nan val_fscore nan test_loss 1.227 test_acc 59.23 test_fscore 57.15 time 5.85\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7252    0.7922    0.7572    1256.0\n",
            "           1     0.4704    0.4804    0.4754     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3254    0.1971    0.2455     208.0\n",
            "           4     0.4857    0.5498    0.5158     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4162    0.4464    0.4308     345.0\n",
            "\n",
            "    accuracy                         0.5923    2610.0\n",
            "   macro avg     0.3461    0.3523    0.3464    2610.0\n",
            "weighted avg     0.5554    0.5923    0.5715    2610.0\n",
            "\n",
            "epoch 28 train_loss 0.8673 train_acc 71.31 train_fscore 69.14 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2342 test_acc 58.85 test_fscore 56.82 time 7.04\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7242    0.7922    0.7567    1256.0\n",
            "           1     0.4781    0.4662    0.4721     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3276    0.1827    0.2346     208.0\n",
            "           4     0.5524    0.4328    0.4854     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3729    0.5739    0.4521     345.0\n",
            "\n",
            "    accuracy                         0.5885    2610.0\n",
            "   macro avg     0.3507    0.3497    0.3430    2610.0\n",
            "weighted avg     0.5604    0.5885    0.5682    2610.0\n",
            "\n",
            "epoch 29 train_loss 0.8695 train_acc 71.51 train_fscore 69.21 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2183 test_acc 58.28 test_fscore 56.51 time 5.79\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7318    0.7779    0.7541    1256.0\n",
            "           1     0.4596    0.4448    0.4521     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3280    0.1971    0.2462     208.0\n",
            "           4     0.5443    0.4279    0.4791     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3665    0.5971    0.4542     345.0\n",
            "\n",
            "    accuracy                         0.5828    2610.0\n",
            "   macro avg     0.3472    0.3493    0.3408    2610.0\n",
            "weighted avg     0.5601    0.5828    0.5651    2610.0\n",
            "\n",
            "epoch 30 train_loss 0.871 train_acc 71.05 train_fscore 68.81 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2494 test_acc 58.28 test_fscore 56.58 time 7.19\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7413    0.7484    0.7448    1256.0\n",
            "           1     0.4559    0.5516    0.4992     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3444    0.1490    0.2081     208.0\n",
            "           4     0.5025    0.5025    0.5025     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3784    0.5594    0.4515     345.0\n",
            "\n",
            "    accuracy                         0.5828    2610.0\n",
            "   macro avg     0.3461    0.3587    0.3437    2610.0\n",
            "weighted avg     0.5607    0.5828    0.5658    2610.0\n",
            "\n",
            "epoch 31 train_loss 0.8575 train_acc 71.84 train_fscore 69.63 valid_loss nan valid_acc nan val_fscore nan test_loss 1.248 test_acc 59.16 test_fscore 56.4 time 5.86\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7199    0.8185    0.7660    1256.0\n",
            "           1     0.5023    0.3950    0.4422     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3425    0.1202    0.1779     208.0\n",
            "           4     0.5589    0.4129    0.4750     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3621    0.6203    0.4573     345.0\n",
            "\n",
            "    accuracy                         0.5916    2610.0\n",
            "   macro avg     0.3551    0.3381    0.3312    2610.0\n",
            "weighted avg     0.5617    0.5916    0.5640    2610.0\n",
            "\n",
            "epoch 32 train_loss 0.8542 train_acc 71.61 train_fscore 69.41 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2475 test_acc 58.81 test_fscore 56.57 time 6.84\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7273    0.7858    0.7555    1256.0\n",
            "           1     0.4667    0.4733    0.4700     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3816    0.1394    0.2042     208.0\n",
            "           4     0.4816    0.5199    0.5000     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3865    0.5130    0.4408     345.0\n",
            "\n",
            "    accuracy                         0.5881    2610.0\n",
            "   macro avg     0.3491    0.3474    0.3386    2610.0\n",
            "weighted avg     0.5559    0.5881    0.5657    2610.0\n",
            "\n",
            "epoch 33 train_loss 0.8504 train_acc 71.79 train_fscore 69.55 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2478 test_acc 58.43 test_fscore 56.24 time 6.2\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7254    0.7803    0.7518    1256.0\n",
            "           1     0.4430    0.4698    0.4560     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3293    0.1298    0.1862     208.0\n",
            "           4     0.5401    0.4527    0.4926     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3764    0.5913    0.4600     345.0\n",
            "\n",
            "    accuracy                         0.5843    2610.0\n",
            "   macro avg     0.3449    0.3463    0.3352    2610.0\n",
            "weighted avg     0.5559    0.5843    0.5624    2610.0\n",
            "\n",
            "epoch 34 train_loss 0.842 train_acc 72.29 train_fscore 70.14 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2714 test_acc 59.31 test_fscore 57.23 time 6.92\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7239    0.7954    0.7580    1256.0\n",
            "           1     0.4789    0.4840    0.4814     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2937    0.2019    0.2393     208.0\n",
            "           4     0.4773    0.5498    0.5110     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4425    0.4348    0.4386     345.0\n",
            "\n",
            "    accuracy                         0.5931    2610.0\n",
            "   macro avg     0.3452    0.3523    0.3469    2610.0\n",
            "weighted avg     0.5553    0.5931    0.5723    2610.0\n",
            "\n",
            "epoch 35 train_loss 0.8392 train_acc 72.27 train_fscore 70.04 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2681 test_acc 56.97 test_fscore 55.99 time 6.06\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7557    0.7094    0.7318    1256.0\n",
            "           1     0.4316    0.5730    0.4924     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3200    0.1923    0.2402     208.0\n",
            "           4     0.5000    0.4975    0.4988     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3659    0.5652    0.4442     345.0\n",
            "\n",
            "    accuracy                         0.5697    2610.0\n",
            "   macro avg     0.3390    0.3625    0.3439    2610.0\n",
            "weighted avg     0.5610    0.5697    0.5599    2610.0\n",
            "\n",
            "epoch 36 train_loss 0.8298 train_acc 72.2 train_fscore 70.09 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2475 test_acc 57.66 test_fscore 56.18 time 6.66\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7314    0.7611    0.7460    1256.0\n",
            "           1     0.4406    0.5018    0.4692     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2667    0.2115    0.2359     208.0\n",
            "           4     0.4820    0.5000    0.4908     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4085    0.4725    0.4382     345.0\n",
            "\n",
            "    accuracy                         0.5766    2610.0\n",
            "   macro avg     0.3328    0.3496    0.3400    2610.0\n",
            "weighted avg     0.5489    0.5766    0.5618    2610.0\n",
            "\n",
            "epoch 37 train_loss 0.8291 train_acc 72.58 train_fscore 70.49 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2543 test_acc 58.51 test_fscore 56.33 time 6.29\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7229    0.7850    0.7527    1256.0\n",
            "           1     0.4185    0.5480    0.4746     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3021    0.1394    0.1908     208.0\n",
            "           4     0.5408    0.4453    0.4884     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3987    0.5188    0.4509     345.0\n",
            "\n",
            "    accuracy                         0.5851    2610.0\n",
            "   macro avg     0.3404    0.3481    0.3368    2610.0\n",
            "weighted avg     0.5530    0.5851    0.5633    2610.0\n",
            "\n",
            "epoch 38 train_loss 0.8203 train_acc 72.79 train_fscore 70.76 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2748 test_acc 58.7 test_fscore 56.17 time 6.28\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7258    0.7882    0.7557    1256.0\n",
            "           1     0.4320    0.5196    0.4717     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3684    0.1010    0.1585     208.0\n",
            "           4     0.4756    0.5100    0.4922     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4057    0.4928    0.4450     345.0\n",
            "\n",
            "    accuracy                         0.5870    2610.0\n",
            "   macro avg     0.3439    0.3445    0.3319    2610.0\n",
            "weighted avg     0.5520    0.5870    0.5617    2610.0\n",
            "\n",
            "epoch 39 train_loss 0.8157 train_acc 72.53 train_fscore 70.41 valid_loss nan valid_acc nan val_fscore nan test_loss 1.26 test_acc 57.89 test_fscore 56.17 time 6.64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7347    0.7651    0.7496    1256.0\n",
            "           1     0.5000    0.4093    0.4501     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3304    0.1827    0.2353     208.0\n",
            "           4     0.5475    0.4154    0.4724     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3528    0.6667    0.4614     345.0\n",
            "\n",
            "    accuracy                         0.5789    2610.0\n",
            "   macro avg     0.3522    0.3485    0.3384    2610.0\n",
            "weighted avg     0.5647    0.5789    0.5617    2610.0\n",
            "\n",
            "epoch 40 train_loss 0.8076 train_acc 73.13 train_fscore 71.05 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2733 test_acc 57.55 test_fscore 55.91 time 5.86\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7378    0.7572    0.7473    1256.0\n",
            "           1     0.3966    0.5801    0.4711     281.0\n",
            "           2     0.1667    0.0200    0.0357      50.0\n",
            "           3     0.3168    0.1538    0.2071     208.0\n",
            "           4     0.4896    0.4677    0.4784     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3995    0.4841    0.4377     345.0\n",
            "\n",
            "    accuracy                         0.5755    2610.0\n",
            "   macro avg     0.3581    0.3518    0.3396    2610.0\n",
            "weighted avg     0.5544    0.5755    0.5591    2610.0\n",
            "\n",
            "epoch 41 train_loss 0.8114 train_acc 72.94 train_fscore 70.84 valid_loss nan valid_acc nan val_fscore nan test_loss 1.256 test_acc 58.08 test_fscore 56.71 time 6.86\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7348    0.7588    0.7466    1256.0\n",
            "           1     0.4702    0.5053    0.4871     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2541    0.2212    0.2365     208.0\n",
            "           4     0.4785    0.5249    0.5006     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4271    0.4754    0.4499     345.0\n",
            "\n",
            "    accuracy                         0.5808    2610.0\n",
            "   macro avg     0.3378    0.3551    0.3458    2610.0\n",
            "weighted avg     0.5546    0.5808    0.5671    2610.0\n",
            "\n",
            "epoch 42 train_loss 0.7998 train_acc 73.54 train_fscore 71.49 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2769 test_acc 57.36 test_fscore 55.97 time 5.76\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7364    0.7452    0.7408    1256.0\n",
            "           1     0.4558    0.4769    0.4661     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2774    0.1827    0.2203     208.0\n",
            "           4     0.5429    0.4403    0.4863     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3655    0.6145    0.4584     345.0\n",
            "\n",
            "    accuracy                         0.5736    2610.0\n",
            "   macro avg     0.3397    0.3514    0.3388    2610.0\n",
            "weighted avg     0.5575    0.5736    0.5597    2610.0\n",
            "\n",
            "epoch 43 train_loss 0.7904 train_acc 73.33 train_fscore 71.36 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2852 test_acc 57.89 test_fscore 55.65 time 6.92\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7214    0.7874    0.7530    1256.0\n",
            "           1     0.4314    0.5374    0.4786     281.0\n",
            "           2     0.2500    0.0200    0.0370      50.0\n",
            "           3     0.3103    0.1298    0.1831     208.0\n",
            "           4     0.5464    0.3806    0.4487     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3668    0.5507    0.4403     345.0\n",
            "\n",
            "    accuracy                         0.5789    2610.0\n",
            "   macro avg     0.3752    0.3437    0.3344    2610.0\n",
            "weighted avg     0.5558    0.5789    0.5565    2610.0\n",
            "\n",
            "epoch 44 train_loss 0.7889 train_acc 73.45 train_fscore 71.52 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2609 test_acc 59.2 test_fscore 57.05 time 5.67\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7183    0.8041    0.7588    1256.0\n",
            "           1     0.4692    0.4875    0.4782     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2817    0.1923    0.2286     208.0\n",
            "           4     0.4880    0.5075    0.4976     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4464    0.4464    0.4464     345.0\n",
            "\n",
            "    accuracy                         0.5920    2610.0\n",
            "   macro avg     0.3434    0.3483    0.3442    2610.0\n",
            "weighted avg     0.5528    0.5920    0.5705    2610.0\n",
            "\n",
            "epoch 45 train_loss 0.7945 train_acc 73.14 train_fscore 71.12 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2994 test_acc 57.59 test_fscore 55.51 time 6.93\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7300    0.7683    0.7486    1256.0\n",
            "           1     0.4198    0.5125    0.4615     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3288    0.1154    0.1708     208.0\n",
            "           4     0.5265    0.4204    0.4675     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3668    0.5826    0.4502     345.0\n",
            "\n",
            "    accuracy                         0.5759    2610.0\n",
            "   macro avg     0.3388    0.3427    0.3284    2610.0\n",
            "weighted avg     0.5522    0.5759    0.5551    2610.0\n",
            "\n",
            "epoch 46 train_loss 0.7786 train_acc 74.0 train_fscore 72.02 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3037 test_acc 58.08 test_fscore 56.3 time 5.59\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7353    0.7651    0.7499    1256.0\n",
            "           1     0.4675    0.5125    0.4890     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2772    0.1346    0.1812     208.0\n",
            "           4     0.5066    0.4751    0.4904     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3780    0.5565    0.4502     345.0\n",
            "\n",
            "    accuracy                         0.5808    2610.0\n",
            "   macro avg     0.3378    0.3491    0.3372    2610.0\n",
            "weighted avg     0.5543    0.5808    0.5630    2610.0\n",
            "\n",
            "epoch 47 train_loss 0.7766 train_acc 74.09 train_fscore 72.21 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2957 test_acc 56.82 test_fscore 55.8 time 6.4\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7335    0.7428    0.7381    1256.0\n",
            "           1     0.4426    0.4804    0.4608     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2176    0.2260    0.2217     208.0\n",
            "           4     0.5303    0.4577    0.4913     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3932    0.5333    0.4526     345.0\n",
            "\n",
            "    accuracy                         0.5682    2610.0\n",
            "   macro avg     0.3310    0.3486    0.3378    2610.0\n",
            "weighted avg     0.5516    0.5682    0.5580    2610.0\n",
            "\n",
            "epoch 48 train_loss 0.7622 train_acc 74.74 train_fscore 72.97 valid_loss nan valid_acc nan val_fscore nan test_loss 1.2962 test_acc 58.66 test_fscore 56.44 time 5.89\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7247    0.8025    0.7616    1256.0\n",
            "           1     0.4249    0.5338    0.4732     281.0\n",
            "           2     0.1667    0.0200    0.0357      50.0\n",
            "           3     0.2830    0.1442    0.1911     208.0\n",
            "           4     0.4637    0.4925    0.4777     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4528    0.4174    0.4344     345.0\n",
            "\n",
            "    accuracy                         0.5866    2610.0\n",
            "   macro avg     0.3594    0.3444    0.3391    2610.0\n",
            "weighted avg     0.5515    0.5866    0.5644    2610.0\n",
            "\n",
            "epoch 49 train_loss 0.7586 train_acc 74.48 train_fscore 72.6 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3062 test_acc 57.51 test_fscore 56.11 time 5.86\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7365    0.7500    0.7432    1256.0\n",
            "           1     0.4298    0.5231    0.4719     281.0\n",
            "           2     0.2000    0.0200    0.0364      50.0\n",
            "           3     0.2783    0.1538    0.1981     208.0\n",
            "           4     0.5174    0.4801    0.4981     402.0\n",
            "           5     0.1250    0.0147    0.0263      68.0\n",
            "           6     0.3791    0.5362    0.4442     345.0\n",
            "\n",
            "    accuracy                         0.5751    2610.0\n",
            "   macro avg     0.3809    0.3540    0.3455    2610.0\n",
            "weighted avg     0.5598    0.5751    0.5611    2610.0\n",
            "\n",
            "epoch 50 train_loss 0.7402 train_acc 75.04 train_fscore 73.31 valid_loss nan valid_acc nan val_fscore nan test_loss 1.322 test_acc 56.25 test_fscore 55.22 time 6.54\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7383    0.7389    0.7386    1256.0\n",
            "           1     0.4563    0.4093    0.4315     281.0\n",
            "           2     0.1429    0.0200    0.0351      50.0\n",
            "           3     0.2462    0.2308    0.2382     208.0\n",
            "           4     0.5556    0.3856    0.4552     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3565    0.6406    0.4580     345.0\n",
            "\n",
            "    accuracy                         0.5625    2610.0\n",
            "   macro avg     0.3565    0.3464    0.3367    2610.0\n",
            "weighted avg     0.5594    0.5625    0.5522    2610.0\n",
            "\n",
            "epoch 51 train_loss 0.7468 train_acc 74.91 train_fscore 73.19 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3299 test_acc 56.86 test_fscore 55.17 time 5.59\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7382    0.7452    0.7417    1256.0\n",
            "           1     0.4036    0.5658    0.4711     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2759    0.1154    0.1627     208.0\n",
            "           4     0.4949    0.4801    0.4874     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3683    0.4986    0.4236     345.0\n",
            "\n",
            "    accuracy                         0.5686    2610.0\n",
            "   macro avg     0.3258    0.3436    0.3266    2610.0\n",
            "weighted avg     0.5456    0.5686    0.5517    2610.0\n",
            "\n",
            "epoch 52 train_loss 0.7388 train_acc 75.28 train_fscore 73.66 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3046 test_acc 58.62 test_fscore 56.64 time 6.75\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7295    0.7707    0.7495    1256.0\n",
            "           1     0.4643    0.5089    0.4856     281.0\n",
            "           2     0.1250    0.0200    0.0345      50.0\n",
            "           3     0.3333    0.1346    0.1918     208.0\n",
            "           4     0.4891    0.5000    0.4945     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4021    0.5478    0.4638     345.0\n",
            "\n",
            "    accuracy                         0.5862    2610.0\n",
            "   macro avg     0.3633    0.3546    0.3457    2610.0\n",
            "weighted avg     0.5585    0.5862    0.5664    2610.0\n",
            "\n",
            "epoch 53 train_loss 0.7275 train_acc 75.14 train_fscore 73.62 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3526 test_acc 58.05 test_fscore 56.23 time 5.67\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7306    0.7731    0.7513    1256.0\n",
            "           1     0.4327    0.5374    0.4794     281.0\n",
            "           2     0.1250    0.0200    0.0345      50.0\n",
            "           3     0.2719    0.1490    0.1925     208.0\n",
            "           4     0.4783    0.4925    0.4853     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4148    0.4725    0.4417     345.0\n",
            "\n",
            "    accuracy                         0.5805    2610.0\n",
            "   macro avg     0.3505    0.3492    0.3407    2610.0\n",
            "weighted avg     0.5507    0.5805    0.5623    2610.0\n",
            "\n",
            "epoch 54 train_loss 0.7167 train_acc 75.49 train_fscore 73.95 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3535 test_acc 56.48 test_fscore 55.05 time 6.79\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7336    0.7500    0.7417    1256.0\n",
            "           1     0.4314    0.4698    0.4497     281.0\n",
            "           2     0.3333    0.0200    0.0377      50.0\n",
            "           3     0.2481    0.1538    0.1899     208.0\n",
            "           4     0.4905    0.4478    0.4681     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3596    0.5420    0.4324     345.0\n",
            "\n",
            "    accuracy                         0.5648    2610.0\n",
            "   macro avg     0.3709    0.3405    0.3314    2610.0\n",
            "weighted avg     0.5487    0.5648    0.5505    2610.0\n",
            "\n",
            "epoch 55 train_loss 0.7086 train_acc 75.98 train_fscore 74.56 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4002 test_acc 56.86 test_fscore 55.14 time 5.72\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7304    0.7635    0.7466    1256.0\n",
            "           1     0.4713    0.4377    0.4539     281.0\n",
            "           2     0.3333    0.0200    0.0377      50.0\n",
            "           3     0.2727    0.1442    0.1887     208.0\n",
            "           4     0.5249    0.3930    0.4495     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3424    0.6174    0.4405     345.0\n",
            "\n",
            "    accuracy                         0.5686    2610.0\n",
            "   macro avg     0.3822    0.3394    0.3310    2610.0\n",
            "weighted avg     0.5565    0.5686    0.5514    2610.0\n",
            "\n",
            "epoch 56 train_loss 0.7039 train_acc 76.24 train_fscore 74.71 valid_loss nan valid_acc nan val_fscore nan test_loss 1.3881 test_acc 56.9 test_fscore 54.85 time 6.61\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7225    0.7731    0.7469    1256.0\n",
            "           1     0.4542    0.4057    0.4286     281.0\n",
            "           2     0.3333    0.0200    0.0377      50.0\n",
            "           3     0.3030    0.1442    0.1954     208.0\n",
            "           4     0.5190    0.3731    0.4342     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3510    0.6348    0.4520     345.0\n",
            "\n",
            "    accuracy                         0.5690    2610.0\n",
            "   macro avg     0.3833    0.3358    0.3278    2610.0\n",
            "weighted avg     0.5534    0.5690    0.5485    2610.0\n",
            "\n",
            "epoch 57 train_loss 0.697 train_acc 76.84 train_fscore 75.49 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4083 test_acc 56.86 test_fscore 55.77 time 5.76\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7406    0.7365    0.7385    1256.0\n",
            "           1     0.4477    0.4413    0.4444     281.0\n",
            "           2     0.1111    0.0200    0.0339      50.0\n",
            "           3     0.2410    0.1923    0.2139     208.0\n",
            "           4     0.4871    0.5149    0.5006     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3864    0.5420    0.4511     345.0\n",
            "\n",
            "    accuracy                         0.5686    2610.0\n",
            "   macro avg     0.3448    0.3496    0.3404    2610.0\n",
            "weighted avg     0.5520    0.5686    0.5577    2610.0\n",
            "\n",
            "epoch 58 train_loss 0.6885 train_acc 76.61 train_fscore 75.43 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4429 test_acc 58.62 test_fscore 56.38 time 5.96\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7251    0.7898    0.7561    1256.0\n",
            "           1     0.4767    0.4377    0.4564     281.0\n",
            "           2     0.1250    0.0200    0.0345      50.0\n",
            "           3     0.3059    0.1250    0.1775     208.0\n",
            "           4     0.4961    0.4726    0.4841     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3905    0.5739    0.4648     345.0\n",
            "\n",
            "    accuracy                         0.5862    2610.0\n",
            "   macro avg     0.3599    0.3456    0.3390    2610.0\n",
            "weighted avg     0.5551    0.5862    0.5638    2610.0\n",
            "\n",
            "epoch 59 train_loss 0.6815 train_acc 77.01 train_fscore 75.73 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4067 test_acc 57.85 test_fscore 56.25 time 6.52\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7335    0.7627    0.7479    1256.0\n",
            "           1     0.4295    0.4769    0.4519     281.0\n",
            "           2     0.1667    0.0200    0.0357      50.0\n",
            "           3     0.2824    0.1779    0.2183     208.0\n",
            "           4     0.4680    0.5274    0.4959     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4190    0.4870    0.4504     345.0\n",
            "\n",
            "    accuracy                         0.5785    2610.0\n",
            "   macro avg     0.3570    0.3503    0.3429    2610.0\n",
            "weighted avg     0.5524    0.5785    0.5625    2610.0\n",
            "\n",
            "epoch 60 train_loss 0.6655 train_acc 77.41 train_fscore 76.18 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4305 test_acc 56.51 test_fscore 55.18 time 5.66\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7400    0.7389    0.7394    1256.0\n",
            "           1     0.4147    0.5018    0.4541     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2683    0.1587    0.1994     208.0\n",
            "           4     0.4640    0.5124    0.4870     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3787    0.4841    0.4249     345.0\n",
            "\n",
            "    accuracy                         0.5651    2610.0\n",
            "   macro avg     0.3237    0.3423    0.3293    2610.0\n",
            "weighted avg     0.5437    0.5651    0.5518    2610.0\n",
            "\n",
            "epoch 61 train_loss 0.6612 train_acc 77.58 train_fscore 76.35 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4486 test_acc 56.48 test_fscore 54.99 time 6.76\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7257    0.7540    0.7396    1256.0\n",
            "           1     0.4388    0.4591    0.4487     281.0\n",
            "           2     0.1667    0.0200    0.0357      50.0\n",
            "           3     0.2764    0.1635    0.2054     208.0\n",
            "           4     0.4615    0.4925    0.4765     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3716    0.4783    0.4183     345.0\n",
            "\n",
            "    accuracy                         0.5648    2610.0\n",
            "   macro avg     0.3487    0.3382    0.3320    2610.0\n",
            "weighted avg     0.5419    0.5648    0.5499    2610.0\n",
            "\n",
            "epoch 62 train_loss 0.6532 train_acc 77.82 train_fscore 76.71 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4056 test_acc 56.48 test_fscore 55.06 time 5.59\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7299    0.7508    0.7402    1256.0\n",
            "           1     0.4187    0.4947    0.4535     281.0\n",
            "           2     0.2000    0.0400    0.0667      50.0\n",
            "           3     0.2248    0.1394    0.1721     208.0\n",
            "           4     0.4628    0.4950    0.4784     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3971    0.4696    0.4303     345.0\n",
            "\n",
            "    accuracy                         0.5648    2610.0\n",
            "   macro avg     0.3476    0.3414    0.3344    2610.0\n",
            "weighted avg     0.5418    0.5648    0.5506    2610.0\n",
            "\n",
            "epoch 63 train_loss 0.6443 train_acc 77.94 train_fscore 76.77 valid_loss nan valid_acc nan val_fscore nan test_loss 1.5512 test_acc 54.87 test_fscore 53.84 time 6.77\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7423    0.7134    0.7276    1256.0\n",
            "           1     0.4305    0.4520    0.4410     281.0\n",
            "           2     0.0714    0.0200    0.0312      50.0\n",
            "           3     0.2718    0.1346    0.1801     208.0\n",
            "           4     0.5406    0.3806    0.4467     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3211    0.6580    0.4316     345.0\n",
            "\n",
            "    accuracy                         0.5487    2610.0\n",
            "   macro avg     0.3397    0.3369    0.3226    2610.0\n",
            "weighted avg     0.5523    0.5487    0.5384    2610.0\n",
            "\n",
            "epoch 64 train_loss 0.6366 train_acc 78.63 train_fscore 77.58 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4563 test_acc 57.13 test_fscore 55.52 time 5.7\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7246    0.7667    0.7451    1256.0\n",
            "           1     0.4217    0.4982    0.4568     281.0\n",
            "           2     0.1667    0.0400    0.0645      50.0\n",
            "           3     0.2553    0.1731    0.2063     208.0\n",
            "           4     0.4831    0.4279    0.4538     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4045    0.5159    0.4535     345.0\n",
            "\n",
            "    accuracy                         0.5713    2610.0\n",
            "   macro avg     0.3509    0.3460    0.3400    2610.0\n",
            "weighted avg     0.5455    0.5713    0.5552    2610.0\n",
            "\n",
            "epoch 65 train_loss 0.6249 train_acc 78.82 train_fscore 77.8 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4879 test_acc 56.13 test_fscore 55.04 time 6.77\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7312    0.7492    0.7401    1256.0\n",
            "           1     0.4440    0.4235    0.4335     281.0\n",
            "           2     0.1000    0.0200    0.0333      50.0\n",
            "           3     0.2328    0.2115    0.2217     208.0\n",
            "           4     0.4604    0.4627    0.4615     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3901    0.5043    0.4399     345.0\n",
            "\n",
            "    accuracy                         0.5613    2610.0\n",
            "   macro avg     0.3369    0.3388    0.3329    2610.0\n",
            "weighted avg     0.5426    0.5613    0.5504    2610.0\n",
            "\n",
            "epoch 66 train_loss 0.6387 train_acc 77.91 train_fscore 76.96 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4762 test_acc 57.39 test_fscore 55.42 time 5.64\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7242    0.7882    0.7549    1256.0\n",
            "           1     0.3848    0.5053    0.4369     281.0\n",
            "           2     0.1250    0.0400    0.0606      50.0\n",
            "           3     0.2700    0.1298    0.1753     208.0\n",
            "           4     0.5121    0.4204    0.4617     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3944    0.4870    0.4358     345.0\n",
            "\n",
            "    accuracy                         0.5739    2610.0\n",
            "   macro avg     0.3444    0.3387    0.3322    2610.0\n",
            "weighted avg     0.5449    0.5739    0.5542    2610.0\n",
            "\n",
            "epoch 67 train_loss 0.6238 train_acc 78.96 train_fscore 78.1 valid_loss nan valid_acc nan val_fscore nan test_loss 1.503 test_acc 56.13 test_fscore 54.64 time 6.35\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7309    0.7460    0.7384    1256.0\n",
            "           1     0.4255    0.4875    0.4544     281.0\n",
            "           2     0.1538    0.0400    0.0635      50.0\n",
            "           3     0.2747    0.1202    0.1672     208.0\n",
            "           4     0.5243    0.4030    0.4557     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3453    0.5855    0.4344     345.0\n",
            "\n",
            "    accuracy                         0.5613    2610.0\n",
            "   macro avg     0.3506    0.3403    0.3305    2610.0\n",
            "weighted avg     0.5488    0.5613    0.5464    2610.0\n",
            "\n",
            "epoch 68 train_loss 0.6181 train_acc 78.89 train_fscore 78.07 valid_loss nan valid_acc nan val_fscore nan test_loss 1.4895 test_acc 57.55 test_fscore 55.48 time 6.15\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7213    0.7914    0.7547    1256.0\n",
            "           1     0.4204    0.4698    0.4437     281.0\n",
            "           2     0.2000    0.0400    0.0667      50.0\n",
            "           3     0.3256    0.1346    0.1905     208.0\n",
            "           4     0.4346    0.5124    0.4703     402.0\n",
            "           5     0.0625    0.0147    0.0238      68.0\n",
            "           6     0.4187    0.4029    0.4106     345.0\n",
            "\n",
            "    accuracy                         0.5755    2610.0\n",
            "   macro avg     0.3690    0.3380    0.3372    2610.0\n",
            "weighted avg     0.5461    0.5755    0.5548    2610.0\n",
            "\n",
            "epoch 69 train_loss 0.5986 train_acc 79.46 train_fscore 78.58 valid_loss nan valid_acc nan val_fscore nan test_loss 1.5115 test_acc 56.59 test_fscore 54.88 time 5.95\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7281    0.7611    0.7443    1256.0\n",
            "           1     0.4060    0.4840    0.4416     281.0\n",
            "           2     0.1000    0.0400    0.0571      50.0\n",
            "           3     0.3077    0.1154    0.1678     208.0\n",
            "           4     0.4755    0.4826    0.4790     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3667    0.4783    0.4151     345.0\n",
            "\n",
            "    accuracy                         0.5659    2610.0\n",
            "   macro avg     0.3406    0.3373    0.3293    2610.0\n",
            "weighted avg     0.5422    0.5659    0.5488    2610.0\n",
            "\n",
            "epoch 70 train_loss 0.5866 train_acc 80.18 train_fscore 79.41 valid_loss nan valid_acc nan val_fscore nan test_loss 1.5828 test_acc 55.13 test_fscore 54.11 time 6.59\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7371    0.7253    0.7311    1256.0\n",
            "           1     0.4006    0.5160    0.4510     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.2578    0.1587    0.1964     208.0\n",
            "           4     0.5015    0.4229    0.4588     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3396    0.5217    0.4114     345.0\n",
            "\n",
            "    accuracy                         0.5513    2610.0\n",
            "   macro avg     0.3195    0.3349    0.3213    2610.0\n",
            "weighted avg     0.5405    0.5513    0.5411    2610.0\n",
            "\n",
            "epoch 71 train_loss 0.5967 train_acc 80.65 train_fscore 79.95 valid_loss nan valid_acc nan val_fscore nan test_loss 1.5828 test_acc 55.98 test_fscore 54.12 time 5.7\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7251    0.7540    0.7393    1256.0\n",
            "           1     0.4553    0.3808    0.4147     281.0\n",
            "           2     0.1250    0.0200    0.0345      50.0\n",
            "           3     0.3151    0.1106    0.1637     208.0\n",
            "           4     0.4970    0.4154    0.4526     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3318    0.6261    0.4337     345.0\n",
            "\n",
            "    accuracy                         0.5598    2610.0\n",
            "   macro avg     0.3499    0.3296    0.3198    2610.0\n",
            "weighted avg     0.5459    0.5598    0.5412    2610.0\n",
            "\n",
            "epoch 72 train_loss 0.5712 train_acc 80.67 train_fscore 79.93 valid_loss nan valid_acc nan val_fscore nan test_loss 1.5831 test_acc 54.37 test_fscore 53.39 time 6.61\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7292    0.7118    0.7204    1256.0\n",
            "           1     0.4357    0.4342    0.4349     281.0\n",
            "           2     0.1111    0.0400    0.0588      50.0\n",
            "           3     0.2424    0.1538    0.1882     208.0\n",
            "           4     0.5016    0.3831    0.4344     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3333    0.6232    0.4343     345.0\n",
            "\n",
            "    accuracy                         0.5437    2610.0\n",
            "   macro avg     0.3362    0.3352    0.3244    2610.0\n",
            "weighted avg     0.5406    0.5437    0.5339    2610.0\n",
            "\n",
            "epoch 73 train_loss 0.5581 train_acc 81.11 train_fscore 80.47 valid_loss nan valid_acc nan val_fscore nan test_loss 1.5848 test_acc 55.21 test_fscore 54.16 time 5.81\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7289    0.7237    0.7263    1256.0\n",
            "           1     0.4262    0.4626    0.4437     281.0\n",
            "           2     0.0769    0.0200    0.0317      50.0\n",
            "           3     0.2192    0.1538    0.1808     208.0\n",
            "           4     0.4918    0.4453    0.4674     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3578    0.5507    0.4338     345.0\n",
            "\n",
            "    accuracy                         0.5521    2610.0\n",
            "   macro avg     0.3287    0.3366    0.3262    2610.0\n",
            "weighted avg     0.5387    0.5521    0.5416    2610.0\n",
            "\n",
            "epoch 74 train_loss 0.5577 train_acc 81.19 train_fscore 80.62 valid_loss nan valid_acc nan val_fscore nan test_loss 1.5902 test_acc 56.82 test_fscore 55.15 time 6.83\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7179    0.7803    0.7478    1256.0\n",
            "           1     0.4211    0.3986    0.4095     281.0\n",
            "           2     0.1111    0.0200    0.0339      50.0\n",
            "           3     0.2639    0.1827    0.2159     208.0\n",
            "           4     0.4930    0.4403    0.4652     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3872    0.5072    0.4391     345.0\n",
            "\n",
            "    accuracy                         0.5682    2610.0\n",
            "   macro avg     0.3420    0.3327    0.3302    2610.0\n",
            "weighted avg     0.5411    0.5682    0.5515    2610.0\n",
            "\n",
            "epoch 75 train_loss 0.5677 train_acc 80.44 train_fscore 79.82 valid_loss nan valid_acc nan val_fscore nan test_loss 1.6021 test_acc 56.28 test_fscore 54.62 time 5.65\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7242    0.7611    0.7422    1256.0\n",
            "           1     0.3984    0.5160    0.4496     281.0\n",
            "           2     0.0769    0.0200    0.0317      50.0\n",
            "           3     0.2788    0.1394    0.1859     208.0\n",
            "           4     0.4476    0.4776    0.4621     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3946    0.4232    0.4084     345.0\n",
            "\n",
            "    accuracy                         0.5628    2610.0\n",
            "   macro avg     0.3315    0.3339    0.3257    2610.0\n",
            "weighted avg     0.5362    0.5628    0.5462    2610.0\n",
            "\n",
            "epoch 76 train_loss 0.5435 train_acc 81.56 train_fscore 80.92 valid_loss nan valid_acc nan val_fscore nan test_loss 1.5479 test_acc 56.09 test_fscore 54.7 time 6.56\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7171    0.7667    0.7411    1256.0\n",
            "           1     0.4362    0.3772    0.4046     281.0\n",
            "           2     0.0976    0.0800    0.0879      50.0\n",
            "           3     0.2677    0.1635    0.2030     208.0\n",
            "           4     0.4575    0.4950    0.4755     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3901    0.4580    0.4213     345.0\n",
            "\n",
            "    accuracy                         0.5609    2610.0\n",
            "   macro avg     0.3380    0.3343    0.3333    2610.0\n",
            "weighted avg     0.5373    0.5609    0.5470    2610.0\n",
            "\n",
            "epoch 77 train_loss 0.5337 train_acc 81.9 train_fscore 81.29 valid_loss nan valid_acc nan val_fscore nan test_loss 1.6418 test_acc 56.02 test_fscore 54.86 time 5.73\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7225    0.7484    0.7352    1256.0\n",
            "           1     0.4391    0.4235    0.4312     281.0\n",
            "           2     0.1176    0.0400    0.0597      50.0\n",
            "           3     0.2265    0.1971    0.2108     208.0\n",
            "           4     0.5046    0.4080    0.4512     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3874    0.5681    0.4606     345.0\n",
            "\n",
            "    accuracy                         0.5602    2610.0\n",
            "   macro avg     0.3425    0.3407    0.3355    2610.0\n",
            "weighted avg     0.5442    0.5602    0.5486    2610.0\n",
            "\n",
            "epoch 78 train_loss 0.5203 train_acc 82.2 train_fscore 81.69 valid_loss nan valid_acc nan val_fscore nan test_loss 1.639 test_acc 55.36 test_fscore 54.63 time 5.89\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7298    0.7420    0.7359    1256.0\n",
            "           1     0.3927    0.4626    0.4248     281.0\n",
            "           2     0.0263    0.0200    0.0227      50.0\n",
            "           3     0.2268    0.2115    0.2189     208.0\n",
            "           4     0.5096    0.3955    0.4454     402.0\n",
            "           5     0.1111    0.0147    0.0260      68.0\n",
            "           6     0.3964    0.5159    0.4484     345.0\n",
            "\n",
            "    accuracy                         0.5536    2610.0\n",
            "   macro avg     0.3418    0.3375    0.3317    2610.0\n",
            "weighted avg     0.5459    0.5536    0.5463    2610.0\n",
            "\n",
            "epoch 79 train_loss 0.5249 train_acc 81.85 train_fscore 81.33 valid_loss nan valid_acc nan val_fscore nan test_loss 1.6495 test_acc 56.17 test_fscore 54.52 time 6.49\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7158    0.7779    0.7455    1256.0\n",
            "           1     0.4209    0.4164    0.4186     281.0\n",
            "           2     0.0541    0.0400    0.0460      50.0\n",
            "           3     0.2841    0.1202    0.1689     208.0\n",
            "           4     0.4820    0.4328    0.4561     402.0\n",
            "           5     0.0370    0.0147    0.0211      68.0\n",
            "           6     0.3744    0.4928    0.4255     345.0\n",
            "\n",
            "    accuracy                         0.5617    2610.0\n",
            "   macro avg     0.3383    0.3278    0.3260    2610.0\n",
            "weighted avg     0.5381    0.5617    0.5452    2610.0\n",
            "\n",
            "epoch 80 train_loss 0.5227 train_acc 81.86 train_fscore 81.39 valid_loss nan valid_acc nan val_fscore nan test_loss 1.7052 test_acc 57.16 test_fscore 55.41 time 5.53\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7170    0.7787    0.7466    1256.0\n",
            "           1     0.4146    0.4662    0.4389     281.0\n",
            "           2     0.1429    0.0200    0.0351      50.0\n",
            "           3     0.2806    0.1875    0.2248     208.0\n",
            "           4     0.4757    0.4627    0.4691     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4067    0.4551    0.4295     345.0\n",
            "\n",
            "    accuracy                         0.5716    2610.0\n",
            "   macro avg     0.3482    0.3386    0.3349    2610.0\n",
            "weighted avg     0.5418    0.5716    0.5541    2610.0\n",
            "\n",
            "epoch 81 train_loss 0.5068 train_acc 82.74 train_fscore 82.22 valid_loss nan valid_acc nan val_fscore nan test_loss 1.6504 test_acc 55.48 test_fscore 54.26 time 6.78\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7108    0.7611    0.7351    1256.0\n",
            "           1     0.4028    0.4128    0.4077     281.0\n",
            "           2     0.0851    0.0800    0.0825      50.0\n",
            "           3     0.2449    0.1731    0.2028     208.0\n",
            "           4     0.4733    0.4403    0.4562     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4046    0.4609    0.4309     345.0\n",
            "\n",
            "    accuracy                         0.5548    2610.0\n",
            "   macro avg     0.3316    0.3326    0.3307    2610.0\n",
            "weighted avg     0.5329    0.5548    0.5426    2610.0\n",
            "\n",
            "epoch 82 train_loss 0.4973 train_acc 82.83 train_fscore 82.37 valid_loss nan valid_acc nan val_fscore nan test_loss 1.6774 test_acc 54.9 test_fscore 53.95 time 5.6\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7273    0.7325    0.7299    1256.0\n",
            "           1     0.4449    0.3879    0.4144     281.0\n",
            "           2     0.0690    0.0400    0.0506      50.0\n",
            "           3     0.2619    0.1587    0.1976     208.0\n",
            "           4     0.4502    0.4502    0.4502     402.0\n",
            "           5     0.1429    0.0294    0.0488      68.0\n",
            "           6     0.3516    0.5391    0.4256     345.0\n",
            "\n",
            "    accuracy                         0.5490    2610.0\n",
            "   macro avg     0.3497    0.3340    0.3310    2610.0\n",
            "weighted avg     0.5396    0.5490    0.5395    2610.0\n",
            "\n",
            "epoch 83 train_loss 0.4869 train_acc 83.23 train_fscore 82.8 valid_loss nan valid_acc nan val_fscore nan test_loss 1.7191 test_acc 55.29 test_fscore 54.16 time 6.7\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7218    0.7500    0.7357    1256.0\n",
            "           1     0.4203    0.4128    0.4165     281.0\n",
            "           2     0.0526    0.0200    0.0290      50.0\n",
            "           3     0.2097    0.1875    0.1980     208.0\n",
            "           4     0.4970    0.4080    0.4481     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3701    0.5246    0.4341     345.0\n",
            "\n",
            "    accuracy                         0.5529    2610.0\n",
            "   macro avg     0.3245    0.3290    0.3230    2610.0\n",
            "weighted avg     0.5358    0.5529    0.5416    2610.0\n",
            "\n",
            "epoch 84 train_loss 0.4812 train_acc 83.43 train_fscore 82.99 valid_loss nan valid_acc nan val_fscore nan test_loss 1.7418 test_acc 54.56 test_fscore 53.47 time 5.67\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7265    0.7189    0.7227    1256.0\n",
            "           1     0.4071    0.4520    0.4283     281.0\n",
            "           2     0.1000    0.0400    0.0571      50.0\n",
            "           3     0.2411    0.1298    0.1687     208.0\n",
            "           4     0.4602    0.4602    0.4602     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3502    0.5217    0.4191     345.0\n",
            "\n",
            "    accuracy                         0.5456    2610.0\n",
            "   macro avg     0.3264    0.3318    0.3223    2610.0\n",
            "weighted avg     0.5317    0.5456    0.5347    2610.0\n",
            "\n",
            "epoch 85 train_loss 0.4753 train_acc 83.71 train_fscore 83.33 valid_loss nan valid_acc nan val_fscore nan test_loss 1.7582 test_acc 55.02 test_fscore 53.89 time 6.8\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7273    0.7261    0.7267    1256.0\n",
            "           1     0.4107    0.4662    0.4367     281.0\n",
            "           2     0.1818    0.0400    0.0656      50.0\n",
            "           3     0.2752    0.1442    0.1893     208.0\n",
            "           4     0.4969    0.4005    0.4435     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3484    0.5797    0.4353     345.0\n",
            "\n",
            "    accuracy                         0.5502    2610.0\n",
            "   macro avg     0.3486    0.3367    0.3281    2610.0\n",
            "weighted avg     0.5422    0.5502    0.5389    2610.0\n",
            "\n",
            "epoch 86 train_loss 0.4709 train_acc 83.75 train_fscore 83.34 valid_loss nan valid_acc nan val_fscore nan test_loss 1.77 test_acc 54.71 test_fscore 53.81 time 5.66\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7357    0.7245    0.7300    1256.0\n",
            "           1     0.4228    0.3701    0.3947     281.0\n",
            "           2     0.1667    0.0600    0.0882      50.0\n",
            "           3     0.2556    0.1635    0.1994     208.0\n",
            "           4     0.4843    0.4229    0.4515     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3377    0.6000    0.4322     345.0\n",
            "\n",
            "    accuracy                         0.5471    2610.0\n",
            "   macro avg     0.3432    0.3344    0.3280    2610.0\n",
            "weighted avg     0.5423    0.5471    0.5381    2610.0\n",
            "\n",
            "epoch 87 train_loss 0.4578 train_acc 84.18 train_fscore 83.81 valid_loss nan valid_acc nan val_fscore nan test_loss 1.8116 test_acc 55.75 test_fscore 54.59 time 6.29\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7266    0.7428    0.7346    1256.0\n",
            "           1     0.4275    0.4199    0.4237     281.0\n",
            "           2     0.0526    0.0200    0.0290      50.0\n",
            "           3     0.2500    0.1779    0.2079     208.0\n",
            "           4     0.4552    0.4925    0.4731     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3836    0.4870    0.4291     345.0\n",
            "\n",
            "    accuracy                         0.5575    2610.0\n",
            "   macro avg     0.3279    0.3343    0.3282    2610.0\n",
            "weighted avg     0.5374    0.5575    0.5459    2610.0\n",
            "\n",
            "epoch 88 train_loss 0.4557 train_acc 84.54 train_fscore 84.18 valid_loss nan valid_acc nan val_fscore nan test_loss 1.8105 test_acc 54.79 test_fscore 53.67 time 6.42\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7224    0.7293    0.7258    1256.0\n",
            "           1     0.4024    0.4840    0.4394     281.0\n",
            "           2     0.0833    0.0400    0.0541      50.0\n",
            "           3     0.2373    0.1346    0.1718     208.0\n",
            "           4     0.4808    0.4353    0.4569     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3552    0.5014    0.4159     345.0\n",
            "\n",
            "    accuracy                         0.5479    2610.0\n",
            "   macro avg     0.3259    0.3321    0.3234    2610.0\n",
            "weighted avg     0.5325    0.5479    0.5367    2610.0\n",
            "\n",
            "epoch 89 train_loss 0.4481 train_acc 84.56 train_fscore 84.17 valid_loss nan valid_acc nan val_fscore nan test_loss 1.758 test_acc 55.71 test_fscore 54.34 time 5.96\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7177    0.7611    0.7388    1256.0\n",
            "           1     0.4007    0.4235    0.4118     281.0\n",
            "           2     0.0870    0.0800    0.0833      50.0\n",
            "           3     0.2832    0.1538    0.1994     208.0\n",
            "           4     0.4429    0.4627    0.4526     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4015    0.4551    0.4266     345.0\n",
            "\n",
            "    accuracy                         0.5571    2610.0\n",
            "   macro avg     0.3333    0.3337    0.3304    2610.0\n",
            "weighted avg     0.5340    0.5571    0.5434    2610.0\n",
            "\n",
            "epoch 90 train_loss 0.459 train_acc 84.26 train_fscore 83.85 valid_loss nan valid_acc nan val_fscore nan test_loss 1.9112 test_acc 55.86 test_fscore 54.59 time 6.58\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7246    0.7500    0.7371    1256.0\n",
            "           1     0.4402    0.4057    0.4222     281.0\n",
            "           2     0.0909    0.0200    0.0328      50.0\n",
            "           3     0.2593    0.1683    0.2041     208.0\n",
            "           4     0.4824    0.4428    0.4617     402.0\n",
            "           5     0.0625    0.0147    0.0238      68.0\n",
            "           6     0.3596    0.5420    0.4324     345.0\n",
            "\n",
            "    accuracy                         0.5586    2610.0\n",
            "   macro avg     0.3456    0.3348    0.3306    2610.0\n",
            "weighted avg     0.5420    0.5586    0.5459    2610.0\n",
            "\n",
            "epoch 91 train_loss 0.4359 train_acc 85.14 train_fscore 84.82 valid_loss nan valid_acc nan val_fscore nan test_loss 1.8095 test_acc 55.33 test_fscore 54.16 time 5.67\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7220    0.7404    0.7311    1256.0\n",
            "           1     0.4201    0.4306    0.4253     281.0\n",
            "           2     0.0952    0.0800    0.0870      50.0\n",
            "           3     0.2547    0.1298    0.1720     208.0\n",
            "           4     0.4800    0.4478    0.4633     402.0\n",
            "           5     0.1000    0.0147    0.0256      68.0\n",
            "           6     0.3613    0.5246    0.4279     345.0\n",
            "\n",
            "    accuracy                         0.5533    2610.0\n",
            "   macro avg     0.3476    0.3383    0.3332    2610.0\n",
            "weighted avg     0.5391    0.5533    0.5416    2610.0\n",
            "\n",
            "epoch 92 train_loss 0.4354 train_acc 84.94 train_fscore 84.58 valid_loss nan valid_acc nan val_fscore nan test_loss 1.8495 test_acc 54.21 test_fscore 53.76 time 6.83\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7258    0.7229    0.7244    1256.0\n",
            "           1     0.3980    0.4164    0.4070     281.0\n",
            "           2     0.0588    0.0800    0.0678      50.0\n",
            "           3     0.2387    0.1779    0.2039     208.0\n",
            "           4     0.5000    0.4204    0.4568     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3704    0.5217    0.4332     345.0\n",
            "\n",
            "    accuracy                         0.5421    2610.0\n",
            "   macro avg     0.3274    0.3342    0.3276    2610.0\n",
            "weighted avg     0.5382    0.5421    0.5376    2610.0\n",
            "\n",
            "epoch 93 train_loss 0.4343 train_acc 85.09 train_fscore 84.76 valid_loss nan valid_acc nan val_fscore nan test_loss 1.8302 test_acc 56.21 test_fscore 54.46 time 5.59\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7117    0.7803    0.7444    1256.0\n",
            "           1     0.4052    0.4413    0.4225     281.0\n",
            "           2     0.1000    0.0800    0.0889      50.0\n",
            "           3     0.2596    0.1298    0.1731     208.0\n",
            "           4     0.4767    0.4328    0.4537     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3882    0.4580    0.4202     345.0\n",
            "\n",
            "    accuracy                         0.5621    2610.0\n",
            "   macro avg     0.3345    0.3317    0.3290    2610.0\n",
            "weighted avg     0.5335    0.5621    0.5446    2610.0\n",
            "\n",
            "epoch 94 train_loss 0.4207 train_acc 85.47 train_fscore 85.13 valid_loss nan valid_acc nan val_fscore nan test_loss 1.913 test_acc 54.9 test_fscore 53.6 time 6.89\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7081    0.7627    0.7344    1256.0\n",
            "           1     0.4211    0.3986    0.4095     281.0\n",
            "           2     0.0714    0.0800    0.0755      50.0\n",
            "           3     0.2397    0.1394    0.1763     208.0\n",
            "           4     0.5069    0.3632    0.4232     402.0\n",
            "           5     0.0385    0.0147    0.0213      68.0\n",
            "           6     0.3660    0.5304    0.4331     345.0\n",
            "\n",
            "    accuracy                         0.5490    2610.0\n",
            "   macro avg     0.3359    0.3270    0.3248    2610.0\n",
            "weighted avg     0.5340    0.5490    0.5360    2610.0\n",
            "\n",
            "epoch 95 train_loss 0.4213 train_acc 85.25 train_fscore 84.97 valid_loss nan valid_acc nan val_fscore nan test_loss 1.8998 test_acc 57.13 test_fscore 55.33 time 5.62\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7121    0.7938    0.7508    1256.0\n",
            "           1     0.4226    0.3986    0.4103     281.0\n",
            "           2     0.1176    0.0400    0.0597      50.0\n",
            "           3     0.2468    0.1875    0.2131     208.0\n",
            "           4     0.5028    0.4428    0.4709     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3995    0.4725    0.4329     345.0\n",
            "\n",
            "    accuracy                         0.5713    2610.0\n",
            "   macro avg     0.3431    0.3336    0.3340    2610.0\n",
            "weighted avg     0.5404    0.5713    0.5533    2610.0\n",
            "\n",
            "epoch 96 train_loss 0.4092 train_acc 85.97 train_fscore 85.71 valid_loss nan valid_acc nan val_fscore nan test_loss 1.9301 test_acc 54.67 test_fscore 54.03 time 6.42\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7288    0.7253    0.7271    1256.0\n",
            "           1     0.4220    0.4235    0.4227     281.0\n",
            "           2     0.0833    0.0800    0.0816      50.0\n",
            "           3     0.2517    0.1731    0.2051     208.0\n",
            "           4     0.4815    0.4527    0.4667     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3535    0.5072    0.4167     345.0\n",
            "\n",
            "    accuracy                         0.5467    2610.0\n",
            "   macro avg     0.3316    0.3374    0.3314    2610.0\n",
            "weighted avg     0.5387    0.5467    0.5403    2610.0\n",
            "\n",
            "epoch 97 train_loss 0.4157 train_acc 85.65 train_fscore 85.39 valid_loss nan valid_acc nan val_fscore nan test_loss 2.0124 test_acc 55.21 test_fscore 54.06 time 5.77\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7249    0.7301    0.7275    1256.0\n",
            "           1     0.4333    0.4164    0.4247     281.0\n",
            "           2     0.2222    0.0400    0.0678      50.0\n",
            "           3     0.2246    0.1490    0.1792     208.0\n",
            "           4     0.4785    0.4701    0.4743     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3517    0.5362    0.4248     345.0\n",
            "\n",
            "    accuracy                         0.5521    2610.0\n",
            "   macro avg     0.3479    0.3346    0.3283    2610.0\n",
            "weighted avg     0.5378    0.5521    0.5406    2610.0\n",
            "\n",
            "epoch 98 train_loss 0.4037 train_acc 85.87 train_fscore 85.61 valid_loss nan valid_acc nan val_fscore nan test_loss 1.9667 test_acc 56.7 test_fscore 54.85 time 5.96\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7164    0.7763    0.7451    1256.0\n",
            "           1     0.4037    0.4626    0.4312     281.0\n",
            "           2     0.1481    0.0800    0.1039      50.0\n",
            "           3     0.3898    0.1106    0.1723     208.0\n",
            "           4     0.4673    0.4627    0.4650     402.0\n",
            "           5     0.0435    0.0147    0.0220      68.0\n",
            "           6     0.3833    0.4667    0.4209     345.0\n",
            "\n",
            "    accuracy                         0.5670    2610.0\n",
            "   macro avg     0.3646    0.3391    0.3372    2610.0\n",
            "weighted avg     0.5459    0.5670    0.5485    2610.0\n",
            "\n",
            "epoch 99 train_loss 0.3942 train_acc 86.37 train_fscore 86.11 valid_loss nan valid_acc nan val_fscore nan test_loss 1.9821 test_acc 55.59 test_fscore 54.23 time 6.55\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7246    0.7373    0.7309    1256.0\n",
            "           1     0.4037    0.4626    0.4312     281.0\n",
            "           2     0.0952    0.0400    0.0563      50.0\n",
            "           3     0.2900    0.1394    0.1883     208.0\n",
            "           4     0.4358    0.5149    0.4721     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3877    0.4551    0.4187     345.0\n",
            "\n",
            "    accuracy                         0.5559    2610.0\n",
            "   macro avg     0.3339    0.3356    0.3282    2610.0\n",
            "weighted avg     0.5354    0.5559    0.5423    2610.0\n",
            "\n",
            "epoch 100 train_loss 0.3837 train_acc 86.66 train_fscore 86.4 valid_loss nan valid_acc nan val_fscore nan test_loss 2.0219 test_acc 55.4 test_fscore 54.17 time 5.41\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7222    0.7452    0.7335    1256.0\n",
            "           1     0.4258    0.3879    0.4060     281.0\n",
            "           2     0.1000    0.1200    0.1091      50.0\n",
            "           3     0.2692    0.1346    0.1795     208.0\n",
            "           4     0.4723    0.4453    0.4584     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.3672    0.5449    0.4387     345.0\n",
            "\n",
            "    accuracy                         0.5540    2610.0\n",
            "   macro avg     0.3367    0.3397    0.3322    2610.0\n",
            "weighted avg     0.5380    0.5540    0.5417    2610.0\n",
            "\n",
            "Test performance..\n",
            "Fscore 57.42 accuracy 59.69\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7286    0.7994    0.7623    1256.0\n",
            "           1     0.4601    0.5338    0.4942     281.0\n",
            "           2     0.0000    0.0000    0.0000      50.0\n",
            "           3     0.3608    0.1683    0.2295     208.0\n",
            "           4     0.4895    0.5199    0.5042     402.0\n",
            "           5     0.0000    0.0000    0.0000      68.0\n",
            "           6     0.4188    0.4638    0.4402     345.0\n",
            "\n",
            "    accuracy                         0.5969    2610.0\n",
            "   macro avg     0.3511    0.3550    0.3472    2610.0\n",
            "weighted avg     0.5597    0.5969    0.5742    2610.0\n",
            "\n",
            "[[1.004e+03 5.800e+01 0.000e+00 3.500e+01 9.800e+01 0.000e+00 6.100e+01]\n",
            " [6.600e+01 1.500e+02 0.000e+00 1.000e+00 3.000e+01 0.000e+00 3.400e+01]\n",
            " [2.600e+01 4.000e+00 0.000e+00 1.000e+00 1.000e+01 0.000e+00 9.000e+00]\n",
            " [9.400e+01 1.500e+01 0.000e+00 3.500e+01 2.000e+01 0.000e+00 4.400e+01]\n",
            " [8.800e+01 4.100e+01 0.000e+00 9.000e+00 2.090e+02 0.000e+00 5.500e+01]\n",
            " [2.600e+01 1.100e+01 0.000e+00 5.000e+00 7.000e+00 0.000e+00 1.900e+01]\n",
            " [7.400e+01 4.700e+01 0.000e+00 1.100e+01 5.300e+01 0.000e+00 1.600e+02]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tt4W_TNYKzzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b8cIgmI6Kz1f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}